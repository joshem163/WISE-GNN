{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af88d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx import ego_graph\n",
    "\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "#from logger import Logger\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7babc9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], adj_t=[183, 183, nnz=325])\n"
     ]
    }
   ],
   "source": [
    "dataset = WebKB(root='/tmp/Texas', name='Texas',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "print(data)\n",
    "#split_idx = dataset.get_idx_split()\n",
    "#train_idx = split_idx['train'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91fdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n",
      "590\n",
      "370\n"
     ]
    }
   ],
   "source": [
    "train_index = np.where(data.train_mask)[0]\n",
    "print(len(train_index))\n",
    "valid_index = np.where(data.val_mask)[0]\n",
    "print(len(valid_index))\n",
    "test_index = np.where(data.test_mask)[0]\n",
    "print(len(test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0b82f",
   "metadata": {},
   "source": [
    "# GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9ef33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47468ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout,heads):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, cached=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.heads=heads\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            #x=F.softmax(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    #print(len(out))\n",
    "    #print(data.y.squeeze(1)[train_idx])\n",
    "    loss = F.nll_loss(out, data.y.squeeze()[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def ACC(Prediction, Label):\n",
    "    correct = Prediction.view(-1).eq(Label).sum().item()\n",
    "    total=len(Label)\n",
    "    return correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, train_idx,valid_idx,test_idx):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    y_pred=y_pred.view(-1)\n",
    "    train_acc=ACC(data.y[train_idx],y_pred[train_idx])\n",
    "    valid_acc=ACC(data.y[valid_idx],y_pred[valid_idx])\n",
    "    test_acc =ACC(data.y[test_idx],y_pred[test_idx])\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e19e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   4   8  11  13  15  19  21  26  32  35  37  39  41  44  45  46\n",
      "  50  53  58  59  60  61  63  65  66  70  71  72  73  74  75  76  77  78\n",
      "  83  85  86  88  89  92  94  95  96 101 103 104 105 107 108 110 113 115\n",
      " 116 119 120 121 123 124 125 126 130 131 133 135 137 139 140 141 142 144\n",
      " 145 146 148 152 153 156 161 168 170 171 173 177 179 181 182]\n"
     ]
    }
   ],
   "source": [
    "idx=[data.train_mask[i][0] for i in range(183)]\n",
    "train_index = np.where(idx)[0]\n",
    "print(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23796d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 01:\n",
      "Highest Train: 97.70\n",
      "Highest Valid: 59.32\n",
      "  Final Train: 95.40\n",
      "   Final Test: 54.05\n",
      "Run 02:\n",
      "Highest Train: 96.55\n",
      "Highest Valid: 57.63\n",
      "  Final Train: 71.26\n",
      "   Final Test: 54.05\n",
      "Run 03:\n",
      "Highest Train: 97.70\n",
      "Highest Valid: 59.32\n",
      "  Final Train: 64.37\n",
      "   Final Test: 48.65\n",
      "Run 04:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 54.24\n",
      "  Final Train: 86.21\n",
      "   Final Test: 48.65\n",
      "Run 05:\n",
      "Highest Train: 96.55\n",
      "Highest Valid: 62.71\n",
      "  Final Train: 49.43\n",
      "   Final Test: 56.76\n",
      "Run 06:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 59.32\n",
      "  Final Train: 70.11\n",
      "   Final Test: 54.05\n",
      "Run 07:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 66.10\n",
      "  Final Train: 64.37\n",
      "   Final Test: 54.05\n",
      "Run 08:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 64.41\n",
      "  Final Train: 62.07\n",
      "   Final Test: 59.46\n",
      "Run 09:\n",
      "Highest Train: 95.40\n",
      "Highest Valid: 55.93\n",
      "  Final Train: 74.71\n",
      "   Final Test: 67.57\n",
      "Run 10:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 52.54\n",
      "  Final Train: 74.71\n",
      "   Final Test: 62.16\n",
      "All runs:\n",
      "Highest Train: 97.93 ± 1.41\n",
      "Highest Valid: 59.15 ± 4.34\n",
      "  Final Train: 71.26 ± 12.86\n",
      "   Final Test: 55.95 ± 5.85\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_channels': 16, 'dropout': 0.5, 'epochs': 50, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-4, 'lr': 0.01}\n",
    "\n",
    "    args = objectview(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    dataset = WebKB(root='/tmp/Texas', name='Texas',transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    model = GCN(data.num_features, args.hidden_channels, dataset.num_classes, args.num_layers,\n",
    "                    args.dropout,args.heads)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        idx_train=[data.train_mask[i][run] for i in range(len(data.y))]\n",
    "        train_idx = np.where(idx_train)[0]\n",
    "        idx_val=[data.val_mask[i][run] for i in range(len(data.y))]\n",
    "        valid_idx = np.where(idx_val)[0]\n",
    "        idx_test=[data.test_mask[i][run] for i in range(len(data.y))]\n",
    "        test_idx = np.where(idx_test)[0]\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                #print(f'Run: {run + 1:02d}, 'f'Epoch: {epoch:02d}, 'f'Loss: {loss:.4f}, 'f'Train: {100 * train_acc:.2f}%, '\n",
    "                 #     f'Valid: {100 * valid_acc:.2f}% '\n",
    "                  #    f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52f151",
   "metadata": {},
   "source": [
    "# WISE embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a09514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], adj_t=[183, 183, nnz=325])\n"
     ]
    }
   ],
   "source": [
    "dataset = WebKB(root='/tmp/Texas', name='Texas',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96f82a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "      <th>1701</th>\n",
       "      <th>1702</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1704 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  1694  1695  1696  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...   0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "   1697  1698  1699  1700  1701  1702  class  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0      0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0      2  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0      4  \n",
       "\n",
       "[5 rows x 1704 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Domain_Fec=pd.DataFrame(data.x.numpy())\n",
    "label=pd.DataFrame(data.y.numpy(),columns =['class'])\n",
    "Data=pd.concat([Domain_Fec,label], axis=1)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2642b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_nodes=len(data.y)\n",
    "fe_len=len(data.x[0])\n",
    "catagories=Data['class'].to_numpy()\n",
    "data_by_class = {cls: Data.loc[Data['class'] == cls].drop(['class'], axis=1) for cls in range(max(catagories) + 1)}\n",
    "basis = [[max(df[i]) for i in range(len(df.columns))] for df in data_by_class.values()]\n",
    "sel_basis = [[int(list(df[i].to_numpy()).count(1) >= int(len(df[i].index)*0.1)) \n",
    "              for i in range(len(df.columns))]\n",
    "             for df in data_by_class.values()]\n",
    "feature_names = [ii for ii in range(fe_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be2970d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity(array1, array2):\n",
    "    intersection = np.sum(np.logical_and(array1, array2))\n",
    "    return intersection\n",
    "Fec=[]\n",
    "for i in range(Number_nodes):\n",
    "#for i in range(2):\n",
    "    vec=[]\n",
    "    f=Data.loc[i, feature_names].values.flatten().tolist()\n",
    "    vec.append(Similarity(f,basis[0]))\n",
    "    vec.append(Similarity(f,basis[1]))\n",
    "    vec.append(Similarity(f,basis[2]))\n",
    "    vec.append(Similarity(f,basis[3]))\n",
    "    vec.append(Similarity(f,basis[4]))\n",
    "    #print(f)\n",
    "    f.clear()\n",
    "    Fec.append(vec)\n",
    "SFec=[]\n",
    "for i in range(Number_nodes):\n",
    "#for i in range(2):\n",
    "    Svec=[]\n",
    "    f=Data.loc[i, feature_names].values.flatten().tolist()\n",
    "    Svec.append(Similarity(f,sel_basis[0]))\n",
    "    Svec.append(Similarity(f,sel_basis[1]))\n",
    "    Svec.append(Similarity(f,sel_basis[2]))\n",
    "    Svec.append(Similarity(f,sel_basis[3]))\n",
    "    Svec.append(Similarity(f,sel_basis[4]))\n",
    "    #print(f)\n",
    "    f.clear()\n",
    "    SFec.append(Svec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "054ee569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 39.,  20.,  40.,  ...,  40.,  30.,  32.],\n",
      "        [231.,  25., 155.,  ..., 155.,  64.,  92.],\n",
      "        [ 25.,  20.,  26.,  ...,  26.,  23.,  23.],\n",
      "        ...,\n",
      "        [ 77.,  32.,  73.,  ...,  73.,  49.,  67.],\n",
      "        [ 61.,  28.,  56.,  ...,  56.,  54.,  51.],\n",
      "        [ 41.,  23.,  37.,  ...,  37.,  36.,  32.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inc_fe=torch.tensor(Fec)\n",
    "sel_fe=torch.tensor(SFec)\n",
    "CC_domain=torch.cat((Inc_fe, sel_fe), 1).float()\n",
    "print(CC_domain)\n",
    "CC_domain.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f7d51",
   "metadata": {},
   "source": [
    "# W-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55c6fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 10], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], adj_t=[183, 183, nnz=325])\n"
     ]
    }
   ],
   "source": [
    "data.x=CC_domain\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4763ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x16a51a3b0>\n",
      "Run 01:\n",
      "Highest Train: 73.56\n",
      "Highest Valid: 71.19\n",
      "  Final Train: 64.37\n",
      "   Final Test: 75.68\n",
      "Run 02:\n",
      "Highest Train: 70.11\n",
      "Highest Valid: 64.41\n",
      "  Final Train: 70.11\n",
      "   Final Test: 72.97\n",
      "Run 03:\n",
      "Highest Train: 70.11\n",
      "Highest Valid: 64.41\n",
      "  Final Train: 65.52\n",
      "   Final Test: 51.35\n",
      "Run 04:\n",
      "Highest Train: 66.67\n",
      "Highest Valid: 67.80\n",
      "  Final Train: 65.52\n",
      "   Final Test: 67.57\n",
      "Run 05:\n",
      "Highest Train: 60.92\n",
      "Highest Valid: 64.41\n",
      "  Final Train: 55.17\n",
      "   Final Test: 54.05\n",
      "Run 06:\n",
      "Highest Train: 68.97\n",
      "Highest Valid: 74.58\n",
      "  Final Train: 65.52\n",
      "   Final Test: 70.27\n",
      "Run 07:\n",
      "Highest Train: 78.16\n",
      "Highest Valid: 76.27\n",
      "  Final Train: 73.56\n",
      "   Final Test: 62.16\n",
      "Run 08:\n",
      "Highest Train: 73.56\n",
      "Highest Valid: 71.19\n",
      "  Final Train: 72.41\n",
      "   Final Test: 70.27\n",
      "Run 09:\n",
      "Highest Train: 66.67\n",
      "Highest Valid: 57.63\n",
      "  Final Train: 62.07\n",
      "   Final Test: 64.86\n",
      "Run 10:\n",
      "Highest Train: 63.22\n",
      "Highest Valid: 57.63\n",
      "  Final Train: 62.07\n",
      "   Final Test: 62.16\n",
      "All runs:\n",
      "Highest Train: 69.20 ± 5.13\n",
      "Highest Valid: 66.95 ± 6.45\n",
      "  Final Train: 65.63 ± 5.43\n",
      "   Final Test: 65.14 ± 7.90\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_channels': 16, 'dropout': 0.5, 'epochs': 50, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-4, 'lr': 0.01}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = WebKB(root='/tmp/Texas', name='Texas',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    #idx_train=[data.train_mask[i][0] for i in range(len(data.y))]\n",
    "    #train_idx = np.where(idx_train)[0]\n",
    "    #idx_val=[data.val_mask[i][0] for i in range(len(data.y))]\n",
    "    #valid_idx = np.where(idx_val)[0]\n",
    "    #idx_test=[data.test_mask[i][0] for i in range(len(data.y))]\n",
    "    #test_idx = np.where(idx_test)[0]\n",
    "    \n",
    "    model = GCN(data.num_features, args.hidden_channels,\n",
    "                    dataset.num_classes, args.num_layers,\n",
    "                    args.dropout,args.heads)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        idx_train=[data.train_mask[i][run] for i in range(len(data.y))]\n",
    "        train_idx = np.where(idx_train)[0]\n",
    "        idx_val=[data.val_mask[i][run] for i in range(len(data.y))]\n",
    "        valid_idx = np.where(idx_val)[0]\n",
    "        idx_test=[data.test_mask[i][run] for i in range(len(data.y))]\n",
    "        test_idx = np.where(idx_test)[0]\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                #print(f'Run: {run + 1:02d}, 'f'Epoch: {epoch:02d}, 'f'Loss: {loss:.4f}, 'f'Train: {100 * train_acc:.2f}%, '\n",
    "                 #     f'Valid: {100 * valid_acc:.2f}% '\n",
    "                  #    f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085c7fd",
   "metadata": {},
   "source": [
    "# Topological encodding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33e47b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], edge_index=[2, 325], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10])\n"
     ]
    }
   ],
   "source": [
    "dataset = WebKB(root='/tmp/Texas', name='Texas')\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52514bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Edge_idx=data.edge_index.numpy()\n",
    "Node=range(Number_nodes)\n",
    "Edgelist=[]\n",
    "for i in range(len(Edge_idx[1])):\n",
    "    Edgelist.append((Edge_idx[0][i],Edge_idx[1][i]))\n",
    "#print(Edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f9d236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a \"plain\" graph is undirected\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# give each a node a 'name', which is a letter in this case.\n",
    "#G.add_node('a')\n",
    "\n",
    "# the add_nodes_from method allows adding nodes from a sequence, in this case a list\n",
    "#nodes_to_add = ['b', 'c', 'd']\n",
    "G.add_nodes_from(Node)\n",
    "\n",
    "# add edge from 'a' to 'b'\n",
    "# since this graph is undirected, the order doesn't matter here\n",
    "#G.add_edge('a', 'b')\n",
    "\n",
    "# just like add_nodes_from, we can add edges from a sequence\n",
    "# edges should be specified as 2-tuples\n",
    "#edges_to_add = [('a', 'c'), ('b', 'c'), ('c', 'd')]\n",
    "G.add_edges_from(Edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781abc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77abd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Topological_Feature_subLevel(adj,filtration_fun, Filtration):\n",
    "        betti_0=[]\n",
    "        betti_1=[]\n",
    "        for p in range(len(Filtration)):\n",
    "            n_active = np.where(np.array(filtration_fun) <= Filtration[p])[0].tolist()\n",
    "            Active_node=np.unique(n_active)\n",
    "            if (len(Active_node)==0):\n",
    "                betti_0.append(0)\n",
    "                betti_1.append(0)\n",
    "            else:\n",
    "                b=adj[Active_node,:][:,Active_node]\n",
    "                my_flag=pyflagser.flagser_unweighted(b, min_dimension=0, max_dimension=2, directed=False, coeff=2, approximation=None)\n",
    "                x = my_flag[\"betti\"]\n",
    "                betti_0.append(x[0])\n",
    "                betti_1.append(x[1])\n",
    "            n_active.clear()\n",
    "        return betti_0,betti_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e40cacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Degree_list(Graph):\n",
    "    degree_list = [Graph.degree(node) for node in Graph.nodes]\n",
    "    return np.array(degree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "118b65fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  |  60 \n",
      "\n",
      "2  |  42 \n",
      "\n",
      "3  |  33 \n",
      "\n",
      "4  |  19 \n",
      "\n",
      "5  |  7 \n",
      "\n",
      "6  |  2 \n",
      "\n",
      "7  |  4 \n",
      "\n",
      "8  |  5 \n",
      "\n",
      "9  |  4 \n",
      "\n",
      "10  |  2 \n",
      "\n",
      "11  |  1 \n",
      "\n",
      "12  |  1 \n",
      "\n",
      "13  |  1 \n",
      "\n",
      "20  |  1 \n",
      "\n",
      "104  |  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "degree_list=Degree_list(G)\n",
    "unique_list=np.unique(degree_list)\n",
    "for d in unique_list:\n",
    "    count=0\n",
    "    for i in range(len(degree_list)):\n",
    "        if degree_list[i]==d:\n",
    "            count=count+1\n",
    "    print(int(d),\" | \",count,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7080881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 182 (100%)"
     ]
    }
   ],
   "source": [
    "import pyflagser\n",
    "Node_fil=[1,2,3,4,5,6,7,8,9,10,20,100]\n",
    "topo_betti_0=[]\n",
    "topo_betti_1=[]\n",
    "Node_Edge=[]\n",
    "for i in range(Number_nodes):\n",
    "    print(\"\\rProcessing file {} ({}%)\".format(i, 100*i//(Number_nodes-1)), end='', flush=True)\n",
    "    subgraph=ego_graph(G, i, radius=2, center=True, undirected=True, distance=None)\n",
    "    filt=Degree_list(subgraph)\n",
    "    A_sub = nx.to_numpy_array(subgraph)# adjacency matrix of subgraph\n",
    "    fe=Topological_Feature_subLevel(A_sub,filt,Node_fil)\n",
    "    topo_betti_0.append(fe[0])\n",
    "    topo_betti_1.append(fe[1])\n",
    "    Node_Edge.append([subgraph.number_of_nodes(),subgraph.number_of_edges()])\n",
    "    #topo_with_NE.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5892bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( Node_Edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49a08a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], adj_t=[183, 183, nnz=325])\n"
     ]
    }
   ],
   "source": [
    "dataset = WebKB(root='/tmp/Texas', name='Texas',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "445f9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_betti0=torch.tensor(topo_betti_0).float()\n",
    "topo_betti1=torch.tensor(topo_betti_1).float()\n",
    "NodeEdge=torch.tensor(Node_Edge).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbeea52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], adj_t=[183, 183, nnz=325], topo=[183, 24])\n"
     ]
    }
   ],
   "source": [
    "data.x=CC_domain\n",
    "topo_fe=torch.cat((topo_betti0,topo_betti1),1)\n",
    "data.topo=topo_fe\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51e354",
   "metadata": {},
   "source": [
    "# Topo-W-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd4668e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout,heads):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, cached=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.heads=heads\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x\n",
    "        #return x.log_softmax(dim=-1)\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters_mlp(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            #x = F.relu(x)\n",
    "            x=F.sigmoid(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        #return torch.log_softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "class MLP2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters_mlp2(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            #x = F.relu(x)\n",
    "            x=F.sigmoid(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n",
    "def train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2):\n",
    "    model.train()\n",
    "    mlp_model.train()\n",
    "    mlp_2.train()\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_mlp.zero_grad()\n",
    "    optimizer_mlp2.zero_grad()\n",
    "    gcn_embedding = model(data.x, data.adj_t)[train_idx]\n",
    "    #print(gcn_embedding)\n",
    "    mlp_embedding = mlp_model(data.topo[train_idx])\n",
    "    #print(mlp_embedding)\n",
    "    combined_embedding = torch.cat((gcn_embedding, mlp_embedding), dim=1)\n",
    "    #print(combined_embedding)\n",
    "    mlp_emb = mlp_2(combined_embedding)\n",
    "    #print(mlp_emb)\n",
    "    loss = F.nll_loss(mlp_emb, data.y.squeeze()[train_idx])\n",
    "    #loss = F.nll_loss(combined_embedding, data.y.squeeze()[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer_mlp2.step()\n",
    "    optimizer.step()\n",
    "    optimizer_mlp.step()\n",
    "    \n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def ACC(Prediction, Label):\n",
    "    correct = Prediction.view(-1).eq(Label).sum().item()\n",
    "    total=len(Label)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx):\n",
    "    model.eval()\n",
    "    mlp_model.eval()\n",
    "    mlp_2.eval()\n",
    "\n",
    "    gcn_out = model(data.x, data.adj_t)\n",
    "    #print(gcn_out[0])\n",
    "    mlp_out=mlp_model(data.topo)\n",
    "    #print(mlp_out)\n",
    "    #out=torch.cat((gcn_out,mlp_out),dim=1)\n",
    "    Com=torch.cat((gcn_out,mlp_out),dim=1)\n",
    "    out=mlp_2(Com)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    #print(y_pred[0])\n",
    "    y_pred=y_pred.view(-1)\n",
    "    train_acc=ACC(data.y[train_idx],y_pred[train_idx])\n",
    "    valid_acc=ACC(data.y[valid_idx],y_pred[valid_idx])\n",
    "    test_acc =ACC(data.y[test_idx],y_pred[test_idx])\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef21f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x16a68e290>\n",
      "Run 01:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 67.80\n",
      "  Final Train: 100.00\n",
      "   Final Test: 56.76\n",
      "Run 02:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 66.10\n",
      "  Final Train: 98.85\n",
      "   Final Test: 54.05\n",
      "Run 03:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 62.71\n",
      "  Final Train: 100.00\n",
      "   Final Test: 43.24\n",
      "Run 04:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 62.71\n",
      "  Final Train: 98.85\n",
      "   Final Test: 51.35\n",
      "Run 05:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 69.49\n",
      "  Final Train: 73.56\n",
      "   Final Test: 62.16\n",
      "Run 06:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 59.32\n",
      "  Final Train: 51.72\n",
      "   Final Test: 56.76\n",
      "Run 07:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 64.41\n",
      "  Final Train: 98.85\n",
      "   Final Test: 64.86\n",
      "Run 08:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 67.80\n",
      "  Final Train: 66.67\n",
      "   Final Test: 56.76\n",
      "Run 09:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 52.54\n",
      "  Final Train: 94.25\n",
      "   Final Test: 64.86\n",
      "Run 10:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 54.24\n",
      "  Final Train: 98.85\n",
      "   Final Test: 70.27\n",
      "All runs:\n",
      "Highest Train: 99.31 ± 0.59\n",
      "Highest Valid: 62.71 ± 5.76\n",
      "  Final Train: 88.16 ± 17.57\n",
      "   Final Test: 58.11 ± 7.78\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_channels': 16, 'dropout': 0.5, 'epochs': 200, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-4, 'lr': 0.01,'hidden_channels_mlp': 20,'dropout_mlp': 0.0,'num_layers_mlp': 3}\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = Planetoid(root='/tmp/cora', name='Cora',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    X = data.topo\n",
    "    y_true = data.y\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    model = GCN(data.num_features, args.hidden_channels,10, args.num_layers,args.dropout,args.heads)\n",
    "    mlp_model = MLP(X.size(-1), args.hidden_channels_mlp, 5,args.num_layers_mlp, args.dropout_mlp)\n",
    "    #print(mlp_model.parameters())\n",
    "    mlp_2 = MLP2(15, 100, dataset.num_classes,3, 0.0)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        idx_train=[data.train_mask[i][run] for i in range(len(data.y))]\n",
    "        train_idx = np.where(idx_train)[0]\n",
    "        idx_val=[data.val_mask[i][run] for i in range(len(data.y))]\n",
    "        valid_idx = np.where(idx_val)[0]\n",
    "        idx_test=[data.test_mask[i][run] for i in range(len(data.y))]\n",
    "        test_idx = np.where(idx_test)[0]\n",
    "        \n",
    "        model.reset_parameters()\n",
    "        mlp_model.reset_parameters_mlp()\n",
    "        mlp_2.reset_parameters_mlp2()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        optimizer_mlp=torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "        optimizer_mlp2=torch.optim.Adam(mlp_2.parameters(), lr=0.001)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2)\n",
    "            result = test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                #print(f'Run: {run + 1:02d}, 'f'Epoch: {epoch:02d}, 'f'Loss: {loss:.4f}, 'f'Train: {100 * train_acc:.2f}%, '\n",
    "                 #     f'Valid: {100 * valid_acc:.2f}% '\n",
    "                  #    f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7dc105",
   "metadata": {},
   "source": [
    "# Topo-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ceda79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], adj_t=[183, 183, nnz=325])\n"
     ]
    }
   ],
   "source": [
    "dataset = WebKB(root='/tmp/Texas', name='Texas',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4407be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10], adj_t=[183, 183, nnz=325], topo=[183, 24])\n"
     ]
    }
   ],
   "source": [
    "topo_fe=torch.cat((topo_betti0,topo_betti1),1)\n",
    "data.topo=topo_fe\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "189943fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x15fbe4730>\n",
      "Run: 01, Epoch: 01, Loss: 1.7170, Train: 16.09%, Valid: 25.42% Test: 10.81%\n",
      "Run: 01, Epoch: 02, Loss: 1.6392, Train: 16.09%, Valid: 25.42% Test: 10.81%\n",
      "Run: 01, Epoch: 03, Loss: 1.5740, Train: 16.09%, Valid: 25.42% Test: 10.81%\n",
      "Run: 01, Epoch: 04, Loss: 1.4845, Train: 16.09%, Valid: 25.42% Test: 10.81%\n",
      "Run: 01, Epoch: 05, Loss: 1.4288, Train: 16.09%, Valid: 25.42% Test: 10.81%\n",
      "Run: 01, Epoch: 06, Loss: 1.3525, Train: 16.09%, Valid: 25.42% Test: 10.81%\n",
      "Run: 01, Epoch: 07, Loss: 1.2665, Train: 16.09%, Valid: 25.42% Test: 10.81%\n",
      "Run: 01, Epoch: 08, Loss: 1.2116, Train: 16.09%, Valid: 25.42% Test: 10.81%\n",
      "Run: 01, Epoch: 09, Loss: 1.1632, Train: 43.68%, Valid: 42.37% Test: 18.92%\n",
      "Run: 01, Epoch: 10, Loss: 1.0839, Train: 78.16%, Valid: 55.93% Test: 51.35%\n",
      "Run: 01, Epoch: 11, Loss: 1.0408, Train: 83.91%, Valid: 49.15% Test: 51.35%\n",
      "Run: 01, Epoch: 12, Loss: 1.0141, Train: 81.61%, Valid: 49.15% Test: 59.46%\n",
      "Run: 01, Epoch: 13, Loss: 0.9560, Train: 80.46%, Valid: 50.85% Test: 62.16%\n",
      "Run: 01, Epoch: 14, Loss: 0.9289, Train: 80.46%, Valid: 50.85% Test: 62.16%\n",
      "Run: 01, Epoch: 15, Loss: 0.8810, Train: 80.46%, Valid: 52.54% Test: 56.76%\n",
      "Run: 01, Epoch: 16, Loss: 0.8584, Train: 80.46%, Valid: 50.85% Test: 59.46%\n",
      "Run: 01, Epoch: 17, Loss: 0.8194, Train: 80.46%, Valid: 50.85% Test: 59.46%\n",
      "Run: 01, Epoch: 18, Loss: 0.8227, Train: 80.46%, Valid: 49.15% Test: 59.46%\n",
      "Run: 01, Epoch: 19, Loss: 0.7931, Train: 81.61%, Valid: 49.15% Test: 59.46%\n",
      "Run: 01, Epoch: 20, Loss: 0.8213, Train: 81.61%, Valid: 49.15% Test: 56.76%\n",
      "Run: 01, Epoch: 21, Loss: 0.7642, Train: 81.61%, Valid: 49.15% Test: 56.76%\n",
      "Run: 01, Epoch: 22, Loss: 0.7853, Train: 81.61%, Valid: 49.15% Test: 59.46%\n",
      "Run: 01, Epoch: 23, Loss: 0.7250, Train: 82.76%, Valid: 49.15% Test: 59.46%\n",
      "Run: 01, Epoch: 24, Loss: 0.7338, Train: 82.76%, Valid: 49.15% Test: 62.16%\n",
      "Run: 01, Epoch: 25, Loss: 0.7117, Train: 82.76%, Valid: 52.54% Test: 62.16%\n",
      "Run: 01, Epoch: 26, Loss: 0.6753, Train: 83.91%, Valid: 50.85% Test: 62.16%\n",
      "Run: 01, Epoch: 27, Loss: 0.6983, Train: 83.91%, Valid: 50.85% Test: 62.16%\n",
      "Run: 01, Epoch: 28, Loss: 0.6968, Train: 83.91%, Valid: 50.85% Test: 62.16%\n",
      "Run: 01, Epoch: 29, Loss: 0.6565, Train: 83.91%, Valid: 50.85% Test: 59.46%\n",
      "Run: 01, Epoch: 30, Loss: 0.6780, Train: 83.91%, Valid: 50.85% Test: 59.46%\n",
      "Run: 01, Epoch: 31, Loss: 0.6626, Train: 83.91%, Valid: 50.85% Test: 56.76%\n",
      "Run: 01, Epoch: 32, Loss: 0.6690, Train: 82.76%, Valid: 54.24% Test: 56.76%\n",
      "Run: 01, Epoch: 33, Loss: 0.6495, Train: 82.76%, Valid: 52.54% Test: 56.76%\n",
      "Run: 01, Epoch: 34, Loss: 0.6565, Train: 82.76%, Valid: 52.54% Test: 56.76%\n",
      "Run: 01, Epoch: 35, Loss: 0.6215, Train: 82.76%, Valid: 50.85% Test: 59.46%\n",
      "Run: 01, Epoch: 36, Loss: 0.6248, Train: 83.91%, Valid: 50.85% Test: 56.76%\n",
      "Run: 01, Epoch: 37, Loss: 0.6286, Train: 83.91%, Valid: 50.85% Test: 56.76%\n",
      "Run: 01, Epoch: 38, Loss: 0.6298, Train: 85.06%, Valid: 50.85% Test: 56.76%\n",
      "Run: 01, Epoch: 39, Loss: 0.5864, Train: 86.21%, Valid: 52.54% Test: 51.35%\n",
      "Run: 01, Epoch: 40, Loss: 0.5706, Train: 86.21%, Valid: 52.54% Test: 54.05%\n",
      "Run: 01, Epoch: 41, Loss: 0.5614, Train: 86.21%, Valid: 52.54% Test: 54.05%\n",
      "Run: 01, Epoch: 42, Loss: 0.5732, Train: 86.21%, Valid: 52.54% Test: 54.05%\n",
      "Run: 01, Epoch: 43, Loss: 0.5742, Train: 86.21%, Valid: 52.54% Test: 54.05%\n",
      "Run: 01, Epoch: 44, Loss: 0.5592, Train: 87.36%, Valid: 52.54% Test: 54.05%\n",
      "Run: 01, Epoch: 45, Loss: 0.5616, Train: 87.36%, Valid: 52.54% Test: 54.05%\n",
      "Run: 01, Epoch: 46, Loss: 0.5834, Train: 87.36%, Valid: 52.54% Test: 54.05%\n",
      "Run: 01, Epoch: 47, Loss: 0.5665, Train: 87.36%, Valid: 50.85% Test: 54.05%\n",
      "Run: 01, Epoch: 48, Loss: 0.5515, Train: 87.36%, Valid: 47.46% Test: 54.05%\n",
      "Run: 01, Epoch: 49, Loss: 0.5108, Train: 86.21%, Valid: 50.85% Test: 54.05%\n",
      "Run: 01, Epoch: 50, Loss: 0.5367, Train: 88.51%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 51, Loss: 0.5398, Train: 87.36%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 52, Loss: 0.5709, Train: 87.36%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 53, Loss: 0.5294, Train: 87.36%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 54, Loss: 0.5228, Train: 89.66%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 55, Loss: 0.4921, Train: 89.66%, Valid: 49.15% Test: 45.95%\n",
      "Run: 01, Epoch: 56, Loss: 0.5016, Train: 88.51%, Valid: 49.15% Test: 48.65%\n",
      "Run: 01, Epoch: 57, Loss: 0.4524, Train: 88.51%, Valid: 49.15% Test: 48.65%\n",
      "Run: 01, Epoch: 58, Loss: 0.5040, Train: 89.66%, Valid: 50.85% Test: 48.65%\n",
      "Run: 01, Epoch: 59, Loss: 0.4822, Train: 90.80%, Valid: 50.85% Test: 48.65%\n",
      "Run: 01, Epoch: 60, Loss: 0.4976, Train: 90.80%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 61, Loss: 0.4893, Train: 90.80%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 62, Loss: 0.4574, Train: 91.95%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 63, Loss: 0.5011, Train: 90.80%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 64, Loss: 0.4387, Train: 90.80%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 65, Loss: 0.4537, Train: 91.95%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 66, Loss: 0.4429, Train: 91.95%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 67, Loss: 0.4648, Train: 91.95%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 68, Loss: 0.5036, Train: 91.95%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 69, Loss: 0.4914, Train: 91.95%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 70, Loss: 0.5255, Train: 91.95%, Valid: 52.54% Test: 45.95%\n",
      "Run: 01, Epoch: 71, Loss: 0.4381, Train: 91.95%, Valid: 52.54% Test: 48.65%\n",
      "Run: 01, Epoch: 72, Loss: 0.3940, Train: 91.95%, Valid: 52.54% Test: 45.95%\n",
      "Run: 01, Epoch: 73, Loss: 0.4314, Train: 94.25%, Valid: 52.54% Test: 45.95%\n",
      "Run: 01, Epoch: 74, Loss: 0.4489, Train: 94.25%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 75, Loss: 0.4280, Train: 94.25%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 76, Loss: 0.3908, Train: 95.40%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 77, Loss: 0.4288, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 78, Loss: 0.4216, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 79, Loss: 0.3881, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 80, Loss: 0.3930, Train: 95.40%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 81, Loss: 0.3872, Train: 95.40%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 82, Loss: 0.4502, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 83, Loss: 0.4905, Train: 94.25%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 84, Loss: 0.4445, Train: 94.25%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 85, Loss: 0.3859, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 86, Loss: 0.4428, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 87, Loss: 0.3703, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 88, Loss: 0.3786, Train: 95.40%, Valid: 49.15% Test: 43.24%\n",
      "Run: 01, Epoch: 89, Loss: 0.3748, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 90, Loss: 0.3799, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 91, Loss: 0.3179, Train: 95.40%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 92, Loss: 0.3573, Train: 95.40%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 93, Loss: 0.3983, Train: 95.40%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 94, Loss: 0.3923, Train: 95.40%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 95, Loss: 0.3272, Train: 96.55%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 96, Loss: 0.3996, Train: 97.70%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 97, Loss: 0.3447, Train: 96.55%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 98, Loss: 0.3289, Train: 96.55%, Valid: 50.85% Test: 45.95%\n",
      "Run: 01, Epoch: 99, Loss: 0.3246, Train: 96.55%, Valid: 52.54% Test: 45.95%\n",
      "Run: 01, Epoch: 100, Loss: 0.3497, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 01, Epoch: 101, Loss: 0.3604, Train: 95.40%, Valid: 52.54% Test: 43.24%\n",
      "Run: 01, Epoch: 102, Loss: 0.3732, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 01, Epoch: 103, Loss: 0.3403, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 01, Epoch: 104, Loss: 0.4242, Train: 96.55%, Valid: 49.15% Test: 43.24%\n",
      "Run: 01, Epoch: 105, Loss: 0.3120, Train: 96.55%, Valid: 49.15% Test: 43.24%\n",
      "Run: 01, Epoch: 106, Loss: 0.2998, Train: 96.55%, Valid: 49.15% Test: 43.24%\n",
      "Run: 01, Epoch: 107, Loss: 0.2756, Train: 96.55%, Valid: 47.46% Test: 43.24%\n",
      "Run: 01, Epoch: 108, Loss: 0.3087, Train: 96.55%, Valid: 47.46% Test: 43.24%\n",
      "Run: 01, Epoch: 109, Loss: 0.3310, Train: 96.55%, Valid: 47.46% Test: 43.24%\n",
      "Run: 01, Epoch: 110, Loss: 0.3282, Train: 95.40%, Valid: 47.46% Test: 43.24%\n",
      "Run: 01, Epoch: 111, Loss: 0.3237, Train: 95.40%, Valid: 47.46% Test: 43.24%\n",
      "Run: 01, Epoch: 112, Loss: 0.3984, Train: 95.40%, Valid: 47.46% Test: 43.24%\n",
      "Run: 01, Epoch: 113, Loss: 0.3391, Train: 95.40%, Valid: 47.46% Test: 43.24%\n",
      "Run: 01, Epoch: 114, Loss: 0.2810, Train: 95.40%, Valid: 49.15% Test: 43.24%\n",
      "Run: 01, Epoch: 115, Loss: 0.3301, Train: 97.70%, Valid: 49.15% Test: 43.24%\n",
      "Run: 01, Epoch: 116, Loss: 0.4016, Train: 97.70%, Valid: 49.15% Test: 43.24%\n",
      "Run: 01, Epoch: 117, Loss: 0.3075, Train: 97.70%, Valid: 50.85% Test: 43.24%\n",
      "Run: 01, Epoch: 118, Loss: 0.3058, Train: 98.85%, Valid: 50.85% Test: 40.54%\n",
      "Run: 01, Epoch: 119, Loss: 0.3166, Train: 98.85%, Valid: 50.85% Test: 40.54%\n",
      "Run: 01, Epoch: 120, Loss: 0.3431, Train: 97.70%, Valid: 50.85% Test: 40.54%\n",
      "Run: 01, Epoch: 121, Loss: 0.3225, Train: 97.70%, Valid: 52.54% Test: 37.84%\n",
      "Run: 01, Epoch: 122, Loss: 0.2694, Train: 96.55%, Valid: 52.54% Test: 37.84%\n",
      "Run: 01, Epoch: 123, Loss: 0.2823, Train: 97.70%, Valid: 52.54% Test: 37.84%\n",
      "Run: 01, Epoch: 124, Loss: 0.2980, Train: 96.55%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 125, Loss: 0.3546, Train: 96.55%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 126, Loss: 0.2333, Train: 96.55%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 127, Loss: 0.2516, Train: 97.70%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 128, Loss: 0.3450, Train: 97.70%, Valid: 47.46% Test: 37.84%\n",
      "Run: 01, Epoch: 129, Loss: 0.4069, Train: 100.00%, Valid: 47.46% Test: 35.14%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 130, Loss: 0.2639, Train: 100.00%, Valid: 44.07% Test: 35.14%\n",
      "Run: 01, Epoch: 131, Loss: 0.3584, Train: 100.00%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 132, Loss: 0.2871, Train: 100.00%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 133, Loss: 0.2874, Train: 98.85%, Valid: 45.76% Test: 32.43%\n",
      "Run: 01, Epoch: 134, Loss: 0.2586, Train: 98.85%, Valid: 45.76% Test: 29.73%\n",
      "Run: 01, Epoch: 135, Loss: 0.2896, Train: 98.85%, Valid: 44.07% Test: 29.73%\n",
      "Run: 01, Epoch: 136, Loss: 0.3131, Train: 98.85%, Valid: 44.07% Test: 32.43%\n",
      "Run: 01, Epoch: 137, Loss: 0.2094, Train: 98.85%, Valid: 44.07% Test: 32.43%\n",
      "Run: 01, Epoch: 138, Loss: 0.2830, Train: 97.70%, Valid: 45.76% Test: 32.43%\n",
      "Run: 01, Epoch: 139, Loss: 0.2708, Train: 96.55%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 140, Loss: 0.2400, Train: 96.55%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 141, Loss: 0.2260, Train: 94.25%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 142, Loss: 0.2125, Train: 94.25%, Valid: 44.07% Test: 35.14%\n",
      "Run: 01, Epoch: 143, Loss: 0.2564, Train: 94.25%, Valid: 44.07% Test: 35.14%\n",
      "Run: 01, Epoch: 144, Loss: 0.2868, Train: 94.25%, Valid: 44.07% Test: 35.14%\n",
      "Run: 01, Epoch: 145, Loss: 0.3262, Train: 95.40%, Valid: 44.07% Test: 35.14%\n",
      "Run: 01, Epoch: 146, Loss: 0.3093, Train: 95.40%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 147, Loss: 0.2598, Train: 95.40%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 148, Loss: 0.2241, Train: 97.70%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 149, Loss: 0.2766, Train: 97.70%, Valid: 45.76% Test: 37.84%\n",
      "Run: 01, Epoch: 150, Loss: 0.2610, Train: 97.70%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 151, Loss: 0.2371, Train: 97.70%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 152, Loss: 0.2648, Train: 98.85%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 153, Loss: 0.3213, Train: 98.85%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 154, Loss: 0.2364, Train: 98.85%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 155, Loss: 0.2759, Train: 97.70%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 156, Loss: 0.2359, Train: 97.70%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 157, Loss: 0.2469, Train: 97.70%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 158, Loss: 0.2574, Train: 97.70%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 159, Loss: 0.2586, Train: 97.70%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 160, Loss: 0.2634, Train: 97.70%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 161, Loss: 0.2614, Train: 98.85%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 162, Loss: 0.2900, Train: 98.85%, Valid: 50.85% Test: 37.84%\n",
      "Run: 01, Epoch: 163, Loss: 0.2897, Train: 98.85%, Valid: 47.46% Test: 37.84%\n",
      "Run: 01, Epoch: 164, Loss: 0.2592, Train: 100.00%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 165, Loss: 0.2763, Train: 100.00%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 166, Loss: 0.2013, Train: 98.85%, Valid: 47.46% Test: 37.84%\n",
      "Run: 01, Epoch: 167, Loss: 0.2098, Train: 98.85%, Valid: 45.76% Test: 37.84%\n",
      "Run: 01, Epoch: 168, Loss: 0.2203, Train: 98.85%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 169, Loss: 0.2618, Train: 97.70%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 170, Loss: 0.2424, Train: 97.70%, Valid: 47.46% Test: 32.43%\n",
      "Run: 01, Epoch: 171, Loss: 0.2335, Train: 97.70%, Valid: 47.46% Test: 32.43%\n",
      "Run: 01, Epoch: 172, Loss: 0.2780, Train: 97.70%, Valid: 47.46% Test: 32.43%\n",
      "Run: 01, Epoch: 173, Loss: 0.2349, Train: 97.70%, Valid: 47.46% Test: 32.43%\n",
      "Run: 01, Epoch: 174, Loss: 0.2727, Train: 97.70%, Valid: 49.15% Test: 32.43%\n",
      "Run: 01, Epoch: 175, Loss: 0.1717, Train: 97.70%, Valid: 47.46% Test: 32.43%\n",
      "Run: 01, Epoch: 176, Loss: 0.2216, Train: 97.70%, Valid: 50.85% Test: 35.14%\n",
      "Run: 01, Epoch: 177, Loss: 0.2462, Train: 97.70%, Valid: 50.85% Test: 35.14%\n",
      "Run: 01, Epoch: 178, Loss: 0.1897, Train: 97.70%, Valid: 50.85% Test: 35.14%\n",
      "Run: 01, Epoch: 179, Loss: 0.3159, Train: 97.70%, Valid: 49.15% Test: 35.14%\n",
      "Run: 01, Epoch: 180, Loss: 0.2652, Train: 97.70%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 181, Loss: 0.2251, Train: 98.85%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 182, Loss: 0.2641, Train: 98.85%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 183, Loss: 0.2338, Train: 98.85%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 184, Loss: 0.2191, Train: 98.85%, Valid: 49.15% Test: 35.14%\n",
      "Run: 01, Epoch: 185, Loss: 0.3127, Train: 98.85%, Valid: 49.15% Test: 37.84%\n",
      "Run: 01, Epoch: 186, Loss: 0.2271, Train: 98.85%, Valid: 47.46% Test: 37.84%\n",
      "Run: 01, Epoch: 187, Loss: 0.2338, Train: 98.85%, Valid: 47.46% Test: 37.84%\n",
      "Run: 01, Epoch: 188, Loss: 0.2134, Train: 98.85%, Valid: 45.76% Test: 37.84%\n",
      "Run: 01, Epoch: 189, Loss: 0.1820, Train: 98.85%, Valid: 45.76% Test: 37.84%\n",
      "Run: 01, Epoch: 190, Loss: 0.2475, Train: 98.85%, Valid: 47.46% Test: 37.84%\n",
      "Run: 01, Epoch: 191, Loss: 0.2404, Train: 98.85%, Valid: 47.46% Test: 37.84%\n",
      "Run: 01, Epoch: 192, Loss: 0.2253, Train: 97.70%, Valid: 47.46% Test: 37.84%\n",
      "Run: 01, Epoch: 193, Loss: 0.2091, Train: 97.70%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 194, Loss: 0.1955, Train: 97.70%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 195, Loss: 0.1998, Train: 97.70%, Valid: 44.07% Test: 35.14%\n",
      "Run: 01, Epoch: 196, Loss: 0.1380, Train: 98.85%, Valid: 45.76% Test: 35.14%\n",
      "Run: 01, Epoch: 197, Loss: 0.2117, Train: 98.85%, Valid: 47.46% Test: 35.14%\n",
      "Run: 01, Epoch: 198, Loss: 0.1975, Train: 98.85%, Valid: 49.15% Test: 35.14%\n",
      "Run: 01, Epoch: 199, Loss: 0.1726, Train: 98.85%, Valid: 50.85% Test: 35.14%\n",
      "Run: 01, Epoch: 200, Loss: 0.2120, Train: 98.85%, Valid: 49.15% Test: 37.84%\n",
      "Run 01:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 55.93\n",
      "  Final Train: 78.16\n",
      "   Final Test: 51.35\n",
      "Run: 02, Epoch: 01, Loss: 2.0422, Train: 8.05%, Valid: 15.25% Test: 5.41%\n",
      "Run: 02, Epoch: 02, Loss: 1.9473, Train: 8.05%, Valid: 15.25% Test: 5.41%\n",
      "Run: 02, Epoch: 03, Loss: 1.8736, Train: 8.05%, Valid: 15.25% Test: 5.41%\n",
      "Run: 02, Epoch: 04, Loss: 1.7696, Train: 8.05%, Valid: 15.25% Test: 5.41%\n",
      "Run: 02, Epoch: 05, Loss: 1.6982, Train: 8.05%, Valid: 15.25% Test: 5.41%\n",
      "Run: 02, Epoch: 06, Loss: 1.6256, Train: 8.05%, Valid: 15.25% Test: 5.41%\n",
      "Run: 02, Epoch: 07, Loss: 1.5545, Train: 8.05%, Valid: 15.25% Test: 5.41%\n",
      "Run: 02, Epoch: 08, Loss: 1.4802, Train: 12.64%, Valid: 15.25% Test: 5.41%\n",
      "Run: 02, Epoch: 09, Loss: 1.4115, Train: 51.72%, Valid: 45.76% Test: 54.05%\n",
      "Run: 02, Epoch: 10, Loss: 1.3905, Train: 51.72%, Valid: 50.85% Test: 56.76%\n",
      "Run: 02, Epoch: 11, Loss: 1.3472, Train: 56.32%, Valid: 54.24% Test: 62.16%\n",
      "Run: 02, Epoch: 12, Loss: 1.2104, Train: 58.62%, Valid: 54.24% Test: 62.16%\n",
      "Run: 02, Epoch: 13, Loss: 1.2120, Train: 60.92%, Valid: 50.85% Test: 62.16%\n",
      "Run: 02, Epoch: 14, Loss: 1.1756, Train: 60.92%, Valid: 49.15% Test: 62.16%\n",
      "Run: 02, Epoch: 15, Loss: 1.1168, Train: 63.22%, Valid: 49.15% Test: 59.46%\n",
      "Run: 02, Epoch: 16, Loss: 1.0936, Train: 63.22%, Valid: 47.46% Test: 56.76%\n",
      "Run: 02, Epoch: 17, Loss: 1.0482, Train: 64.37%, Valid: 47.46% Test: 56.76%\n",
      "Run: 02, Epoch: 18, Loss: 1.0302, Train: 65.52%, Valid: 45.76% Test: 56.76%\n",
      "Run: 02, Epoch: 19, Loss: 1.0099, Train: 70.11%, Valid: 50.85% Test: 56.76%\n",
      "Run: 02, Epoch: 20, Loss: 0.9641, Train: 68.97%, Valid: 47.46% Test: 59.46%\n",
      "Run: 02, Epoch: 21, Loss: 1.0300, Train: 68.97%, Valid: 47.46% Test: 56.76%\n",
      "Run: 02, Epoch: 22, Loss: 0.9806, Train: 67.82%, Valid: 49.15% Test: 56.76%\n",
      "Run: 02, Epoch: 23, Loss: 0.9424, Train: 66.67%, Valid: 45.76% Test: 56.76%\n",
      "Run: 02, Epoch: 24, Loss: 0.9355, Train: 67.82%, Valid: 45.76% Test: 56.76%\n",
      "Run: 02, Epoch: 25, Loss: 0.8376, Train: 67.82%, Valid: 45.76% Test: 56.76%\n",
      "Run: 02, Epoch: 26, Loss: 0.8380, Train: 67.82%, Valid: 45.76% Test: 59.46%\n",
      "Run: 02, Epoch: 27, Loss: 0.8364, Train: 68.97%, Valid: 44.07% Test: 59.46%\n",
      "Run: 02, Epoch: 28, Loss: 0.8315, Train: 68.97%, Valid: 44.07% Test: 59.46%\n",
      "Run: 02, Epoch: 29, Loss: 0.8724, Train: 72.41%, Valid: 44.07% Test: 56.76%\n",
      "Run: 02, Epoch: 30, Loss: 0.8114, Train: 75.86%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 31, Loss: 0.8813, Train: 75.86%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 32, Loss: 0.8011, Train: 75.86%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 33, Loss: 0.7881, Train: 77.01%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 34, Loss: 0.7309, Train: 77.01%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 35, Loss: 0.8036, Train: 78.16%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 36, Loss: 0.7885, Train: 79.31%, Valid: 49.15% Test: 54.05%\n",
      "Run: 02, Epoch: 37, Loss: 0.7352, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 38, Loss: 0.7380, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 39, Loss: 0.7519, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 40, Loss: 0.7248, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 41, Loss: 0.7443, Train: 80.46%, Valid: 45.76% Test: 54.05%\n",
      "Run: 02, Epoch: 42, Loss: 0.6814, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 43, Loss: 0.6993, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 44, Loss: 0.7213, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 45, Loss: 0.6823, Train: 80.46%, Valid: 49.15% Test: 54.05%\n",
      "Run: 02, Epoch: 46, Loss: 0.6518, Train: 80.46%, Valid: 49.15% Test: 54.05%\n",
      "Run: 02, Epoch: 47, Loss: 0.6707, Train: 80.46%, Valid: 50.85% Test: 54.05%\n",
      "Run: 02, Epoch: 48, Loss: 0.6811, Train: 80.46%, Valid: 49.15% Test: 54.05%\n",
      "Run: 02, Epoch: 49, Loss: 0.6367, Train: 80.46%, Valid: 50.85% Test: 54.05%\n",
      "Run: 02, Epoch: 50, Loss: 0.6290, Train: 81.61%, Valid: 50.85% Test: 54.05%\n",
      "Run: 02, Epoch: 51, Loss: 0.6447, Train: 81.61%, Valid: 52.54% Test: 54.05%\n",
      "Run: 02, Epoch: 52, Loss: 0.6080, Train: 82.76%, Valid: 52.54% Test: 54.05%\n",
      "Run: 02, Epoch: 53, Loss: 0.6479, Train: 81.61%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 54, Loss: 0.6621, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 55, Loss: 0.7044, Train: 80.46%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 56, Loss: 0.6311, Train: 81.61%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 57, Loss: 0.6570, Train: 82.76%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 58, Loss: 0.6411, Train: 81.61%, Valid: 45.76% Test: 56.76%\n",
      "Run: 02, Epoch: 59, Loss: 0.6097, Train: 82.76%, Valid: 42.37% Test: 56.76%\n",
      "Run: 02, Epoch: 60, Loss: 0.6387, Train: 82.76%, Valid: 42.37% Test: 54.05%\n",
      "Run: 02, Epoch: 61, Loss: 0.6424, Train: 83.91%, Valid: 42.37% Test: 54.05%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 02, Epoch: 62, Loss: 0.6062, Train: 83.91%, Valid: 42.37% Test: 54.05%\n",
      "Run: 02, Epoch: 63, Loss: 0.6040, Train: 83.91%, Valid: 44.07% Test: 54.05%\n",
      "Run: 02, Epoch: 64, Loss: 0.5887, Train: 85.06%, Valid: 44.07% Test: 54.05%\n",
      "Run: 02, Epoch: 65, Loss: 0.5794, Train: 85.06%, Valid: 44.07% Test: 54.05%\n",
      "Run: 02, Epoch: 66, Loss: 0.5855, Train: 85.06%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 67, Loss: 0.5442, Train: 83.91%, Valid: 47.46% Test: 54.05%\n",
      "Run: 02, Epoch: 68, Loss: 0.5739, Train: 87.36%, Valid: 47.46% Test: 59.46%\n",
      "Run: 02, Epoch: 69, Loss: 0.5330, Train: 87.36%, Valid: 45.76% Test: 59.46%\n",
      "Run: 02, Epoch: 70, Loss: 0.5849, Train: 87.36%, Valid: 44.07% Test: 56.76%\n",
      "Run: 02, Epoch: 71, Loss: 0.5491, Train: 87.36%, Valid: 42.37% Test: 56.76%\n",
      "Run: 02, Epoch: 72, Loss: 0.5489, Train: 87.36%, Valid: 42.37% Test: 56.76%\n",
      "Run: 02, Epoch: 73, Loss: 0.5711, Train: 88.51%, Valid: 40.68% Test: 56.76%\n",
      "Run: 02, Epoch: 74, Loss: 0.5582, Train: 89.66%, Valid: 42.37% Test: 56.76%\n",
      "Run: 02, Epoch: 75, Loss: 0.5411, Train: 88.51%, Valid: 45.76% Test: 56.76%\n",
      "Run: 02, Epoch: 76, Loss: 0.5383, Train: 88.51%, Valid: 45.76% Test: 51.35%\n",
      "Run: 02, Epoch: 77, Loss: 0.5313, Train: 86.21%, Valid: 45.76% Test: 51.35%\n",
      "Run: 02, Epoch: 78, Loss: 0.5189, Train: 87.36%, Valid: 44.07% Test: 51.35%\n",
      "Run: 02, Epoch: 79, Loss: 0.4855, Train: 87.36%, Valid: 44.07% Test: 48.65%\n",
      "Run: 02, Epoch: 80, Loss: 0.5794, Train: 86.21%, Valid: 44.07% Test: 48.65%\n",
      "Run: 02, Epoch: 81, Loss: 0.5375, Train: 86.21%, Valid: 44.07% Test: 51.35%\n",
      "Run: 02, Epoch: 82, Loss: 0.4917, Train: 86.21%, Valid: 44.07% Test: 51.35%\n",
      "Run: 02, Epoch: 83, Loss: 0.5174, Train: 86.21%, Valid: 44.07% Test: 51.35%\n",
      "Run: 02, Epoch: 84, Loss: 0.5174, Train: 89.66%, Valid: 44.07% Test: 51.35%\n",
      "Run: 02, Epoch: 85, Loss: 0.5565, Train: 89.66%, Valid: 44.07% Test: 51.35%\n",
      "Run: 02, Epoch: 86, Loss: 0.4789, Train: 89.66%, Valid: 42.37% Test: 51.35%\n",
      "Run: 02, Epoch: 87, Loss: 0.5275, Train: 89.66%, Valid: 40.68% Test: 51.35%\n",
      "Run: 02, Epoch: 88, Loss: 0.5344, Train: 88.51%, Valid: 40.68% Test: 51.35%\n",
      "Run: 02, Epoch: 89, Loss: 0.5033, Train: 88.51%, Valid: 42.37% Test: 51.35%\n",
      "Run: 02, Epoch: 90, Loss: 0.4851, Train: 90.80%, Valid: 42.37% Test: 51.35%\n",
      "Run: 02, Epoch: 91, Loss: 0.4937, Train: 89.66%, Valid: 42.37% Test: 51.35%\n",
      "Run: 02, Epoch: 92, Loss: 0.4512, Train: 89.66%, Valid: 42.37% Test: 51.35%\n",
      "Run: 02, Epoch: 93, Loss: 0.4939, Train: 89.66%, Valid: 42.37% Test: 54.05%\n",
      "Run: 02, Epoch: 94, Loss: 0.4756, Train: 89.66%, Valid: 42.37% Test: 54.05%\n",
      "Run: 02, Epoch: 95, Loss: 0.4808, Train: 89.66%, Valid: 42.37% Test: 56.76%\n",
      "Run: 02, Epoch: 96, Loss: 0.4680, Train: 90.80%, Valid: 44.07% Test: 56.76%\n",
      "Run: 02, Epoch: 97, Loss: 0.4670, Train: 90.80%, Valid: 44.07% Test: 56.76%\n",
      "Run: 02, Epoch: 98, Loss: 0.5026, Train: 90.80%, Valid: 44.07% Test: 56.76%\n",
      "Run: 02, Epoch: 99, Loss: 0.4755, Train: 90.80%, Valid: 44.07% Test: 59.46%\n",
      "Run: 02, Epoch: 100, Loss: 0.4844, Train: 90.80%, Valid: 44.07% Test: 56.76%\n",
      "Run: 02, Epoch: 101, Loss: 0.4787, Train: 90.80%, Valid: 42.37% Test: 56.76%\n",
      "Run: 02, Epoch: 102, Loss: 0.4605, Train: 89.66%, Valid: 40.68% Test: 56.76%\n",
      "Run: 02, Epoch: 103, Loss: 0.4840, Train: 90.80%, Valid: 40.68% Test: 51.35%\n",
      "Run: 02, Epoch: 104, Loss: 0.4086, Train: 88.51%, Valid: 40.68% Test: 51.35%\n",
      "Run: 02, Epoch: 105, Loss: 0.4301, Train: 88.51%, Valid: 42.37% Test: 51.35%\n",
      "Run: 02, Epoch: 106, Loss: 0.4173, Train: 88.51%, Valid: 42.37% Test: 51.35%\n",
      "Run: 02, Epoch: 107, Loss: 0.4147, Train: 88.51%, Valid: 42.37% Test: 51.35%\n",
      "Run: 02, Epoch: 108, Loss: 0.4255, Train: 91.95%, Valid: 40.68% Test: 51.35%\n",
      "Run: 02, Epoch: 109, Loss: 0.3976, Train: 93.10%, Valid: 42.37% Test: 54.05%\n",
      "Run: 02, Epoch: 110, Loss: 0.4655, Train: 93.10%, Valid: 40.68% Test: 54.05%\n",
      "Run: 02, Epoch: 111, Loss: 0.3878, Train: 93.10%, Valid: 37.29% Test: 54.05%\n",
      "Run: 02, Epoch: 112, Loss: 0.3892, Train: 93.10%, Valid: 37.29% Test: 54.05%\n",
      "Run: 02, Epoch: 113, Loss: 0.4861, Train: 93.10%, Valid: 35.59% Test: 54.05%\n",
      "Run: 02, Epoch: 114, Loss: 0.4103, Train: 93.10%, Valid: 37.29% Test: 54.05%\n",
      "Run: 02, Epoch: 115, Loss: 0.4165, Train: 91.95%, Valid: 37.29% Test: 54.05%\n",
      "Run: 02, Epoch: 116, Loss: 0.4518, Train: 93.10%, Valid: 37.29% Test: 54.05%\n",
      "Run: 02, Epoch: 117, Loss: 0.4084, Train: 94.25%, Valid: 38.98% Test: 54.05%\n",
      "Run: 02, Epoch: 118, Loss: 0.4249, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 02, Epoch: 119, Loss: 0.4207, Train: 94.25%, Valid: 38.98% Test: 54.05%\n",
      "Run: 02, Epoch: 120, Loss: 0.4395, Train: 94.25%, Valid: 40.68% Test: 54.05%\n",
      "Run: 02, Epoch: 121, Loss: 0.4112, Train: 93.10%, Valid: 40.68% Test: 54.05%\n",
      "Run: 02, Epoch: 122, Loss: 0.4421, Train: 94.25%, Valid: 40.68% Test: 54.05%\n",
      "Run: 02, Epoch: 123, Loss: 0.3883, Train: 93.10%, Valid: 38.98% Test: 51.35%\n",
      "Run: 02, Epoch: 124, Loss: 0.3757, Train: 93.10%, Valid: 38.98% Test: 51.35%\n",
      "Run: 02, Epoch: 125, Loss: 0.3981, Train: 93.10%, Valid: 38.98% Test: 51.35%\n",
      "Run: 02, Epoch: 126, Loss: 0.3991, Train: 93.10%, Valid: 38.98% Test: 54.05%\n",
      "Run: 02, Epoch: 127, Loss: 0.3962, Train: 94.25%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 128, Loss: 0.5197, Train: 94.25%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 129, Loss: 0.3236, Train: 94.25%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 130, Loss: 0.3292, Train: 94.25%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 131, Loss: 0.3497, Train: 94.25%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 132, Loss: 0.3418, Train: 94.25%, Valid: 37.29% Test: 62.16%\n",
      "Run: 02, Epoch: 133, Loss: 0.4128, Train: 94.25%, Valid: 37.29% Test: 56.76%\n",
      "Run: 02, Epoch: 134, Loss: 0.4088, Train: 94.25%, Valid: 40.68% Test: 56.76%\n",
      "Run: 02, Epoch: 135, Loss: 0.3907, Train: 95.40%, Valid: 40.68% Test: 54.05%\n",
      "Run: 02, Epoch: 136, Loss: 0.3894, Train: 95.40%, Valid: 40.68% Test: 54.05%\n",
      "Run: 02, Epoch: 137, Loss: 0.4159, Train: 94.25%, Valid: 40.68% Test: 51.35%\n",
      "Run: 02, Epoch: 138, Loss: 0.3501, Train: 94.25%, Valid: 40.68% Test: 51.35%\n",
      "Run: 02, Epoch: 139, Loss: 0.3682, Train: 94.25%, Valid: 35.59% Test: 51.35%\n",
      "Run: 02, Epoch: 140, Loss: 0.3839, Train: 94.25%, Valid: 35.59% Test: 54.05%\n",
      "Run: 02, Epoch: 141, Loss: 0.4414, Train: 94.25%, Valid: 35.59% Test: 54.05%\n",
      "Run: 02, Epoch: 142, Loss: 0.4052, Train: 94.25%, Valid: 35.59% Test: 59.46%\n",
      "Run: 02, Epoch: 143, Loss: 0.4447, Train: 94.25%, Valid: 35.59% Test: 59.46%\n",
      "Run: 02, Epoch: 144, Loss: 0.3680, Train: 95.40%, Valid: 33.90% Test: 59.46%\n",
      "Run: 02, Epoch: 145, Loss: 0.3486, Train: 95.40%, Valid: 33.90% Test: 56.76%\n",
      "Run: 02, Epoch: 146, Loss: 0.3646, Train: 95.40%, Valid: 33.90% Test: 56.76%\n",
      "Run: 02, Epoch: 147, Loss: 0.4050, Train: 94.25%, Valid: 33.90% Test: 54.05%\n",
      "Run: 02, Epoch: 148, Loss: 0.4290, Train: 94.25%, Valid: 33.90% Test: 56.76%\n",
      "Run: 02, Epoch: 149, Loss: 0.3561, Train: 94.25%, Valid: 33.90% Test: 56.76%\n",
      "Run: 02, Epoch: 150, Loss: 0.3373, Train: 94.25%, Valid: 33.90% Test: 56.76%\n",
      "Run: 02, Epoch: 151, Loss: 0.3352, Train: 94.25%, Valid: 33.90% Test: 54.05%\n",
      "Run: 02, Epoch: 152, Loss: 0.4114, Train: 94.25%, Valid: 33.90% Test: 54.05%\n",
      "Run: 02, Epoch: 153, Loss: 0.3457, Train: 95.40%, Valid: 35.59% Test: 54.05%\n",
      "Run: 02, Epoch: 154, Loss: 0.3324, Train: 94.25%, Valid: 35.59% Test: 54.05%\n",
      "Run: 02, Epoch: 155, Loss: 0.3131, Train: 94.25%, Valid: 35.59% Test: 54.05%\n",
      "Run: 02, Epoch: 156, Loss: 0.3497, Train: 94.25%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 157, Loss: 0.4040, Train: 94.25%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 158, Loss: 0.3246, Train: 94.25%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 159, Loss: 0.3449, Train: 94.25%, Valid: 38.98% Test: 54.05%\n",
      "Run: 02, Epoch: 160, Loss: 0.3418, Train: 94.25%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 161, Loss: 0.3437, Train: 95.40%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 162, Loss: 0.3328, Train: 95.40%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 163, Loss: 0.3448, Train: 95.40%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 164, Loss: 0.2947, Train: 95.40%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 165, Loss: 0.3020, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 02, Epoch: 166, Loss: 0.2848, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 02, Epoch: 167, Loss: 0.3151, Train: 96.55%, Valid: 38.98% Test: 54.05%\n",
      "Run: 02, Epoch: 168, Loss: 0.3148, Train: 96.55%, Valid: 40.68% Test: 54.05%\n",
      "Run: 02, Epoch: 169, Loss: 0.3113, Train: 96.55%, Valid: 40.68% Test: 56.76%\n",
      "Run: 02, Epoch: 170, Loss: 0.4078, Train: 95.40%, Valid: 38.98% Test: 56.76%\n",
      "Run: 02, Epoch: 171, Loss: 0.4157, Train: 95.40%, Valid: 37.29% Test: 56.76%\n",
      "Run: 02, Epoch: 172, Loss: 0.3785, Train: 95.40%, Valid: 37.29% Test: 56.76%\n",
      "Run: 02, Epoch: 173, Loss: 0.3640, Train: 95.40%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 174, Loss: 0.3194, Train: 94.25%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 175, Loss: 0.3256, Train: 94.25%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 176, Loss: 0.3541, Train: 94.25%, Valid: 37.29% Test: 54.05%\n",
      "Run: 02, Epoch: 177, Loss: 0.3398, Train: 94.25%, Valid: 37.29% Test: 54.05%\n",
      "Run: 02, Epoch: 178, Loss: 0.3165, Train: 94.25%, Valid: 37.29% Test: 51.35%\n",
      "Run: 02, Epoch: 179, Loss: 0.3599, Train: 94.25%, Valid: 37.29% Test: 51.35%\n",
      "Run: 02, Epoch: 180, Loss: 0.3052, Train: 94.25%, Valid: 37.29% Test: 54.05%\n",
      "Run: 02, Epoch: 181, Loss: 0.3025, Train: 94.25%, Valid: 37.29% Test: 51.35%\n",
      "Run: 02, Epoch: 182, Loss: 0.3268, Train: 94.25%, Valid: 37.29% Test: 51.35%\n",
      "Run: 02, Epoch: 183, Loss: 0.3188, Train: 94.25%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 184, Loss: 0.3916, Train: 94.25%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 185, Loss: 0.3386, Train: 94.25%, Valid: 35.59% Test: 56.76%\n",
      "Run: 02, Epoch: 186, Loss: 0.3570, Train: 94.25%, Valid: 37.29% Test: 56.76%\n",
      "Run: 02, Epoch: 187, Loss: 0.2915, Train: 95.40%, Valid: 38.98% Test: 59.46%\n",
      "Run: 02, Epoch: 188, Loss: 0.3331, Train: 95.40%, Valid: 40.68% Test: 59.46%\n",
      "Run: 02, Epoch: 189, Loss: 0.2540, Train: 95.40%, Valid: 40.68% Test: 59.46%\n",
      "Run: 02, Epoch: 190, Loss: 0.3651, Train: 95.40%, Valid: 40.68% Test: 59.46%\n",
      "Run: 02, Epoch: 191, Loss: 0.3230, Train: 96.55%, Valid: 38.98% Test: 59.46%\n",
      "Run: 02, Epoch: 192, Loss: 0.2944, Train: 96.55%, Valid: 38.98% Test: 59.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 02, Epoch: 193, Loss: 0.3308, Train: 96.55%, Valid: 38.98% Test: 59.46%\n",
      "Run: 02, Epoch: 194, Loss: 0.3158, Train: 95.40%, Valid: 38.98% Test: 59.46%\n",
      "Run: 02, Epoch: 195, Loss: 0.2522, Train: 95.40%, Valid: 38.98% Test: 59.46%\n",
      "Run: 02, Epoch: 196, Loss: 0.3480, Train: 95.40%, Valid: 38.98% Test: 59.46%\n",
      "Run: 02, Epoch: 197, Loss: 0.3270, Train: 95.40%, Valid: 38.98% Test: 59.46%\n",
      "Run: 02, Epoch: 198, Loss: 0.3637, Train: 95.40%, Valid: 38.98% Test: 59.46%\n",
      "Run: 02, Epoch: 199, Loss: 0.2653, Train: 95.40%, Valid: 38.98% Test: 62.16%\n",
      "Run: 02, Epoch: 200, Loss: 0.2912, Train: 94.25%, Valid: 38.98% Test: 62.16%\n",
      "Run 02:\n",
      "Highest Train: 96.55\n",
      "Highest Valid: 54.24\n",
      "  Final Train: 56.32\n",
      "   Final Test: 62.16\n",
      "Run: 03, Epoch: 01, Loss: 1.6081, Train: 0.00%, Valid: 0.00% Test: 2.70%\n",
      "Run: 03, Epoch: 02, Loss: 1.5186, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 03, Loss: 1.4498, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 04, Loss: 1.3795, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 05, Loss: 1.3270, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 06, Loss: 1.2626, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 07, Loss: 1.1886, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 08, Loss: 1.1365, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 09, Loss: 1.1306, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 10, Loss: 1.0163, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 11, Loss: 0.9904, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 12, Loss: 0.9700, Train: 58.62%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 13, Loss: 0.8853, Train: 60.92%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 14, Loss: 0.8763, Train: 62.07%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 15, Loss: 0.8069, Train: 62.07%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 16, Loss: 0.8287, Train: 62.07%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 17, Loss: 0.7863, Train: 63.22%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 18, Loss: 0.7478, Train: 63.22%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 19, Loss: 0.7224, Train: 65.52%, Valid: 54.24% Test: 48.65%\n",
      "Run: 03, Epoch: 20, Loss: 0.6915, Train: 66.67%, Valid: 55.93% Test: 48.65%\n",
      "Run: 03, Epoch: 21, Loss: 0.7295, Train: 68.97%, Valid: 55.93% Test: 48.65%\n",
      "Run: 03, Epoch: 22, Loss: 0.7010, Train: 72.41%, Valid: 55.93% Test: 51.35%\n",
      "Run: 03, Epoch: 23, Loss: 0.6985, Train: 73.56%, Valid: 55.93% Test: 51.35%\n",
      "Run: 03, Epoch: 24, Loss: 0.6717, Train: 77.01%, Valid: 55.93% Test: 45.95%\n",
      "Run: 03, Epoch: 25, Loss: 0.6435, Train: 80.46%, Valid: 55.93% Test: 43.24%\n",
      "Run: 03, Epoch: 26, Loss: 0.6302, Train: 82.76%, Valid: 54.24% Test: 40.54%\n",
      "Run: 03, Epoch: 27, Loss: 0.6541, Train: 83.91%, Valid: 54.24% Test: 40.54%\n",
      "Run: 03, Epoch: 28, Loss: 0.6177, Train: 83.91%, Valid: 52.54% Test: 40.54%\n",
      "Run: 03, Epoch: 29, Loss: 0.5889, Train: 85.06%, Valid: 52.54% Test: 40.54%\n",
      "Run: 03, Epoch: 30, Loss: 0.6106, Train: 85.06%, Valid: 54.24% Test: 40.54%\n",
      "Run: 03, Epoch: 31, Loss: 0.5838, Train: 85.06%, Valid: 49.15% Test: 37.84%\n",
      "Run: 03, Epoch: 32, Loss: 0.5629, Train: 86.21%, Valid: 49.15% Test: 37.84%\n",
      "Run: 03, Epoch: 33, Loss: 0.5614, Train: 87.36%, Valid: 47.46% Test: 37.84%\n",
      "Run: 03, Epoch: 34, Loss: 0.5583, Train: 88.51%, Valid: 47.46% Test: 37.84%\n",
      "Run: 03, Epoch: 35, Loss: 0.5672, Train: 88.51%, Valid: 47.46% Test: 37.84%\n",
      "Run: 03, Epoch: 36, Loss: 0.5553, Train: 88.51%, Valid: 45.76% Test: 37.84%\n",
      "Run: 03, Epoch: 37, Loss: 0.5040, Train: 88.51%, Valid: 45.76% Test: 37.84%\n",
      "Run: 03, Epoch: 38, Loss: 0.5135, Train: 88.51%, Valid: 44.07% Test: 37.84%\n",
      "Run: 03, Epoch: 39, Loss: 0.5140, Train: 88.51%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 40, Loss: 0.5412, Train: 88.51%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 41, Loss: 0.4844, Train: 88.51%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 42, Loss: 0.4883, Train: 88.51%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 43, Loss: 0.4689, Train: 89.66%, Valid: 45.76% Test: 37.84%\n",
      "Run: 03, Epoch: 44, Loss: 0.4633, Train: 89.66%, Valid: 45.76% Test: 37.84%\n",
      "Run: 03, Epoch: 45, Loss: 0.4793, Train: 89.66%, Valid: 45.76% Test: 37.84%\n",
      "Run: 03, Epoch: 46, Loss: 0.4874, Train: 89.66%, Valid: 47.46% Test: 37.84%\n",
      "Run: 03, Epoch: 47, Loss: 0.4255, Train: 88.51%, Valid: 49.15% Test: 37.84%\n",
      "Run: 03, Epoch: 48, Loss: 0.4639, Train: 90.80%, Valid: 49.15% Test: 37.84%\n",
      "Run: 03, Epoch: 49, Loss: 0.4070, Train: 90.80%, Valid: 49.15% Test: 37.84%\n",
      "Run: 03, Epoch: 50, Loss: 0.4624, Train: 90.80%, Valid: 45.76% Test: 37.84%\n",
      "Run: 03, Epoch: 51, Loss: 0.4627, Train: 90.80%, Valid: 45.76% Test: 40.54%\n",
      "Run: 03, Epoch: 52, Loss: 0.4069, Train: 90.80%, Valid: 45.76% Test: 40.54%\n",
      "Run: 03, Epoch: 53, Loss: 0.4111, Train: 93.10%, Valid: 44.07% Test: 40.54%\n",
      "Run: 03, Epoch: 54, Loss: 0.4163, Train: 91.95%, Valid: 44.07% Test: 40.54%\n",
      "Run: 03, Epoch: 55, Loss: 0.4382, Train: 91.95%, Valid: 44.07% Test: 40.54%\n",
      "Run: 03, Epoch: 56, Loss: 0.4083, Train: 91.95%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 57, Loss: 0.4321, Train: 90.80%, Valid: 44.07% Test: 40.54%\n",
      "Run: 03, Epoch: 58, Loss: 0.4240, Train: 90.80%, Valid: 44.07% Test: 43.24%\n",
      "Run: 03, Epoch: 59, Loss: 0.4059, Train: 90.80%, Valid: 44.07% Test: 43.24%\n",
      "Run: 03, Epoch: 60, Loss: 0.4174, Train: 91.95%, Valid: 44.07% Test: 43.24%\n",
      "Run: 03, Epoch: 61, Loss: 0.3424, Train: 91.95%, Valid: 42.37% Test: 43.24%\n",
      "Run: 03, Epoch: 62, Loss: 0.3798, Train: 94.25%, Valid: 42.37% Test: 43.24%\n",
      "Run: 03, Epoch: 63, Loss: 0.3558, Train: 94.25%, Valid: 42.37% Test: 43.24%\n",
      "Run: 03, Epoch: 64, Loss: 0.3480, Train: 94.25%, Valid: 44.07% Test: 43.24%\n",
      "Run: 03, Epoch: 65, Loss: 0.3978, Train: 95.40%, Valid: 44.07% Test: 40.54%\n",
      "Run: 03, Epoch: 66, Loss: 0.3963, Train: 94.25%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 67, Loss: 0.3616, Train: 94.25%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 68, Loss: 0.3389, Train: 94.25%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 69, Loss: 0.3521, Train: 93.10%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 70, Loss: 0.3261, Train: 93.10%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 71, Loss: 0.3610, Train: 93.10%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 72, Loss: 0.3764, Train: 94.25%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 73, Loss: 0.3173, Train: 94.25%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 74, Loss: 0.3428, Train: 94.25%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 75, Loss: 0.3283, Train: 93.10%, Valid: 44.07% Test: 37.84%\n",
      "Run: 03, Epoch: 76, Loss: 0.3163, Train: 93.10%, Valid: 45.76% Test: 37.84%\n",
      "Run: 03, Epoch: 77, Loss: 0.3043, Train: 90.80%, Valid: 44.07% Test: 40.54%\n",
      "Run: 03, Epoch: 78, Loss: 0.3228, Train: 90.80%, Valid: 45.76% Test: 40.54%\n",
      "Run: 03, Epoch: 79, Loss: 0.3679, Train: 91.95%, Valid: 45.76% Test: 40.54%\n",
      "Run: 03, Epoch: 80, Loss: 0.3070, Train: 91.95%, Valid: 45.76% Test: 40.54%\n",
      "Run: 03, Epoch: 81, Loss: 0.3211, Train: 93.10%, Valid: 47.46% Test: 40.54%\n",
      "Run: 03, Epoch: 82, Loss: 0.2961, Train: 93.10%, Valid: 49.15% Test: 40.54%\n",
      "Run: 03, Epoch: 83, Loss: 0.3105, Train: 94.25%, Valid: 45.76% Test: 40.54%\n",
      "Run: 03, Epoch: 84, Loss: 0.3045, Train: 95.40%, Valid: 47.46% Test: 40.54%\n",
      "Run: 03, Epoch: 85, Loss: 0.3465, Train: 95.40%, Valid: 45.76% Test: 43.24%\n",
      "Run: 03, Epoch: 86, Loss: 0.2725, Train: 95.40%, Valid: 45.76% Test: 43.24%\n",
      "Run: 03, Epoch: 87, Loss: 0.2893, Train: 96.55%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 88, Loss: 0.3094, Train: 97.70%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 89, Loss: 0.3373, Train: 97.70%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 90, Loss: 0.3710, Train: 97.70%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 91, Loss: 0.3415, Train: 97.70%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 92, Loss: 0.2530, Train: 97.70%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 93, Loss: 0.2700, Train: 97.70%, Valid: 42.37% Test: 35.14%\n",
      "Run: 03, Epoch: 94, Loss: 0.3119, Train: 96.55%, Valid: 42.37% Test: 32.43%\n",
      "Run: 03, Epoch: 95, Loss: 0.2946, Train: 97.70%, Valid: 42.37% Test: 32.43%\n",
      "Run: 03, Epoch: 96, Loss: 0.3012, Train: 97.70%, Valid: 42.37% Test: 32.43%\n",
      "Run: 03, Epoch: 97, Loss: 0.2792, Train: 97.70%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 98, Loss: 0.2719, Train: 97.70%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 99, Loss: 0.2972, Train: 97.70%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 100, Loss: 0.2581, Train: 97.70%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 101, Loss: 0.2674, Train: 97.70%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 102, Loss: 0.2452, Train: 96.55%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 103, Loss: 0.2538, Train: 96.55%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 104, Loss: 0.2868, Train: 96.55%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 105, Loss: 0.2168, Train: 96.55%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 106, Loss: 0.2281, Train: 96.55%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 107, Loss: 0.2579, Train: 97.70%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 108, Loss: 0.2148, Train: 97.70%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 109, Loss: 0.2482, Train: 97.70%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 110, Loss: 0.2065, Train: 97.70%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 111, Loss: 0.2138, Train: 97.70%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 112, Loss: 0.3351, Train: 97.70%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 113, Loss: 0.2490, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 114, Loss: 0.2460, Train: 98.85%, Valid: 42.37% Test: 35.14%\n",
      "Run: 03, Epoch: 115, Loss: 0.2704, Train: 98.85%, Valid: 42.37% Test: 35.14%\n",
      "Run: 03, Epoch: 116, Loss: 0.2464, Train: 98.85%, Valid: 42.37% Test: 35.14%\n",
      "Run: 03, Epoch: 117, Loss: 0.2337, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 118, Loss: 0.2351, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 119, Loss: 0.2802, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 120, Loss: 0.2281, Train: 100.00%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 121, Loss: 0.2243, Train: 100.00%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 122, Loss: 0.2928, Train: 100.00%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 123, Loss: 0.2870, Train: 100.00%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 124, Loss: 0.2574, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 125, Loss: 0.2189, Train: 98.85%, Valid: 42.37% Test: 35.14%\n",
      "Run: 03, Epoch: 126, Loss: 0.1867, Train: 98.85%, Valid: 42.37% Test: 35.14%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 03, Epoch: 127, Loss: 0.2754, Train: 98.85%, Valid: 42.37% Test: 35.14%\n",
      "Run: 03, Epoch: 128, Loss: 0.2635, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 129, Loss: 0.2260, Train: 98.85%, Valid: 42.37% Test: 32.43%\n",
      "Run: 03, Epoch: 130, Loss: 0.2379, Train: 98.85%, Valid: 40.68% Test: 29.73%\n",
      "Run: 03, Epoch: 131, Loss: 0.2360, Train: 98.85%, Valid: 40.68% Test: 29.73%\n",
      "Run: 03, Epoch: 132, Loss: 0.2090, Train: 98.85%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 133, Loss: 0.2219, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 134, Loss: 0.2831, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 135, Loss: 0.1973, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 136, Loss: 0.2143, Train: 98.85%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 137, Loss: 0.2114, Train: 98.85%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 138, Loss: 0.2123, Train: 98.85%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 139, Loss: 0.2466, Train: 98.85%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 140, Loss: 0.2496, Train: 100.00%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 141, Loss: 0.3979, Train: 100.00%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 142, Loss: 0.1752, Train: 100.00%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 143, Loss: 0.2561, Train: 100.00%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 144, Loss: 0.2066, Train: 100.00%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 145, Loss: 0.1895, Train: 100.00%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 146, Loss: 0.2588, Train: 100.00%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 147, Loss: 0.2486, Train: 100.00%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 148, Loss: 0.2368, Train: 100.00%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 149, Loss: 0.1928, Train: 100.00%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 150, Loss: 0.1602, Train: 100.00%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 151, Loss: 0.2265, Train: 98.85%, Valid: 40.68% Test: 37.84%\n",
      "Run: 03, Epoch: 152, Loss: 0.1617, Train: 98.85%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 153, Loss: 0.1598, Train: 97.70%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 154, Loss: 0.1843, Train: 97.70%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 155, Loss: 0.1712, Train: 97.70%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 156, Loss: 0.1632, Train: 97.70%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 157, Loss: 0.1623, Train: 97.70%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 158, Loss: 0.2476, Train: 98.85%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 159, Loss: 0.2246, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 160, Loss: 0.2407, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 161, Loss: 0.2077, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 162, Loss: 0.1805, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 163, Loss: 0.1710, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 164, Loss: 0.2111, Train: 98.85%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 165, Loss: 0.2578, Train: 100.00%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 166, Loss: 0.1906, Train: 100.00%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 167, Loss: 0.2102, Train: 100.00%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 168, Loss: 0.2682, Train: 100.00%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 169, Loss: 0.1723, Train: 100.00%, Valid: 42.37% Test: 35.14%\n",
      "Run: 03, Epoch: 170, Loss: 0.1647, Train: 98.85%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 171, Loss: 0.2006, Train: 100.00%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 172, Loss: 0.1631, Train: 100.00%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 173, Loss: 0.2277, Train: 100.00%, Valid: 42.37% Test: 40.54%\n",
      "Run: 03, Epoch: 174, Loss: 0.1756, Train: 100.00%, Valid: 42.37% Test: 37.84%\n",
      "Run: 03, Epoch: 175, Loss: 0.1984, Train: 100.00%, Valid: 42.37% Test: 43.24%\n",
      "Run: 03, Epoch: 176, Loss: 0.1869, Train: 100.00%, Valid: 45.76% Test: 40.54%\n",
      "Run: 03, Epoch: 177, Loss: 0.2914, Train: 100.00%, Valid: 44.07% Test: 40.54%\n",
      "Run: 03, Epoch: 178, Loss: 0.2413, Train: 97.70%, Valid: 40.68% Test: 40.54%\n",
      "Run: 03, Epoch: 179, Loss: 0.1933, Train: 97.70%, Valid: 40.68% Test: 43.24%\n",
      "Run: 03, Epoch: 180, Loss: 0.2111, Train: 98.85%, Valid: 45.76% Test: 43.24%\n",
      "Run: 03, Epoch: 181, Loss: 0.1580, Train: 97.70%, Valid: 44.07% Test: 43.24%\n",
      "Run: 03, Epoch: 182, Loss: 0.1997, Train: 97.70%, Valid: 44.07% Test: 43.24%\n",
      "Run: 03, Epoch: 183, Loss: 0.2012, Train: 97.70%, Valid: 45.76% Test: 43.24%\n",
      "Run: 03, Epoch: 184, Loss: 0.2506, Train: 98.85%, Valid: 44.07% Test: 37.84%\n",
      "Run: 03, Epoch: 185, Loss: 0.2026, Train: 98.85%, Valid: 44.07% Test: 37.84%\n",
      "Run: 03, Epoch: 186, Loss: 0.1809, Train: 98.85%, Valid: 44.07% Test: 40.54%\n",
      "Run: 03, Epoch: 187, Loss: 0.1970, Train: 98.85%, Valid: 44.07% Test: 35.14%\n",
      "Run: 03, Epoch: 188, Loss: 0.1399, Train: 98.85%, Valid: 44.07% Test: 35.14%\n",
      "Run: 03, Epoch: 189, Loss: 0.3250, Train: 98.85%, Valid: 44.07% Test: 32.43%\n",
      "Run: 03, Epoch: 190, Loss: 0.1636, Train: 100.00%, Valid: 44.07% Test: 32.43%\n",
      "Run: 03, Epoch: 191, Loss: 0.2310, Train: 100.00%, Valid: 44.07% Test: 32.43%\n",
      "Run: 03, Epoch: 192, Loss: 0.2145, Train: 100.00%, Valid: 44.07% Test: 32.43%\n",
      "Run: 03, Epoch: 193, Loss: 0.1447, Train: 100.00%, Valid: 44.07% Test: 32.43%\n",
      "Run: 03, Epoch: 194, Loss: 0.1937, Train: 100.00%, Valid: 44.07% Test: 35.14%\n",
      "Run: 03, Epoch: 195, Loss: 0.1717, Train: 100.00%, Valid: 42.37% Test: 35.14%\n",
      "Run: 03, Epoch: 196, Loss: 0.1934, Train: 100.00%, Valid: 42.37% Test: 32.43%\n",
      "Run: 03, Epoch: 197, Loss: 0.2998, Train: 100.00%, Valid: 42.37% Test: 32.43%\n",
      "Run: 03, Epoch: 198, Loss: 0.1813, Train: 100.00%, Valid: 40.68% Test: 35.14%\n",
      "Run: 03, Epoch: 199, Loss: 0.1851, Train: 100.00%, Valid: 40.68% Test: 32.43%\n",
      "Run: 03, Epoch: 200, Loss: 0.1591, Train: 100.00%, Valid: 40.68% Test: 35.14%\n",
      "Run 03:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 55.93\n",
      "  Final Train: 66.67\n",
      "   Final Test: 48.65\n",
      "Run: 04, Epoch: 01, Loss: 1.5162, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 02, Loss: 1.4334, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 03, Loss: 1.3763, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 04, Loss: 1.3374, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 05, Loss: 1.2639, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 06, Loss: 1.1876, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 07, Loss: 1.1593, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 08, Loss: 1.1414, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 09, Loss: 1.0694, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 10, Loss: 1.0189, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 11, Loss: 0.9886, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 12, Loss: 0.9486, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 13, Loss: 0.9088, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 14, Loss: 0.8621, Train: 54.02%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 15, Loss: 0.9372, Train: 56.32%, Valid: 52.54% Test: 62.16%\n",
      "Run: 04, Epoch: 16, Loss: 0.8911, Train: 58.62%, Valid: 52.54% Test: 59.46%\n",
      "Run: 04, Epoch: 17, Loss: 0.8181, Train: 59.77%, Valid: 52.54% Test: 59.46%\n",
      "Run: 04, Epoch: 18, Loss: 0.8449, Train: 60.92%, Valid: 54.24% Test: 59.46%\n",
      "Run: 04, Epoch: 19, Loss: 0.8258, Train: 62.07%, Valid: 54.24% Test: 59.46%\n",
      "Run: 04, Epoch: 20, Loss: 0.9048, Train: 64.37%, Valid: 54.24% Test: 56.76%\n",
      "Run: 04, Epoch: 21, Loss: 0.7935, Train: 66.67%, Valid: 54.24% Test: 56.76%\n",
      "Run: 04, Epoch: 22, Loss: 0.7753, Train: 73.56%, Valid: 57.63% Test: 62.16%\n",
      "Run: 04, Epoch: 23, Loss: 0.7164, Train: 75.86%, Valid: 57.63% Test: 59.46%\n",
      "Run: 04, Epoch: 24, Loss: 0.7345, Train: 75.86%, Valid: 57.63% Test: 59.46%\n",
      "Run: 04, Epoch: 25, Loss: 0.7088, Train: 74.71%, Valid: 57.63% Test: 62.16%\n",
      "Run: 04, Epoch: 26, Loss: 0.7012, Train: 74.71%, Valid: 55.93% Test: 62.16%\n",
      "Run: 04, Epoch: 27, Loss: 0.7026, Train: 72.41%, Valid: 55.93% Test: 62.16%\n",
      "Run: 04, Epoch: 28, Loss: 0.7010, Train: 71.26%, Valid: 55.93% Test: 62.16%\n",
      "Run: 04, Epoch: 29, Loss: 0.6786, Train: 74.71%, Valid: 55.93% Test: 59.46%\n",
      "Run: 04, Epoch: 30, Loss: 0.6508, Train: 75.86%, Valid: 55.93% Test: 59.46%\n",
      "Run: 04, Epoch: 31, Loss: 0.6970, Train: 78.16%, Valid: 55.93% Test: 56.76%\n",
      "Run: 04, Epoch: 32, Loss: 0.6705, Train: 79.31%, Valid: 52.54% Test: 59.46%\n",
      "Run: 04, Epoch: 33, Loss: 0.7119, Train: 82.76%, Valid: 54.24% Test: 56.76%\n",
      "Run: 04, Epoch: 34, Loss: 0.7038, Train: 82.76%, Valid: 52.54% Test: 56.76%\n",
      "Run: 04, Epoch: 35, Loss: 0.6126, Train: 82.76%, Valid: 52.54% Test: 56.76%\n",
      "Run: 04, Epoch: 36, Loss: 0.6566, Train: 82.76%, Valid: 52.54% Test: 54.05%\n",
      "Run: 04, Epoch: 37, Loss: 0.7801, Train: 82.76%, Valid: 52.54% Test: 54.05%\n",
      "Run: 04, Epoch: 38, Loss: 0.6105, Train: 82.76%, Valid: 52.54% Test: 54.05%\n",
      "Run: 04, Epoch: 39, Loss: 0.6134, Train: 83.91%, Valid: 52.54% Test: 54.05%\n",
      "Run: 04, Epoch: 40, Loss: 0.5765, Train: 86.21%, Valid: 52.54% Test: 56.76%\n",
      "Run: 04, Epoch: 41, Loss: 0.6316, Train: 87.36%, Valid: 52.54% Test: 56.76%\n",
      "Run: 04, Epoch: 42, Loss: 0.6581, Train: 87.36%, Valid: 52.54% Test: 54.05%\n",
      "Run: 04, Epoch: 43, Loss: 0.6282, Train: 87.36%, Valid: 54.24% Test: 51.35%\n",
      "Run: 04, Epoch: 44, Loss: 0.5898, Train: 87.36%, Valid: 54.24% Test: 48.65%\n",
      "Run: 04, Epoch: 45, Loss: 0.6100, Train: 86.21%, Valid: 52.54% Test: 48.65%\n",
      "Run: 04, Epoch: 46, Loss: 0.6258, Train: 87.36%, Valid: 52.54% Test: 48.65%\n",
      "Run: 04, Epoch: 47, Loss: 0.5669, Train: 87.36%, Valid: 52.54% Test: 48.65%\n",
      "Run: 04, Epoch: 48, Loss: 0.5806, Train: 87.36%, Valid: 52.54% Test: 48.65%\n",
      "Run: 04, Epoch: 49, Loss: 0.6217, Train: 88.51%, Valid: 55.93% Test: 48.65%\n",
      "Run: 04, Epoch: 50, Loss: 0.6114, Train: 87.36%, Valid: 55.93% Test: 45.95%\n",
      "Run: 04, Epoch: 51, Loss: 0.5452, Train: 88.51%, Valid: 55.93% Test: 45.95%\n",
      "Run: 04, Epoch: 52, Loss: 0.5932, Train: 90.80%, Valid: 59.32% Test: 45.95%\n",
      "Run: 04, Epoch: 53, Loss: 0.5589, Train: 89.66%, Valid: 57.63% Test: 48.65%\n",
      "Run: 04, Epoch: 54, Loss: 0.6104, Train: 89.66%, Valid: 57.63% Test: 48.65%\n",
      "Run: 04, Epoch: 55, Loss: 0.6166, Train: 90.80%, Valid: 55.93% Test: 48.65%\n",
      "Run: 04, Epoch: 56, Loss: 0.4697, Train: 90.80%, Valid: 55.93% Test: 48.65%\n",
      "Run: 04, Epoch: 57, Loss: 0.5334, Train: 91.95%, Valid: 55.93% Test: 45.95%\n",
      "Run: 04, Epoch: 58, Loss: 0.5253, Train: 91.95%, Valid: 55.93% Test: 43.24%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 04, Epoch: 59, Loss: 0.5165, Train: 91.95%, Valid: 55.93% Test: 43.24%\n",
      "Run: 04, Epoch: 60, Loss: 0.5443, Train: 91.95%, Valid: 57.63% Test: 43.24%\n",
      "Run: 04, Epoch: 61, Loss: 0.5595, Train: 91.95%, Valid: 57.63% Test: 43.24%\n",
      "Run: 04, Epoch: 62, Loss: 0.5555, Train: 91.95%, Valid: 54.24% Test: 43.24%\n",
      "Run: 04, Epoch: 63, Loss: 0.5129, Train: 90.80%, Valid: 50.85% Test: 40.54%\n",
      "Run: 04, Epoch: 64, Loss: 0.5786, Train: 90.80%, Valid: 50.85% Test: 40.54%\n",
      "Run: 04, Epoch: 65, Loss: 0.5600, Train: 91.95%, Valid: 50.85% Test: 40.54%\n",
      "Run: 04, Epoch: 66, Loss: 0.5161, Train: 90.80%, Valid: 50.85% Test: 40.54%\n",
      "Run: 04, Epoch: 67, Loss: 0.5222, Train: 91.95%, Valid: 52.54% Test: 40.54%\n",
      "Run: 04, Epoch: 68, Loss: 0.4643, Train: 91.95%, Valid: 52.54% Test: 43.24%\n",
      "Run: 04, Epoch: 69, Loss: 0.5191, Train: 93.10%, Valid: 52.54% Test: 43.24%\n",
      "Run: 04, Epoch: 70, Loss: 0.4844, Train: 93.10%, Valid: 52.54% Test: 40.54%\n",
      "Run: 04, Epoch: 71, Loss: 0.4623, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 72, Loss: 0.5035, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 73, Loss: 0.4402, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 74, Loss: 0.4717, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 75, Loss: 0.4700, Train: 91.95%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 76, Loss: 0.4806, Train: 91.95%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 77, Loss: 0.4148, Train: 91.95%, Valid: 55.93% Test: 40.54%\n",
      "Run: 04, Epoch: 78, Loss: 0.4599, Train: 91.95%, Valid: 55.93% Test: 40.54%\n",
      "Run: 04, Epoch: 79, Loss: 0.4736, Train: 93.10%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 80, Loss: 0.4252, Train: 93.10%, Valid: 55.93% Test: 40.54%\n",
      "Run: 04, Epoch: 81, Loss: 0.4437, Train: 93.10%, Valid: 55.93% Test: 40.54%\n",
      "Run: 04, Epoch: 82, Loss: 0.5509, Train: 91.95%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 83, Loss: 0.4458, Train: 91.95%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 84, Loss: 0.4088, Train: 90.80%, Valid: 54.24% Test: 35.14%\n",
      "Run: 04, Epoch: 85, Loss: 0.4906, Train: 91.95%, Valid: 54.24% Test: 35.14%\n",
      "Run: 04, Epoch: 86, Loss: 0.4484, Train: 91.95%, Valid: 54.24% Test: 35.14%\n",
      "Run: 04, Epoch: 87, Loss: 0.3961, Train: 91.95%, Valid: 54.24% Test: 35.14%\n",
      "Run: 04, Epoch: 88, Loss: 0.5081, Train: 91.95%, Valid: 52.54% Test: 35.14%\n",
      "Run: 04, Epoch: 89, Loss: 0.4170, Train: 91.95%, Valid: 50.85% Test: 35.14%\n",
      "Run: 04, Epoch: 90, Loss: 0.4034, Train: 91.95%, Valid: 49.15% Test: 35.14%\n",
      "Run: 04, Epoch: 91, Loss: 0.3848, Train: 91.95%, Valid: 50.85% Test: 35.14%\n",
      "Run: 04, Epoch: 92, Loss: 0.3684, Train: 93.10%, Valid: 52.54% Test: 35.14%\n",
      "Run: 04, Epoch: 93, Loss: 0.4352, Train: 93.10%, Valid: 52.54% Test: 37.84%\n",
      "Run: 04, Epoch: 94, Loss: 0.4138, Train: 93.10%, Valid: 52.54% Test: 37.84%\n",
      "Run: 04, Epoch: 95, Loss: 0.4719, Train: 93.10%, Valid: 52.54% Test: 40.54%\n",
      "Run: 04, Epoch: 96, Loss: 0.4333, Train: 93.10%, Valid: 54.24% Test: 43.24%\n",
      "Run: 04, Epoch: 97, Loss: 0.3422, Train: 93.10%, Valid: 55.93% Test: 43.24%\n",
      "Run: 04, Epoch: 98, Loss: 0.4016, Train: 93.10%, Valid: 57.63% Test: 37.84%\n",
      "Run: 04, Epoch: 99, Loss: 0.4736, Train: 93.10%, Valid: 57.63% Test: 37.84%\n",
      "Run: 04, Epoch: 100, Loss: 0.4383, Train: 94.25%, Valid: 57.63% Test: 37.84%\n",
      "Run: 04, Epoch: 101, Loss: 0.3856, Train: 93.10%, Valid: 57.63% Test: 37.84%\n",
      "Run: 04, Epoch: 102, Loss: 0.4160, Train: 93.10%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 103, Loss: 0.4395, Train: 93.10%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 104, Loss: 0.3459, Train: 93.10%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 105, Loss: 0.3877, Train: 94.25%, Valid: 54.24% Test: 35.14%\n",
      "Run: 04, Epoch: 106, Loss: 0.3543, Train: 94.25%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 107, Loss: 0.4011, Train: 94.25%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 108, Loss: 0.3919, Train: 93.10%, Valid: 55.93% Test: 37.84%\n",
      "Run: 04, Epoch: 109, Loss: 0.3554, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 110, Loss: 0.3614, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 111, Loss: 0.4128, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 112, Loss: 0.4323, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 113, Loss: 0.3771, Train: 93.10%, Valid: 54.24% Test: 40.54%\n",
      "Run: 04, Epoch: 114, Loss: 0.3398, Train: 93.10%, Valid: 55.93% Test: 40.54%\n",
      "Run: 04, Epoch: 115, Loss: 0.3540, Train: 93.10%, Valid: 55.93% Test: 40.54%\n",
      "Run: 04, Epoch: 116, Loss: 0.4021, Train: 93.10%, Valid: 55.93% Test: 40.54%\n",
      "Run: 04, Epoch: 117, Loss: 0.4103, Train: 91.95%, Valid: 52.54% Test: 40.54%\n",
      "Run: 04, Epoch: 118, Loss: 0.3791, Train: 91.95%, Valid: 52.54% Test: 40.54%\n",
      "Run: 04, Epoch: 119, Loss: 0.4265, Train: 91.95%, Valid: 50.85% Test: 37.84%\n",
      "Run: 04, Epoch: 120, Loss: 0.3590, Train: 93.10%, Valid: 52.54% Test: 37.84%\n",
      "Run: 04, Epoch: 121, Loss: 0.5109, Train: 93.10%, Valid: 47.46% Test: 37.84%\n",
      "Run: 04, Epoch: 122, Loss: 0.3408, Train: 94.25%, Valid: 45.76% Test: 35.14%\n",
      "Run: 04, Epoch: 123, Loss: 0.4328, Train: 93.10%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 124, Loss: 0.3993, Train: 93.10%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 125, Loss: 0.4459, Train: 93.10%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 126, Loss: 0.4073, Train: 93.10%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 127, Loss: 0.3135, Train: 93.10%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 128, Loss: 0.3894, Train: 94.25%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 129, Loss: 0.3555, Train: 94.25%, Valid: 42.37% Test: 37.84%\n",
      "Run: 04, Epoch: 130, Loss: 0.3831, Train: 94.25%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 131, Loss: 0.3514, Train: 94.25%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 132, Loss: 0.3564, Train: 94.25%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 133, Loss: 0.3399, Train: 94.25%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 134, Loss: 0.3558, Train: 94.25%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 135, Loss: 0.3306, Train: 95.40%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 136, Loss: 0.3608, Train: 95.40%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 137, Loss: 0.3474, Train: 96.55%, Valid: 47.46% Test: 35.14%\n",
      "Run: 04, Epoch: 138, Loss: 0.3404, Train: 96.55%, Valid: 45.76% Test: 35.14%\n",
      "Run: 04, Epoch: 139, Loss: 0.2971, Train: 95.40%, Valid: 47.46% Test: 35.14%\n",
      "Run: 04, Epoch: 140, Loss: 0.3979, Train: 95.40%, Valid: 47.46% Test: 35.14%\n",
      "Run: 04, Epoch: 141, Loss: 0.2998, Train: 96.55%, Valid: 47.46% Test: 35.14%\n",
      "Run: 04, Epoch: 142, Loss: 0.3257, Train: 96.55%, Valid: 47.46% Test: 35.14%\n",
      "Run: 04, Epoch: 143, Loss: 0.2879, Train: 96.55%, Valid: 47.46% Test: 37.84%\n",
      "Run: 04, Epoch: 144, Loss: 0.2864, Train: 96.55%, Valid: 45.76% Test: 37.84%\n",
      "Run: 04, Epoch: 145, Loss: 0.3595, Train: 96.55%, Valid: 47.46% Test: 37.84%\n",
      "Run: 04, Epoch: 146, Loss: 0.2919, Train: 96.55%, Valid: 47.46% Test: 37.84%\n",
      "Run: 04, Epoch: 147, Loss: 0.2929, Train: 96.55%, Valid: 47.46% Test: 37.84%\n",
      "Run: 04, Epoch: 148, Loss: 0.3249, Train: 96.55%, Valid: 45.76% Test: 37.84%\n",
      "Run: 04, Epoch: 149, Loss: 0.2470, Train: 96.55%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 150, Loss: 0.2642, Train: 96.55%, Valid: 45.76% Test: 37.84%\n",
      "Run: 04, Epoch: 151, Loss: 0.3114, Train: 96.55%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 152, Loss: 0.4303, Train: 96.55%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 153, Loss: 0.2228, Train: 96.55%, Valid: 42.37% Test: 37.84%\n",
      "Run: 04, Epoch: 154, Loss: 0.4395, Train: 96.55%, Valid: 42.37% Test: 37.84%\n",
      "Run: 04, Epoch: 155, Loss: 0.3361, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 156, Loss: 0.2274, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 157, Loss: 0.2875, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 158, Loss: 0.3254, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 159, Loss: 0.3086, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 160, Loss: 0.3081, Train: 97.70%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 161, Loss: 0.3346, Train: 97.70%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 162, Loss: 0.3299, Train: 97.70%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 163, Loss: 0.2323, Train: 97.70%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 164, Loss: 0.2806, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 165, Loss: 0.2980, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 166, Loss: 0.3459, Train: 95.40%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 167, Loss: 0.3185, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 168, Loss: 0.2749, Train: 96.55%, Valid: 45.76% Test: 35.14%\n",
      "Run: 04, Epoch: 169, Loss: 0.3457, Train: 96.55%, Valid: 45.76% Test: 35.14%\n",
      "Run: 04, Epoch: 170, Loss: 0.2900, Train: 95.40%, Valid: 45.76% Test: 35.14%\n",
      "Run: 04, Epoch: 171, Loss: 0.2590, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 172, Loss: 0.2571, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 173, Loss: 0.2782, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run: 04, Epoch: 174, Loss: 0.2683, Train: 97.70%, Valid: 42.37% Test: 35.14%\n",
      "Run: 04, Epoch: 175, Loss: 0.3053, Train: 96.55%, Valid: 38.98% Test: 35.14%\n",
      "Run: 04, Epoch: 176, Loss: 0.2766, Train: 96.55%, Valid: 37.29% Test: 35.14%\n",
      "Run: 04, Epoch: 177, Loss: 0.2592, Train: 96.55%, Valid: 37.29% Test: 35.14%\n",
      "Run: 04, Epoch: 178, Loss: 0.3060, Train: 96.55%, Valid: 37.29% Test: 29.73%\n",
      "Run: 04, Epoch: 179, Loss: 0.2517, Train: 96.55%, Valid: 37.29% Test: 29.73%\n",
      "Run: 04, Epoch: 180, Loss: 0.2917, Train: 96.55%, Valid: 37.29% Test: 29.73%\n",
      "Run: 04, Epoch: 181, Loss: 0.2859, Train: 96.55%, Valid: 37.29% Test: 29.73%\n",
      "Run: 04, Epoch: 182, Loss: 0.3211, Train: 96.55%, Valid: 37.29% Test: 29.73%\n",
      "Run: 04, Epoch: 183, Loss: 0.3008, Train: 96.55%, Valid: 35.59% Test: 29.73%\n",
      "Run: 04, Epoch: 184, Loss: 0.2507, Train: 96.55%, Valid: 37.29% Test: 29.73%\n",
      "Run: 04, Epoch: 185, Loss: 0.3031, Train: 96.55%, Valid: 33.90% Test: 27.03%\n",
      "Run: 04, Epoch: 186, Loss: 0.2574, Train: 97.70%, Valid: 35.59% Test: 27.03%\n",
      "Run: 04, Epoch: 187, Loss: 0.2497, Train: 97.70%, Valid: 37.29% Test: 29.73%\n",
      "Run: 04, Epoch: 188, Loss: 0.2263, Train: 97.70%, Valid: 37.29% Test: 29.73%\n",
      "Run: 04, Epoch: 189, Loss: 0.2819, Train: 97.70%, Valid: 40.68% Test: 32.43%\n",
      "Run: 04, Epoch: 190, Loss: 0.2109, Train: 97.70%, Valid: 42.37% Test: 32.43%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 04, Epoch: 191, Loss: 0.2680, Train: 97.70%, Valid: 44.07% Test: 32.43%\n",
      "Run: 04, Epoch: 192, Loss: 0.2291, Train: 96.55%, Valid: 44.07% Test: 32.43%\n",
      "Run: 04, Epoch: 193, Loss: 0.2337, Train: 97.70%, Valid: 42.37% Test: 32.43%\n",
      "Run: 04, Epoch: 194, Loss: 0.2998, Train: 97.70%, Valid: 42.37% Test: 32.43%\n",
      "Run: 04, Epoch: 195, Loss: 0.2462, Train: 97.70%, Valid: 42.37% Test: 32.43%\n",
      "Run: 04, Epoch: 196, Loss: 0.2951, Train: 97.70%, Valid: 42.37% Test: 35.14%\n",
      "Run: 04, Epoch: 197, Loss: 0.3957, Train: 97.70%, Valid: 42.37% Test: 37.84%\n",
      "Run: 04, Epoch: 198, Loss: 0.2312, Train: 97.70%, Valid: 42.37% Test: 37.84%\n",
      "Run: 04, Epoch: 199, Loss: 0.2967, Train: 97.70%, Valid: 44.07% Test: 37.84%\n",
      "Run: 04, Epoch: 200, Loss: 0.2084, Train: 96.55%, Valid: 44.07% Test: 35.14%\n",
      "Run 04:\n",
      "Highest Train: 97.70\n",
      "Highest Valid: 59.32\n",
      "  Final Train: 90.80\n",
      "   Final Test: 45.95\n",
      "Run: 05, Epoch: 01, Loss: 1.7989, Train: 18.39%, Valid: 16.95% Test: 18.92%\n",
      "Run: 05, Epoch: 02, Loss: 1.6966, Train: 18.39%, Valid: 16.95% Test: 18.92%\n",
      "Run: 05, Epoch: 03, Loss: 1.6609, Train: 18.39%, Valid: 16.95% Test: 18.92%\n",
      "Run: 05, Epoch: 04, Loss: 1.5643, Train: 18.39%, Valid: 16.95% Test: 18.92%\n",
      "Run: 05, Epoch: 05, Loss: 1.5143, Train: 28.74%, Valid: 10.17% Test: 21.62%\n",
      "Run: 05, Epoch: 06, Loss: 1.4408, Train: 28.74%, Valid: 6.78% Test: 16.22%\n",
      "Run: 05, Epoch: 07, Loss: 1.4250, Train: 27.59%, Valid: 8.47% Test: 16.22%\n",
      "Run: 05, Epoch: 08, Loss: 1.3832, Train: 42.53%, Valid: 11.86% Test: 27.03%\n",
      "Run: 05, Epoch: 09, Loss: 1.3296, Train: 68.97%, Valid: 52.54% Test: 51.35%\n",
      "Run: 05, Epoch: 10, Loss: 1.2698, Train: 63.22%, Valid: 59.32% Test: 56.76%\n",
      "Run: 05, Epoch: 11, Loss: 1.2193, Train: 59.77%, Valid: 62.71% Test: 56.76%\n",
      "Run: 05, Epoch: 12, Loss: 1.1739, Train: 56.32%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 13, Loss: 1.1499, Train: 55.17%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 14, Loss: 1.0854, Train: 54.02%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 15, Loss: 1.0812, Train: 55.17%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 16, Loss: 0.9959, Train: 59.77%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 17, Loss: 0.9844, Train: 60.92%, Valid: 62.71% Test: 54.05%\n",
      "Run: 05, Epoch: 18, Loss: 0.9400, Train: 65.52%, Valid: 64.41% Test: 54.05%\n",
      "Run: 05, Epoch: 19, Loss: 0.9360, Train: 66.67%, Valid: 64.41% Test: 54.05%\n",
      "Run: 05, Epoch: 20, Loss: 0.9555, Train: 73.56%, Valid: 64.41% Test: 54.05%\n",
      "Run: 05, Epoch: 21, Loss: 0.9116, Train: 77.01%, Valid: 64.41% Test: 54.05%\n",
      "Run: 05, Epoch: 22, Loss: 0.9374, Train: 73.56%, Valid: 66.10% Test: 56.76%\n",
      "Run: 05, Epoch: 23, Loss: 0.8767, Train: 73.56%, Valid: 66.10% Test: 56.76%\n",
      "Run: 05, Epoch: 24, Loss: 0.8668, Train: 72.41%, Valid: 64.41% Test: 56.76%\n",
      "Run: 05, Epoch: 25, Loss: 0.8775, Train: 72.41%, Valid: 64.41% Test: 56.76%\n",
      "Run: 05, Epoch: 26, Loss: 0.8415, Train: 75.86%, Valid: 64.41% Test: 56.76%\n",
      "Run: 05, Epoch: 27, Loss: 0.8241, Train: 78.16%, Valid: 67.80% Test: 59.46%\n",
      "Run: 05, Epoch: 28, Loss: 0.7905, Train: 77.01%, Valid: 64.41% Test: 54.05%\n",
      "Run: 05, Epoch: 29, Loss: 0.7962, Train: 75.86%, Valid: 62.71% Test: 54.05%\n",
      "Run: 05, Epoch: 30, Loss: 0.7828, Train: 77.01%, Valid: 62.71% Test: 54.05%\n",
      "Run: 05, Epoch: 31, Loss: 0.7701, Train: 77.01%, Valid: 59.32% Test: 54.05%\n",
      "Run: 05, Epoch: 32, Loss: 0.7672, Train: 77.01%, Valid: 59.32% Test: 56.76%\n",
      "Run: 05, Epoch: 33, Loss: 0.7503, Train: 77.01%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 34, Loss: 0.7189, Train: 78.16%, Valid: 57.63% Test: 56.76%\n",
      "Run: 05, Epoch: 35, Loss: 0.7268, Train: 77.01%, Valid: 59.32% Test: 51.35%\n",
      "Run: 05, Epoch: 36, Loss: 0.7399, Train: 77.01%, Valid: 59.32% Test: 54.05%\n",
      "Run: 05, Epoch: 37, Loss: 0.7075, Train: 77.01%, Valid: 59.32% Test: 54.05%\n",
      "Run: 05, Epoch: 38, Loss: 0.6952, Train: 79.31%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 39, Loss: 0.6820, Train: 80.46%, Valid: 57.63% Test: 51.35%\n",
      "Run: 05, Epoch: 40, Loss: 0.6554, Train: 83.91%, Valid: 59.32% Test: 51.35%\n",
      "Run: 05, Epoch: 41, Loss: 0.6830, Train: 85.06%, Valid: 59.32% Test: 54.05%\n",
      "Run: 05, Epoch: 42, Loss: 0.6699, Train: 85.06%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 43, Loss: 0.6553, Train: 86.21%, Valid: 55.93% Test: 54.05%\n",
      "Run: 05, Epoch: 44, Loss: 0.6136, Train: 87.36%, Valid: 55.93% Test: 54.05%\n",
      "Run: 05, Epoch: 45, Loss: 0.6276, Train: 89.66%, Valid: 54.24% Test: 54.05%\n",
      "Run: 05, Epoch: 46, Loss: 0.5968, Train: 90.80%, Valid: 52.54% Test: 54.05%\n",
      "Run: 05, Epoch: 47, Loss: 0.5979, Train: 94.25%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 48, Loss: 0.6103, Train: 91.95%, Valid: 54.24% Test: 54.05%\n",
      "Run: 05, Epoch: 49, Loss: 0.6187, Train: 91.95%, Valid: 55.93% Test: 54.05%\n",
      "Run: 05, Epoch: 50, Loss: 0.6050, Train: 91.95%, Valid: 57.63% Test: 54.05%\n",
      "Run: 05, Epoch: 51, Loss: 0.5887, Train: 91.95%, Valid: 57.63% Test: 56.76%\n",
      "Run: 05, Epoch: 52, Loss: 0.5977, Train: 91.95%, Valid: 59.32% Test: 56.76%\n",
      "Run: 05, Epoch: 53, Loss: 0.5727, Train: 91.95%, Valid: 59.32% Test: 56.76%\n",
      "Run: 05, Epoch: 54, Loss: 0.5415, Train: 90.80%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 55, Loss: 0.5544, Train: 93.10%, Valid: 59.32% Test: 51.35%\n",
      "Run: 05, Epoch: 56, Loss: 0.5150, Train: 94.25%, Valid: 59.32% Test: 51.35%\n",
      "Run: 05, Epoch: 57, Loss: 0.5006, Train: 93.10%, Valid: 59.32% Test: 48.65%\n",
      "Run: 05, Epoch: 58, Loss: 0.5675, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 59, Loss: 0.5277, Train: 95.40%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 60, Loss: 0.5153, Train: 93.10%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 61, Loss: 0.5596, Train: 93.10%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 62, Loss: 0.5141, Train: 94.25%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 63, Loss: 0.5190, Train: 94.25%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 64, Loss: 0.5046, Train: 93.10%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 65, Loss: 0.4820, Train: 94.25%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 66, Loss: 0.4678, Train: 95.40%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 67, Loss: 0.4588, Train: 94.25%, Valid: 55.93% Test: 45.95%\n",
      "Run: 05, Epoch: 68, Loss: 0.4680, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 69, Loss: 0.4558, Train: 95.40%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 70, Loss: 0.4644, Train: 94.25%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 71, Loss: 0.4585, Train: 94.25%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 72, Loss: 0.4476, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 73, Loss: 0.5215, Train: 95.40%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 74, Loss: 0.4673, Train: 95.40%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 75, Loss: 0.4259, Train: 95.40%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 76, Loss: 0.4475, Train: 94.25%, Valid: 52.54% Test: 48.65%\n",
      "Run: 05, Epoch: 77, Loss: 0.4735, Train: 94.25%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 78, Loss: 0.4145, Train: 95.40%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 79, Loss: 0.4027, Train: 95.40%, Valid: 55.93% Test: 45.95%\n",
      "Run: 05, Epoch: 80, Loss: 0.3908, Train: 94.25%, Valid: 55.93% Test: 45.95%\n",
      "Run: 05, Epoch: 81, Loss: 0.4038, Train: 94.25%, Valid: 55.93% Test: 45.95%\n",
      "Run: 05, Epoch: 82, Loss: 0.4058, Train: 95.40%, Valid: 55.93% Test: 45.95%\n",
      "Run: 05, Epoch: 83, Loss: 0.4128, Train: 96.55%, Valid: 55.93% Test: 45.95%\n",
      "Run: 05, Epoch: 84, Loss: 0.4580, Train: 96.55%, Valid: 57.63% Test: 51.35%\n",
      "Run: 05, Epoch: 85, Loss: 0.4165, Train: 96.55%, Valid: 55.93% Test: 51.35%\n",
      "Run: 05, Epoch: 86, Loss: 0.4722, Train: 95.40%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 87, Loss: 0.4311, Train: 94.25%, Valid: 54.24% Test: 54.05%\n",
      "Run: 05, Epoch: 88, Loss: 0.3572, Train: 93.10%, Valid: 54.24% Test: 54.05%\n",
      "Run: 05, Epoch: 89, Loss: 0.4004, Train: 94.25%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 90, Loss: 0.3567, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 91, Loss: 0.3508, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 92, Loss: 0.3650, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 93, Loss: 0.3978, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 94, Loss: 0.3732, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 95, Loss: 0.3669, Train: 96.55%, Valid: 57.63% Test: 51.35%\n",
      "Run: 05, Epoch: 96, Loss: 0.3746, Train: 96.55%, Valid: 57.63% Test: 51.35%\n",
      "Run: 05, Epoch: 97, Loss: 0.3961, Train: 96.55%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 98, Loss: 0.4191, Train: 95.40%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 99, Loss: 0.3574, Train: 95.40%, Valid: 52.54% Test: 48.65%\n",
      "Run: 05, Epoch: 100, Loss: 0.3684, Train: 94.25%, Valid: 52.54% Test: 48.65%\n",
      "Run: 05, Epoch: 101, Loss: 0.3579, Train: 94.25%, Valid: 52.54% Test: 48.65%\n",
      "Run: 05, Epoch: 102, Loss: 0.3693, Train: 95.40%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 103, Loss: 0.3540, Train: 94.25%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 104, Loss: 0.3570, Train: 95.40%, Valid: 52.54% Test: 51.35%\n",
      "Run: 05, Epoch: 105, Loss: 0.3522, Train: 95.40%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 106, Loss: 0.3234, Train: 95.40%, Valid: 55.93% Test: 51.35%\n",
      "Run: 05, Epoch: 107, Loss: 0.3624, Train: 95.40%, Valid: 55.93% Test: 51.35%\n",
      "Run: 05, Epoch: 108, Loss: 0.3964, Train: 96.55%, Valid: 55.93% Test: 51.35%\n",
      "Run: 05, Epoch: 109, Loss: 0.3702, Train: 96.55%, Valid: 55.93% Test: 54.05%\n",
      "Run: 05, Epoch: 110, Loss: 0.2699, Train: 96.55%, Valid: 59.32% Test: 51.35%\n",
      "Run: 05, Epoch: 111, Loss: 0.3334, Train: 96.55%, Valid: 59.32% Test: 54.05%\n",
      "Run: 05, Epoch: 112, Loss: 0.2907, Train: 96.55%, Valid: 59.32% Test: 51.35%\n",
      "Run: 05, Epoch: 113, Loss: 0.3589, Train: 96.55%, Valid: 55.93% Test: 51.35%\n",
      "Run: 05, Epoch: 114, Loss: 0.3769, Train: 95.40%, Valid: 55.93% Test: 51.35%\n",
      "Run: 05, Epoch: 115, Loss: 0.3218, Train: 95.40%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 116, Loss: 0.3684, Train: 95.40%, Valid: 55.93% Test: 51.35%\n",
      "Run: 05, Epoch: 117, Loss: 0.3382, Train: 94.25%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 118, Loss: 0.3353, Train: 94.25%, Valid: 55.93% Test: 51.35%\n",
      "Run: 05, Epoch: 119, Loss: 0.3556, Train: 94.25%, Valid: 55.93% Test: 48.65%\n",
      "Run: 05, Epoch: 120, Loss: 0.3932, Train: 94.25%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 121, Loss: 0.3544, Train: 95.40%, Valid: 55.93% Test: 48.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 05, Epoch: 122, Loss: 0.3197, Train: 95.40%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 123, Loss: 0.3019, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 124, Loss: 0.3502, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 125, Loss: 0.3370, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 126, Loss: 0.3490, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 127, Loss: 0.3005, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 128, Loss: 0.3051, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 129, Loss: 0.2715, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 05, Epoch: 130, Loss: 0.3247, Train: 95.40%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 131, Loss: 0.2844, Train: 95.40%, Valid: 50.85% Test: 43.24%\n",
      "Run: 05, Epoch: 132, Loss: 0.4157, Train: 95.40%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 133, Loss: 0.2813, Train: 95.40%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 134, Loss: 0.3110, Train: 95.40%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 135, Loss: 0.3332, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 136, Loss: 0.3457, Train: 95.40%, Valid: 52.54% Test: 48.65%\n",
      "Run: 05, Epoch: 137, Loss: 0.3038, Train: 95.40%, Valid: 52.54% Test: 51.35%\n",
      "Run: 05, Epoch: 138, Loss: 0.3140, Train: 95.40%, Valid: 52.54% Test: 51.35%\n",
      "Run: 05, Epoch: 139, Loss: 0.3088, Train: 96.55%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 140, Loss: 0.3089, Train: 96.55%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 141, Loss: 0.3005, Train: 96.55%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 142, Loss: 0.3310, Train: 96.55%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 143, Loss: 0.2771, Train: 96.55%, Valid: 54.24% Test: 45.95%\n",
      "Run: 05, Epoch: 144, Loss: 0.2879, Train: 96.55%, Valid: 54.24% Test: 45.95%\n",
      "Run: 05, Epoch: 145, Loss: 0.3217, Train: 96.55%, Valid: 52.54% Test: 48.65%\n",
      "Run: 05, Epoch: 146, Loss: 0.2730, Train: 96.55%, Valid: 52.54% Test: 51.35%\n",
      "Run: 05, Epoch: 147, Loss: 0.2951, Train: 96.55%, Valid: 52.54% Test: 51.35%\n",
      "Run: 05, Epoch: 148, Loss: 0.2458, Train: 96.55%, Valid: 52.54% Test: 48.65%\n",
      "Run: 05, Epoch: 149, Loss: 0.2891, Train: 96.55%, Valid: 50.85% Test: 48.65%\n",
      "Run: 05, Epoch: 150, Loss: 0.3517, Train: 96.55%, Valid: 50.85% Test: 48.65%\n",
      "Run: 05, Epoch: 151, Loss: 0.2755, Train: 96.55%, Valid: 49.15% Test: 48.65%\n",
      "Run: 05, Epoch: 152, Loss: 0.2712, Train: 96.55%, Valid: 50.85% Test: 48.65%\n",
      "Run: 05, Epoch: 153, Loss: 0.2623, Train: 96.55%, Valid: 49.15% Test: 48.65%\n",
      "Run: 05, Epoch: 154, Loss: 0.2541, Train: 96.55%, Valid: 49.15% Test: 48.65%\n",
      "Run: 05, Epoch: 155, Loss: 0.2582, Train: 96.55%, Valid: 49.15% Test: 48.65%\n",
      "Run: 05, Epoch: 156, Loss: 0.2789, Train: 96.55%, Valid: 52.54% Test: 45.95%\n",
      "Run: 05, Epoch: 157, Loss: 0.2882, Train: 96.55%, Valid: 52.54% Test: 45.95%\n",
      "Run: 05, Epoch: 158, Loss: 0.2954, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 159, Loss: 0.2698, Train: 96.55%, Valid: 52.54% Test: 43.24%\n",
      "Run: 05, Epoch: 160, Loss: 0.2828, Train: 96.55%, Valid: 49.15% Test: 43.24%\n",
      "Run: 05, Epoch: 161, Loss: 0.3935, Train: 96.55%, Valid: 49.15% Test: 40.54%\n",
      "Run: 05, Epoch: 162, Loss: 0.2515, Train: 96.55%, Valid: 49.15% Test: 40.54%\n",
      "Run: 05, Epoch: 163, Loss: 0.2748, Train: 96.55%, Valid: 49.15% Test: 45.95%\n",
      "Run: 05, Epoch: 164, Loss: 0.2767, Train: 95.40%, Valid: 49.15% Test: 43.24%\n",
      "Run: 05, Epoch: 165, Loss: 0.2739, Train: 96.55%, Valid: 49.15% Test: 43.24%\n",
      "Run: 05, Epoch: 166, Loss: 0.2361, Train: 96.55%, Valid: 50.85% Test: 43.24%\n",
      "Run: 05, Epoch: 167, Loss: 0.3105, Train: 96.55%, Valid: 50.85% Test: 45.95%\n",
      "Run: 05, Epoch: 168, Loss: 0.2969, Train: 96.55%, Valid: 52.54% Test: 45.95%\n",
      "Run: 05, Epoch: 169, Loss: 0.2415, Train: 96.55%, Valid: 50.85% Test: 45.95%\n",
      "Run: 05, Epoch: 170, Loss: 0.2503, Train: 96.55%, Valid: 55.93% Test: 45.95%\n",
      "Run: 05, Epoch: 171, Loss: 0.2560, Train: 96.55%, Valid: 57.63% Test: 48.65%\n",
      "Run: 05, Epoch: 172, Loss: 0.2758, Train: 96.55%, Valid: 57.63% Test: 51.35%\n",
      "Run: 05, Epoch: 173, Loss: 0.2751, Train: 96.55%, Valid: 57.63% Test: 54.05%\n",
      "Run: 05, Epoch: 174, Loss: 0.2780, Train: 96.55%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 175, Loss: 0.2755, Train: 96.55%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 176, Loss: 0.2781, Train: 96.55%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 177, Loss: 0.2398, Train: 96.55%, Valid: 61.02% Test: 54.05%\n",
      "Run: 05, Epoch: 178, Loss: 0.2730, Train: 96.55%, Valid: 61.02% Test: 56.76%\n",
      "Run: 05, Epoch: 179, Loss: 0.2567, Train: 96.55%, Valid: 61.02% Test: 56.76%\n",
      "Run: 05, Epoch: 180, Loss: 0.2735, Train: 95.40%, Valid: 59.32% Test: 56.76%\n",
      "Run: 05, Epoch: 181, Loss: 0.2086, Train: 95.40%, Valid: 57.63% Test: 54.05%\n",
      "Run: 05, Epoch: 182, Loss: 0.2618, Train: 95.40%, Valid: 57.63% Test: 54.05%\n",
      "Run: 05, Epoch: 183, Loss: 0.2746, Train: 95.40%, Valid: 57.63% Test: 54.05%\n",
      "Run: 05, Epoch: 184, Loss: 0.2345, Train: 95.40%, Valid: 57.63% Test: 56.76%\n",
      "Run: 05, Epoch: 185, Loss: 0.2866, Train: 95.40%, Valid: 57.63% Test: 56.76%\n",
      "Run: 05, Epoch: 186, Loss: 0.2332, Train: 95.40%, Valid: 57.63% Test: 56.76%\n",
      "Run: 05, Epoch: 187, Loss: 0.2555, Train: 96.55%, Valid: 57.63% Test: 54.05%\n",
      "Run: 05, Epoch: 188, Loss: 0.2999, Train: 96.55%, Valid: 54.24% Test: 54.05%\n",
      "Run: 05, Epoch: 189, Loss: 0.3132, Train: 96.55%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 190, Loss: 0.3141, Train: 96.55%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 191, Loss: 0.2683, Train: 96.55%, Valid: 54.24% Test: 51.35%\n",
      "Run: 05, Epoch: 192, Loss: 0.2159, Train: 96.55%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 193, Loss: 0.3049, Train: 96.55%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 194, Loss: 0.2503, Train: 96.55%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 195, Loss: 0.2231, Train: 96.55%, Valid: 54.24% Test: 48.65%\n",
      "Run: 05, Epoch: 196, Loss: 0.3004, Train: 95.40%, Valid: 54.24% Test: 45.95%\n",
      "Run: 05, Epoch: 197, Loss: 0.2344, Train: 95.40%, Valid: 54.24% Test: 43.24%\n",
      "Run: 05, Epoch: 198, Loss: 0.2230, Train: 95.40%, Valid: 54.24% Test: 45.95%\n",
      "Run: 05, Epoch: 199, Loss: 0.2501, Train: 96.55%, Valid: 54.24% Test: 45.95%\n",
      "Run: 05, Epoch: 200, Loss: 0.2405, Train: 96.55%, Valid: 54.24% Test: 45.95%\n",
      "Run 05:\n",
      "Highest Train: 96.55\n",
      "Highest Valid: 67.80\n",
      "  Final Train: 78.16\n",
      "   Final Test: 59.46\n",
      "Run: 06, Epoch: 01, Loss: 1.6231, Train: 18.39%, Valid: 15.25% Test: 13.51%\n",
      "Run: 06, Epoch: 02, Loss: 1.5367, Train: 18.39%, Valid: 15.25% Test: 13.51%\n",
      "Run: 06, Epoch: 03, Loss: 1.4802, Train: 18.39%, Valid: 15.25% Test: 13.51%\n",
      "Run: 06, Epoch: 04, Loss: 1.4226, Train: 18.39%, Valid: 15.25% Test: 13.51%\n",
      "Run: 06, Epoch: 05, Loss: 1.3661, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 06, Loss: 1.3152, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 07, Loss: 1.2616, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 08, Loss: 1.2312, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 09, Loss: 1.1763, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 10, Loss: 1.1379, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 11, Loss: 1.1028, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 12, Loss: 1.0378, Train: 52.87%, Valid: 57.63% Test: 56.76%\n",
      "Run: 06, Epoch: 13, Loss: 1.0122, Train: 54.02%, Valid: 57.63% Test: 56.76%\n",
      "Run: 06, Epoch: 14, Loss: 0.9946, Train: 57.47%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 15, Loss: 1.0017, Train: 59.77%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 16, Loss: 0.9263, Train: 62.07%, Valid: 59.32% Test: 56.76%\n",
      "Run: 06, Epoch: 17, Loss: 0.9014, Train: 63.22%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 18, Loss: 0.8811, Train: 63.22%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 19, Loss: 0.8374, Train: 64.37%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 20, Loss: 0.8163, Train: 64.37%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 21, Loss: 0.8461, Train: 65.52%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 22, Loss: 0.7788, Train: 65.52%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 23, Loss: 0.8376, Train: 67.82%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 24, Loss: 0.8263, Train: 68.97%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 25, Loss: 0.8418, Train: 68.97%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 26, Loss: 0.7275, Train: 70.11%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 27, Loss: 0.7146, Train: 72.41%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 28, Loss: 0.7498, Train: 75.86%, Valid: 59.32% Test: 62.16%\n",
      "Run: 06, Epoch: 29, Loss: 0.7287, Train: 80.46%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 30, Loss: 0.7160, Train: 79.31%, Valid: 59.32% Test: 59.46%\n",
      "Run: 06, Epoch: 31, Loss: 0.6705, Train: 81.61%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 32, Loss: 0.7046, Train: 82.76%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 33, Loss: 0.6559, Train: 82.76%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 34, Loss: 0.6171, Train: 82.76%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 35, Loss: 0.6156, Train: 82.76%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 36, Loss: 0.6245, Train: 82.76%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 37, Loss: 0.6465, Train: 82.76%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 38, Loss: 0.5833, Train: 81.61%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 39, Loss: 0.5663, Train: 82.76%, Valid: 55.93% Test: 62.16%\n",
      "Run: 06, Epoch: 40, Loss: 0.6215, Train: 83.91%, Valid: 57.63% Test: 62.16%\n",
      "Run: 06, Epoch: 41, Loss: 0.5394, Train: 83.91%, Valid: 57.63% Test: 62.16%\n",
      "Run: 06, Epoch: 42, Loss: 0.6743, Train: 85.06%, Valid: 57.63% Test: 62.16%\n",
      "Run: 06, Epoch: 43, Loss: 0.5392, Train: 86.21%, Valid: 52.54% Test: 62.16%\n",
      "Run: 06, Epoch: 44, Loss: 0.5134, Train: 88.51%, Valid: 54.24% Test: 62.16%\n",
      "Run: 06, Epoch: 45, Loss: 0.5722, Train: 88.51%, Valid: 50.85% Test: 59.46%\n",
      "Run: 06, Epoch: 46, Loss: 0.5988, Train: 89.66%, Valid: 49.15% Test: 56.76%\n",
      "Run: 06, Epoch: 47, Loss: 0.5749, Train: 90.80%, Valid: 47.46% Test: 56.76%\n",
      "Run: 06, Epoch: 48, Loss: 0.5471, Train: 91.95%, Valid: 47.46% Test: 56.76%\n",
      "Run: 06, Epoch: 49, Loss: 0.6022, Train: 88.51%, Valid: 45.76% Test: 56.76%\n",
      "Run: 06, Epoch: 50, Loss: 0.6154, Train: 93.10%, Valid: 47.46% Test: 54.05%\n",
      "Run: 06, Epoch: 51, Loss: 0.5765, Train: 93.10%, Valid: 49.15% Test: 56.76%\n",
      "Run: 06, Epoch: 52, Loss: 0.5938, Train: 93.10%, Valid: 47.46% Test: 56.76%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 06, Epoch: 53, Loss: 0.4666, Train: 91.95%, Valid: 47.46% Test: 56.76%\n",
      "Run: 06, Epoch: 54, Loss: 0.4871, Train: 91.95%, Valid: 47.46% Test: 56.76%\n",
      "Run: 06, Epoch: 55, Loss: 0.5665, Train: 91.95%, Valid: 47.46% Test: 56.76%\n",
      "Run: 06, Epoch: 56, Loss: 0.5167, Train: 91.95%, Valid: 50.85% Test: 59.46%\n",
      "Run: 06, Epoch: 57, Loss: 0.5676, Train: 90.80%, Valid: 50.85% Test: 59.46%\n",
      "Run: 06, Epoch: 58, Loss: 0.4951, Train: 91.95%, Valid: 50.85% Test: 59.46%\n",
      "Run: 06, Epoch: 59, Loss: 0.5316, Train: 93.10%, Valid: 50.85% Test: 62.16%\n",
      "Run: 06, Epoch: 60, Loss: 0.5228, Train: 93.10%, Valid: 52.54% Test: 62.16%\n",
      "Run: 06, Epoch: 61, Loss: 0.4734, Train: 91.95%, Valid: 55.93% Test: 59.46%\n",
      "Run: 06, Epoch: 62, Loss: 0.4607, Train: 89.66%, Valid: 57.63% Test: 59.46%\n",
      "Run: 06, Epoch: 63, Loss: 0.5203, Train: 90.80%, Valid: 52.54% Test: 62.16%\n",
      "Run: 06, Epoch: 64, Loss: 0.4909, Train: 90.80%, Valid: 52.54% Test: 56.76%\n",
      "Run: 06, Epoch: 65, Loss: 0.4782, Train: 93.10%, Valid: 52.54% Test: 56.76%\n",
      "Run: 06, Epoch: 66, Loss: 0.4819, Train: 93.10%, Valid: 50.85% Test: 56.76%\n",
      "Run: 06, Epoch: 67, Loss: 0.4498, Train: 91.95%, Valid: 52.54% Test: 56.76%\n",
      "Run: 06, Epoch: 68, Loss: 0.4398, Train: 93.10%, Valid: 54.24% Test: 56.76%\n",
      "Run: 06, Epoch: 69, Loss: 0.4595, Train: 93.10%, Valid: 52.54% Test: 56.76%\n",
      "Run: 06, Epoch: 70, Loss: 0.5200, Train: 94.25%, Valid: 50.85% Test: 56.76%\n",
      "Run: 06, Epoch: 71, Loss: 0.4255, Train: 94.25%, Valid: 50.85% Test: 56.76%\n",
      "Run: 06, Epoch: 72, Loss: 0.4397, Train: 94.25%, Valid: 49.15% Test: 51.35%\n",
      "Run: 06, Epoch: 73, Loss: 0.4463, Train: 94.25%, Valid: 49.15% Test: 51.35%\n",
      "Run: 06, Epoch: 74, Loss: 0.4389, Train: 95.40%, Valid: 47.46% Test: 48.65%\n",
      "Run: 06, Epoch: 75, Loss: 0.4485, Train: 95.40%, Valid: 47.46% Test: 51.35%\n",
      "Run: 06, Epoch: 76, Loss: 0.4505, Train: 95.40%, Valid: 47.46% Test: 51.35%\n",
      "Run: 06, Epoch: 77, Loss: 0.4777, Train: 95.40%, Valid: 49.15% Test: 51.35%\n",
      "Run: 06, Epoch: 78, Loss: 0.4342, Train: 95.40%, Valid: 49.15% Test: 51.35%\n",
      "Run: 06, Epoch: 79, Loss: 0.4224, Train: 95.40%, Valid: 47.46% Test: 51.35%\n",
      "Run: 06, Epoch: 80, Loss: 0.4098, Train: 95.40%, Valid: 47.46% Test: 51.35%\n",
      "Run: 06, Epoch: 81, Loss: 0.4004, Train: 95.40%, Valid: 47.46% Test: 51.35%\n",
      "Run: 06, Epoch: 82, Loss: 0.4280, Train: 95.40%, Valid: 45.76% Test: 54.05%\n",
      "Run: 06, Epoch: 83, Loss: 0.4505, Train: 95.40%, Valid: 45.76% Test: 54.05%\n",
      "Run: 06, Epoch: 84, Loss: 0.3965, Train: 95.40%, Valid: 45.76% Test: 51.35%\n",
      "Run: 06, Epoch: 85, Loss: 0.3316, Train: 94.25%, Valid: 44.07% Test: 54.05%\n",
      "Run: 06, Epoch: 86, Loss: 0.4444, Train: 94.25%, Valid: 44.07% Test: 54.05%\n",
      "Run: 06, Epoch: 87, Loss: 0.4879, Train: 94.25%, Valid: 45.76% Test: 51.35%\n",
      "Run: 06, Epoch: 88, Loss: 0.3390, Train: 94.25%, Valid: 47.46% Test: 51.35%\n",
      "Run: 06, Epoch: 89, Loss: 0.3857, Train: 94.25%, Valid: 47.46% Test: 51.35%\n",
      "Run: 06, Epoch: 90, Loss: 0.3978, Train: 95.40%, Valid: 47.46% Test: 51.35%\n",
      "Run: 06, Epoch: 91, Loss: 0.4199, Train: 95.40%, Valid: 45.76% Test: 54.05%\n",
      "Run: 06, Epoch: 92, Loss: 0.3636, Train: 95.40%, Valid: 45.76% Test: 54.05%\n",
      "Run: 06, Epoch: 93, Loss: 0.3803, Train: 95.40%, Valid: 45.76% Test: 51.35%\n",
      "Run: 06, Epoch: 94, Loss: 0.3261, Train: 95.40%, Valid: 45.76% Test: 51.35%\n",
      "Run: 06, Epoch: 95, Loss: 0.3209, Train: 95.40%, Valid: 44.07% Test: 51.35%\n",
      "Run: 06, Epoch: 96, Loss: 0.3947, Train: 95.40%, Valid: 44.07% Test: 48.65%\n",
      "Run: 06, Epoch: 97, Loss: 0.3457, Train: 95.40%, Valid: 44.07% Test: 48.65%\n",
      "Run: 06, Epoch: 98, Loss: 0.3344, Train: 95.40%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 99, Loss: 0.3542, Train: 95.40%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 100, Loss: 0.3790, Train: 96.55%, Valid: 42.37% Test: 51.35%\n",
      "Run: 06, Epoch: 101, Loss: 0.4449, Train: 97.70%, Valid: 42.37% Test: 51.35%\n",
      "Run: 06, Epoch: 102, Loss: 0.3438, Train: 97.70%, Valid: 42.37% Test: 51.35%\n",
      "Run: 06, Epoch: 103, Loss: 0.2827, Train: 96.55%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 104, Loss: 0.3116, Train: 96.55%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 105, Loss: 0.3036, Train: 96.55%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 106, Loss: 0.3382, Train: 96.55%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 107, Loss: 0.3851, Train: 96.55%, Valid: 42.37% Test: 51.35%\n",
      "Run: 06, Epoch: 108, Loss: 0.3937, Train: 96.55%, Valid: 42.37% Test: 51.35%\n",
      "Run: 06, Epoch: 109, Loss: 0.2964, Train: 96.55%, Valid: 42.37% Test: 54.05%\n",
      "Run: 06, Epoch: 110, Loss: 0.3688, Train: 96.55%, Valid: 42.37% Test: 51.35%\n",
      "Run: 06, Epoch: 111, Loss: 0.3744, Train: 97.70%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 112, Loss: 0.3547, Train: 97.70%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 113, Loss: 0.3349, Train: 96.55%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 114, Loss: 0.4295, Train: 96.55%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 115, Loss: 0.3303, Train: 96.55%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 116, Loss: 0.3888, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 117, Loss: 0.3368, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 118, Loss: 0.2928, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 119, Loss: 0.3627, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 120, Loss: 0.3325, Train: 96.55%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 121, Loss: 0.3315, Train: 96.55%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 122, Loss: 0.2954, Train: 96.55%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 123, Loss: 0.2702, Train: 96.55%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 124, Loss: 0.2943, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 125, Loss: 0.4086, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 126, Loss: 0.2604, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 127, Loss: 0.3440, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 128, Loss: 0.2358, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 129, Loss: 0.3151, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 130, Loss: 0.2732, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 131, Loss: 0.3048, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 132, Loss: 0.3232, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 133, Loss: 0.2586, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 134, Loss: 0.3451, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 135, Loss: 0.2643, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 136, Loss: 0.2973, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 137, Loss: 0.3282, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 138, Loss: 0.2927, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 139, Loss: 0.2669, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 140, Loss: 0.2506, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 141, Loss: 0.2958, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 142, Loss: 0.2739, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 143, Loss: 0.2781, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 144, Loss: 0.3110, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 145, Loss: 0.2315, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 146, Loss: 0.2940, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 147, Loss: 0.2758, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 148, Loss: 0.3247, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 149, Loss: 0.3973, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 150, Loss: 0.2799, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 151, Loss: 0.1970, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 152, Loss: 0.2285, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 153, Loss: 0.2910, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 154, Loss: 0.3903, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 155, Loss: 0.2588, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 156, Loss: 0.3566, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 157, Loss: 0.3442, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 158, Loss: 0.2868, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 159, Loss: 0.2544, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 160, Loss: 0.2598, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 161, Loss: 0.3579, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 162, Loss: 0.2985, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 163, Loss: 0.2523, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 164, Loss: 0.2209, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 165, Loss: 0.2381, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 166, Loss: 0.2335, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 167, Loss: 0.2763, Train: 97.70%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 168, Loss: 0.2190, Train: 97.70%, Valid: 40.68% Test: 48.65%\n",
      "Run: 06, Epoch: 169, Loss: 0.2516, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 170, Loss: 0.2782, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 171, Loss: 0.2739, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 172, Loss: 0.2569, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 173, Loss: 0.2109, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 174, Loss: 0.3042, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 175, Loss: 0.2925, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 176, Loss: 0.3101, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 177, Loss: 0.3354, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 178, Loss: 0.2751, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 179, Loss: 0.2206, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 180, Loss: 0.2269, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 181, Loss: 0.2571, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 182, Loss: 0.2388, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 183, Loss: 0.2283, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 184, Loss: 0.2466, Train: 97.70%, Valid: 44.07% Test: 48.65%\n",
      "Run: 06, Epoch: 185, Loss: 0.2999, Train: 97.70%, Valid: 44.07% Test: 48.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 06, Epoch: 186, Loss: 0.2857, Train: 97.70%, Valid: 44.07% Test: 48.65%\n",
      "Run: 06, Epoch: 187, Loss: 0.2641, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 188, Loss: 0.2345, Train: 97.70%, Valid: 44.07% Test: 45.95%\n",
      "Run: 06, Epoch: 189, Loss: 0.2567, Train: 97.70%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 190, Loss: 0.2232, Train: 97.70%, Valid: 42.37% Test: 48.65%\n",
      "Run: 06, Epoch: 191, Loss: 0.1740, Train: 97.70%, Valid: 40.68% Test: 48.65%\n",
      "Run: 06, Epoch: 192, Loss: 0.2319, Train: 96.55%, Valid: 40.68% Test: 48.65%\n",
      "Run: 06, Epoch: 193, Loss: 0.2920, Train: 97.70%, Valid: 40.68% Test: 45.95%\n",
      "Run: 06, Epoch: 194, Loss: 0.1873, Train: 97.70%, Valid: 40.68% Test: 45.95%\n",
      "Run: 06, Epoch: 195, Loss: 0.2446, Train: 97.70%, Valid: 40.68% Test: 45.95%\n",
      "Run: 06, Epoch: 196, Loss: 0.2341, Train: 97.70%, Valid: 40.68% Test: 45.95%\n",
      "Run: 06, Epoch: 197, Loss: 0.2725, Train: 97.70%, Valid: 42.37% Test: 45.95%\n",
      "Run: 06, Epoch: 198, Loss: 0.2298, Train: 97.70%, Valid: 40.68% Test: 45.95%\n",
      "Run: 06, Epoch: 199, Loss: 0.2796, Train: 97.70%, Valid: 40.68% Test: 45.95%\n",
      "Run: 06, Epoch: 200, Loss: 0.3277, Train: 97.70%, Valid: 40.68% Test: 45.95%\n",
      "Run 06:\n",
      "Highest Train: 97.70\n",
      "Highest Valid: 59.32\n",
      "  Final Train: 51.72\n",
      "   Final Test: 56.76\n",
      "Run: 07, Epoch: 01, Loss: 1.6377, Train: 12.64%, Valid: 6.78% Test: 8.11%\n",
      "Run: 07, Epoch: 02, Loss: 1.5288, Train: 12.64%, Valid: 6.78% Test: 8.11%\n",
      "Run: 07, Epoch: 03, Loss: 1.4519, Train: 12.64%, Valid: 6.78% Test: 8.11%\n",
      "Run: 07, Epoch: 04, Loss: 1.3858, Train: 54.02%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 05, Loss: 1.3443, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 06, Loss: 1.2919, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 07, Loss: 1.2081, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 08, Loss: 1.1717, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 09, Loss: 1.1209, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 10, Loss: 1.0960, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 11, Loss: 1.0365, Train: 51.72%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 12, Loss: 0.9923, Train: 51.72%, Valid: 61.02% Test: 56.76%\n",
      "Run: 07, Epoch: 13, Loss: 0.9848, Train: 51.72%, Valid: 61.02% Test: 56.76%\n",
      "Run: 07, Epoch: 14, Loss: 0.9303, Train: 51.72%, Valid: 61.02% Test: 56.76%\n",
      "Run: 07, Epoch: 15, Loss: 0.9038, Train: 54.02%, Valid: 62.71% Test: 56.76%\n",
      "Run: 07, Epoch: 16, Loss: 0.8948, Train: 55.17%, Valid: 64.41% Test: 56.76%\n",
      "Run: 07, Epoch: 17, Loss: 0.8813, Train: 58.62%, Valid: 66.10% Test: 56.76%\n",
      "Run: 07, Epoch: 18, Loss: 0.8759, Train: 60.92%, Valid: 64.41% Test: 56.76%\n",
      "Run: 07, Epoch: 19, Loss: 0.8428, Train: 60.92%, Valid: 64.41% Test: 56.76%\n",
      "Run: 07, Epoch: 20, Loss: 0.8121, Train: 63.22%, Valid: 62.71% Test: 56.76%\n",
      "Run: 07, Epoch: 21, Loss: 0.7965, Train: 63.22%, Valid: 64.41% Test: 56.76%\n",
      "Run: 07, Epoch: 22, Loss: 0.7770, Train: 63.22%, Valid: 64.41% Test: 56.76%\n",
      "Run: 07, Epoch: 23, Loss: 0.7491, Train: 64.37%, Valid: 64.41% Test: 56.76%\n",
      "Run: 07, Epoch: 24, Loss: 0.7455, Train: 65.52%, Valid: 64.41% Test: 56.76%\n",
      "Run: 07, Epoch: 25, Loss: 0.7840, Train: 64.37%, Valid: 57.63% Test: 56.76%\n",
      "Run: 07, Epoch: 26, Loss: 0.7332, Train: 65.52%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 27, Loss: 0.6997, Train: 67.82%, Valid: 59.32% Test: 56.76%\n",
      "Run: 07, Epoch: 28, Loss: 0.7139, Train: 68.97%, Valid: 55.93% Test: 62.16%\n",
      "Run: 07, Epoch: 29, Loss: 0.7049, Train: 70.11%, Valid: 57.63% Test: 62.16%\n",
      "Run: 07, Epoch: 30, Loss: 0.7268, Train: 71.26%, Valid: 57.63% Test: 56.76%\n",
      "Run: 07, Epoch: 31, Loss: 0.6786, Train: 73.56%, Valid: 54.24% Test: 54.05%\n",
      "Run: 07, Epoch: 32, Loss: 0.6518, Train: 70.11%, Valid: 55.93% Test: 54.05%\n",
      "Run: 07, Epoch: 33, Loss: 0.6666, Train: 71.26%, Valid: 54.24% Test: 54.05%\n",
      "Run: 07, Epoch: 34, Loss: 0.6705, Train: 72.41%, Valid: 54.24% Test: 54.05%\n",
      "Run: 07, Epoch: 35, Loss: 0.6147, Train: 73.56%, Valid: 54.24% Test: 54.05%\n",
      "Run: 07, Epoch: 36, Loss: 0.6181, Train: 79.31%, Valid: 57.63% Test: 54.05%\n",
      "Run: 07, Epoch: 37, Loss: 0.6207, Train: 80.46%, Valid: 55.93% Test: 56.76%\n",
      "Run: 07, Epoch: 38, Loss: 0.6149, Train: 80.46%, Valid: 55.93% Test: 56.76%\n",
      "Run: 07, Epoch: 39, Loss: 0.5889, Train: 81.61%, Valid: 55.93% Test: 56.76%\n",
      "Run: 07, Epoch: 40, Loss: 0.6080, Train: 81.61%, Valid: 54.24% Test: 56.76%\n",
      "Run: 07, Epoch: 41, Loss: 0.6124, Train: 83.91%, Valid: 57.63% Test: 56.76%\n",
      "Run: 07, Epoch: 42, Loss: 0.5319, Train: 85.06%, Valid: 57.63% Test: 56.76%\n",
      "Run: 07, Epoch: 43, Loss: 0.5621, Train: 86.21%, Valid: 55.93% Test: 59.46%\n",
      "Run: 07, Epoch: 44, Loss: 0.5864, Train: 90.80%, Valid: 52.54% Test: 59.46%\n",
      "Run: 07, Epoch: 45, Loss: 0.5850, Train: 90.80%, Valid: 50.85% Test: 59.46%\n",
      "Run: 07, Epoch: 46, Loss: 0.6093, Train: 90.80%, Valid: 50.85% Test: 59.46%\n",
      "Run: 07, Epoch: 47, Loss: 0.5662, Train: 90.80%, Valid: 52.54% Test: 56.76%\n",
      "Run: 07, Epoch: 48, Loss: 0.5251, Train: 90.80%, Valid: 52.54% Test: 56.76%\n",
      "Run: 07, Epoch: 49, Loss: 0.5501, Train: 90.80%, Valid: 50.85% Test: 59.46%\n",
      "Run: 07, Epoch: 50, Loss: 0.5181, Train: 91.95%, Valid: 49.15% Test: 59.46%\n",
      "Run: 07, Epoch: 51, Loss: 0.5107, Train: 91.95%, Valid: 49.15% Test: 56.76%\n",
      "Run: 07, Epoch: 52, Loss: 0.5057, Train: 94.25%, Valid: 49.15% Test: 56.76%\n",
      "Run: 07, Epoch: 53, Loss: 0.5127, Train: 94.25%, Valid: 49.15% Test: 56.76%\n",
      "Run: 07, Epoch: 54, Loss: 0.5115, Train: 94.25%, Valid: 49.15% Test: 59.46%\n",
      "Run: 07, Epoch: 55, Loss: 0.4817, Train: 94.25%, Valid: 49.15% Test: 59.46%\n",
      "Run: 07, Epoch: 56, Loss: 0.5059, Train: 94.25%, Valid: 49.15% Test: 56.76%\n",
      "Run: 07, Epoch: 57, Loss: 0.4992, Train: 95.40%, Valid: 49.15% Test: 54.05%\n",
      "Run: 07, Epoch: 58, Loss: 0.4898, Train: 95.40%, Valid: 47.46% Test: 54.05%\n",
      "Run: 07, Epoch: 59, Loss: 0.5021, Train: 95.40%, Valid: 47.46% Test: 54.05%\n",
      "Run: 07, Epoch: 60, Loss: 0.4914, Train: 95.40%, Valid: 47.46% Test: 51.35%\n",
      "Run: 07, Epoch: 61, Loss: 0.4642, Train: 94.25%, Valid: 47.46% Test: 51.35%\n",
      "Run: 07, Epoch: 62, Loss: 0.4848, Train: 93.10%, Valid: 45.76% Test: 48.65%\n",
      "Run: 07, Epoch: 63, Loss: 0.5135, Train: 93.10%, Valid: 45.76% Test: 51.35%\n",
      "Run: 07, Epoch: 64, Loss: 0.4282, Train: 93.10%, Valid: 45.76% Test: 48.65%\n",
      "Run: 07, Epoch: 65, Loss: 0.4379, Train: 93.10%, Valid: 45.76% Test: 48.65%\n",
      "Run: 07, Epoch: 66, Loss: 0.4198, Train: 93.10%, Valid: 45.76% Test: 48.65%\n",
      "Run: 07, Epoch: 67, Loss: 0.4725, Train: 93.10%, Valid: 45.76% Test: 48.65%\n",
      "Run: 07, Epoch: 68, Loss: 0.4696, Train: 93.10%, Valid: 45.76% Test: 48.65%\n",
      "Run: 07, Epoch: 69, Loss: 0.4586, Train: 93.10%, Valid: 45.76% Test: 48.65%\n",
      "Run: 07, Epoch: 70, Loss: 0.4980, Train: 93.10%, Valid: 45.76% Test: 54.05%\n",
      "Run: 07, Epoch: 71, Loss: 0.3777, Train: 93.10%, Valid: 45.76% Test: 54.05%\n",
      "Run: 07, Epoch: 72, Loss: 0.4911, Train: 95.40%, Valid: 44.07% Test: 54.05%\n",
      "Run: 07, Epoch: 73, Loss: 0.4415, Train: 95.40%, Valid: 44.07% Test: 56.76%\n",
      "Run: 07, Epoch: 74, Loss: 0.4142, Train: 94.25%, Valid: 42.37% Test: 56.76%\n",
      "Run: 07, Epoch: 75, Loss: 0.3971, Train: 94.25%, Valid: 42.37% Test: 56.76%\n",
      "Run: 07, Epoch: 76, Loss: 0.4461, Train: 94.25%, Valid: 44.07% Test: 56.76%\n",
      "Run: 07, Epoch: 77, Loss: 0.3622, Train: 94.25%, Valid: 45.76% Test: 56.76%\n",
      "Run: 07, Epoch: 78, Loss: 0.3914, Train: 95.40%, Valid: 47.46% Test: 56.76%\n",
      "Run: 07, Epoch: 79, Loss: 0.4085, Train: 95.40%, Valid: 47.46% Test: 56.76%\n",
      "Run: 07, Epoch: 80, Loss: 0.3987, Train: 95.40%, Valid: 45.76% Test: 56.76%\n",
      "Run: 07, Epoch: 81, Loss: 0.3778, Train: 95.40%, Valid: 44.07% Test: 56.76%\n",
      "Run: 07, Epoch: 82, Loss: 0.3719, Train: 96.55%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 83, Loss: 0.3699, Train: 96.55%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 84, Loss: 0.4045, Train: 96.55%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 85, Loss: 0.3422, Train: 96.55%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 86, Loss: 0.3531, Train: 96.55%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 87, Loss: 0.3478, Train: 95.40%, Valid: 42.37% Test: 56.76%\n",
      "Run: 07, Epoch: 88, Loss: 0.3244, Train: 94.25%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 89, Loss: 0.3675, Train: 95.40%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 90, Loss: 0.3698, Train: 95.40%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 91, Loss: 0.3462, Train: 95.40%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 92, Loss: 0.3860, Train: 95.40%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 93, Loss: 0.3350, Train: 96.55%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 94, Loss: 0.3872, Train: 95.40%, Valid: 42.37% Test: 56.76%\n",
      "Run: 07, Epoch: 95, Loss: 0.4153, Train: 95.40%, Valid: 42.37% Test: 56.76%\n",
      "Run: 07, Epoch: 96, Loss: 0.4013, Train: 95.40%, Valid: 42.37% Test: 56.76%\n",
      "Run: 07, Epoch: 97, Loss: 0.4399, Train: 95.40%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 98, Loss: 0.3649, Train: 95.40%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 99, Loss: 0.3130, Train: 95.40%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 100, Loss: 0.3364, Train: 95.40%, Valid: 40.68% Test: 56.76%\n",
      "Run: 07, Epoch: 101, Loss: 0.3380, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 102, Loss: 0.3230, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 103, Loss: 0.3235, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 104, Loss: 0.3052, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 105, Loss: 0.2763, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 106, Loss: 0.3162, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 107, Loss: 0.3335, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 108, Loss: 0.3502, Train: 94.25%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 109, Loss: 0.4196, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 110, Loss: 0.3715, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 111, Loss: 0.3184, Train: 95.40%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 112, Loss: 0.2991, Train: 95.40%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 113, Loss: 0.4173, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 114, Loss: 0.3493, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 115, Loss: 0.3847, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 116, Loss: 0.2489, Train: 95.40%, Valid: 38.98% Test: 54.05%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 07, Epoch: 117, Loss: 0.3502, Train: 95.40%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 118, Loss: 0.2982, Train: 96.55%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 119, Loss: 0.3277, Train: 96.55%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 120, Loss: 0.2969, Train: 96.55%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 121, Loss: 0.2982, Train: 96.55%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 122, Loss: 0.3068, Train: 96.55%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 123, Loss: 0.2965, Train: 95.40%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 124, Loss: 0.3172, Train: 96.55%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 125, Loss: 0.3482, Train: 96.55%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 126, Loss: 0.2830, Train: 95.40%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 127, Loss: 0.3564, Train: 96.55%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 128, Loss: 0.3091, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 129, Loss: 0.2572, Train: 97.70%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 130, Loss: 0.3105, Train: 97.70%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 131, Loss: 0.2496, Train: 97.70%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 132, Loss: 0.3077, Train: 97.70%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 133, Loss: 0.2686, Train: 97.70%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 134, Loss: 0.2698, Train: 97.70%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 135, Loss: 0.3515, Train: 97.70%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 136, Loss: 0.2532, Train: 97.70%, Valid: 40.68% Test: 54.05%\n",
      "Run: 07, Epoch: 137, Loss: 0.3180, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 138, Loss: 0.2584, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 139, Loss: 0.2414, Train: 97.70%, Valid: 38.98% Test: 56.76%\n",
      "Run: 07, Epoch: 140, Loss: 0.3307, Train: 97.70%, Valid: 38.98% Test: 56.76%\n",
      "Run: 07, Epoch: 141, Loss: 0.3442, Train: 97.70%, Valid: 38.98% Test: 56.76%\n",
      "Run: 07, Epoch: 142, Loss: 0.2626, Train: 96.55%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 143, Loss: 0.2229, Train: 96.55%, Valid: 38.98% Test: 48.65%\n",
      "Run: 07, Epoch: 144, Loss: 0.2606, Train: 96.55%, Valid: 37.29% Test: 43.24%\n",
      "Run: 07, Epoch: 145, Loss: 0.3028, Train: 96.55%, Valid: 37.29% Test: 48.65%\n",
      "Run: 07, Epoch: 146, Loss: 0.3396, Train: 96.55%, Valid: 38.98% Test: 48.65%\n",
      "Run: 07, Epoch: 147, Loss: 0.3076, Train: 96.55%, Valid: 38.98% Test: 45.95%\n",
      "Run: 07, Epoch: 148, Loss: 0.2386, Train: 96.55%, Valid: 40.68% Test: 45.95%\n",
      "Run: 07, Epoch: 149, Loss: 0.2115, Train: 96.55%, Valid: 40.68% Test: 45.95%\n",
      "Run: 07, Epoch: 150, Loss: 0.2343, Train: 97.70%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 151, Loss: 0.3288, Train: 96.55%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 152, Loss: 0.3462, Train: 96.55%, Valid: 40.68% Test: 45.95%\n",
      "Run: 07, Epoch: 153, Loss: 0.2729, Train: 96.55%, Valid: 38.98% Test: 45.95%\n",
      "Run: 07, Epoch: 154, Loss: 0.2919, Train: 96.55%, Valid: 40.68% Test: 45.95%\n",
      "Run: 07, Epoch: 155, Loss: 0.2639, Train: 96.55%, Valid: 38.98% Test: 45.95%\n",
      "Run: 07, Epoch: 156, Loss: 0.2182, Train: 96.55%, Valid: 38.98% Test: 45.95%\n",
      "Run: 07, Epoch: 157, Loss: 0.2714, Train: 97.70%, Valid: 38.98% Test: 45.95%\n",
      "Run: 07, Epoch: 158, Loss: 0.2714, Train: 97.70%, Valid: 38.98% Test: 45.95%\n",
      "Run: 07, Epoch: 159, Loss: 0.3126, Train: 97.70%, Valid: 38.98% Test: 48.65%\n",
      "Run: 07, Epoch: 160, Loss: 0.2923, Train: 97.70%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 161, Loss: 0.2233, Train: 97.70%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 162, Loss: 0.3824, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 163, Loss: 0.2735, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 164, Loss: 0.2931, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 165, Loss: 0.2445, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 166, Loss: 0.3497, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 167, Loss: 0.2370, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 168, Loss: 0.2697, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 169, Loss: 0.2528, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 170, Loss: 0.2529, Train: 97.70%, Valid: 38.98% Test: 54.05%\n",
      "Run: 07, Epoch: 171, Loss: 0.2163, Train: 97.70%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 172, Loss: 0.1899, Train: 97.70%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 173, Loss: 0.2558, Train: 96.55%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 174, Loss: 0.2208, Train: 96.55%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 175, Loss: 0.2439, Train: 96.55%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 176, Loss: 0.2139, Train: 96.55%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 177, Loss: 0.2770, Train: 96.55%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 178, Loss: 0.2301, Train: 96.55%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 179, Loss: 0.2405, Train: 96.55%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 180, Loss: 0.2460, Train: 96.55%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 181, Loss: 0.2600, Train: 97.70%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 182, Loss: 0.2227, Train: 97.70%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 183, Loss: 0.2380, Train: 96.55%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 184, Loss: 0.2352, Train: 96.55%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 185, Loss: 0.1825, Train: 96.55%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 186, Loss: 0.2408, Train: 96.55%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 187, Loss: 0.2273, Train: 96.55%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 188, Loss: 0.2539, Train: 96.55%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 189, Loss: 0.2258, Train: 96.55%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 190, Loss: 0.2068, Train: 96.55%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 191, Loss: 0.2441, Train: 96.55%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 192, Loss: 0.3266, Train: 97.70%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 193, Loss: 0.2845, Train: 97.70%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 194, Loss: 0.2130, Train: 97.70%, Valid: 40.68% Test: 51.35%\n",
      "Run: 07, Epoch: 195, Loss: 0.2403, Train: 97.70%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 196, Loss: 0.2685, Train: 97.70%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 197, Loss: 0.1810, Train: 97.70%, Valid: 40.68% Test: 48.65%\n",
      "Run: 07, Epoch: 198, Loss: 0.1887, Train: 97.70%, Valid: 38.98% Test: 51.35%\n",
      "Run: 07, Epoch: 199, Loss: 0.2009, Train: 97.70%, Valid: 37.29% Test: 51.35%\n",
      "Run: 07, Epoch: 200, Loss: 0.2838, Train: 97.70%, Valid: 37.29% Test: 51.35%\n",
      "Run 07:\n",
      "Highest Train: 97.70\n",
      "Highest Valid: 66.10\n",
      "  Final Train: 58.62\n",
      "   Final Test: 56.76\n",
      "Run: 08, Epoch: 01, Loss: 1.5762, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 02, Loss: 1.4724, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 03, Loss: 1.4300, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 04, Loss: 1.3599, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 05, Loss: 1.3050, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 06, Loss: 1.2649, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 07, Loss: 1.2102, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 08, Loss: 1.1557, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 09, Loss: 1.1071, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 10, Loss: 1.0728, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 11, Loss: 1.0497, Train: 47.13%, Valid: 62.71% Test: 62.16%\n",
      "Run: 08, Epoch: 12, Loss: 1.0204, Train: 48.28%, Valid: 64.41% Test: 62.16%\n",
      "Run: 08, Epoch: 13, Loss: 0.9596, Train: 49.43%, Valid: 64.41% Test: 62.16%\n",
      "Run: 08, Epoch: 14, Loss: 0.9297, Train: 56.32%, Valid: 64.41% Test: 59.46%\n",
      "Run: 08, Epoch: 15, Loss: 0.9349, Train: 60.92%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 16, Loss: 0.8897, Train: 63.22%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 17, Loss: 0.8747, Train: 63.22%, Valid: 64.41% Test: 59.46%\n",
      "Run: 08, Epoch: 18, Loss: 0.8505, Train: 65.52%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 19, Loss: 0.8445, Train: 66.67%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 20, Loss: 0.7959, Train: 66.67%, Valid: 64.41% Test: 62.16%\n",
      "Run: 08, Epoch: 21, Loss: 0.7876, Train: 66.67%, Valid: 66.10% Test: 62.16%\n",
      "Run: 08, Epoch: 22, Loss: 0.7906, Train: 66.67%, Valid: 66.10% Test: 62.16%\n",
      "Run: 08, Epoch: 23, Loss: 0.7646, Train: 67.82%, Valid: 67.80% Test: 62.16%\n",
      "Run: 08, Epoch: 24, Loss: 0.7631, Train: 67.82%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 25, Loss: 0.7257, Train: 70.11%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 26, Loss: 0.7295, Train: 71.26%, Valid: 67.80% Test: 59.46%\n",
      "Run: 08, Epoch: 27, Loss: 0.7476, Train: 71.26%, Valid: 67.80% Test: 59.46%\n",
      "Run: 08, Epoch: 28, Loss: 0.6952, Train: 74.71%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 29, Loss: 0.7317, Train: 78.16%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 30, Loss: 0.6760, Train: 78.16%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 31, Loss: 0.7126, Train: 78.16%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 32, Loss: 0.6767, Train: 77.01%, Valid: 66.10% Test: 59.46%\n",
      "Run: 08, Epoch: 33, Loss: 0.6139, Train: 80.46%, Valid: 62.71% Test: 59.46%\n",
      "Run: 08, Epoch: 34, Loss: 0.6202, Train: 80.46%, Valid: 59.32% Test: 59.46%\n",
      "Run: 08, Epoch: 35, Loss: 0.6400, Train: 82.76%, Valid: 57.63% Test: 56.76%\n",
      "Run: 08, Epoch: 36, Loss: 0.6501, Train: 86.21%, Valid: 59.32% Test: 54.05%\n",
      "Run: 08, Epoch: 37, Loss: 0.5977, Train: 88.51%, Valid: 57.63% Test: 56.76%\n",
      "Run: 08, Epoch: 38, Loss: 0.6297, Train: 88.51%, Valid: 57.63% Test: 56.76%\n",
      "Run: 08, Epoch: 39, Loss: 0.5953, Train: 88.51%, Valid: 59.32% Test: 56.76%\n",
      "Run: 08, Epoch: 40, Loss: 0.6017, Train: 88.51%, Valid: 55.93% Test: 56.76%\n",
      "Run: 08, Epoch: 41, Loss: 0.6177, Train: 88.51%, Valid: 54.24% Test: 56.76%\n",
      "Run: 08, Epoch: 42, Loss: 0.5723, Train: 88.51%, Valid: 54.24% Test: 56.76%\n",
      "Run: 08, Epoch: 43, Loss: 0.5608, Train: 88.51%, Valid: 54.24% Test: 56.76%\n",
      "Run: 08, Epoch: 44, Loss: 0.5414, Train: 88.51%, Valid: 54.24% Test: 56.76%\n",
      "Run: 08, Epoch: 45, Loss: 0.5615, Train: 88.51%, Valid: 57.63% Test: 51.35%\n",
      "Run: 08, Epoch: 46, Loss: 0.5556, Train: 89.66%, Valid: 55.93% Test: 45.95%\n",
      "Run: 08, Epoch: 47, Loss: 0.5556, Train: 90.80%, Valid: 55.93% Test: 45.95%\n",
      "Run: 08, Epoch: 48, Loss: 0.5208, Train: 89.66%, Valid: 57.63% Test: 45.95%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 08, Epoch: 49, Loss: 0.5532, Train: 89.66%, Valid: 59.32% Test: 48.65%\n",
      "Run: 08, Epoch: 50, Loss: 0.4972, Train: 90.80%, Valid: 59.32% Test: 48.65%\n",
      "Run: 08, Epoch: 51, Loss: 0.4984, Train: 90.80%, Valid: 59.32% Test: 51.35%\n",
      "Run: 08, Epoch: 52, Loss: 0.5164, Train: 90.80%, Valid: 61.02% Test: 51.35%\n",
      "Run: 08, Epoch: 53, Loss: 0.4834, Train: 91.95%, Valid: 61.02% Test: 51.35%\n",
      "Run: 08, Epoch: 54, Loss: 0.5305, Train: 91.95%, Valid: 61.02% Test: 54.05%\n",
      "Run: 08, Epoch: 55, Loss: 0.5261, Train: 90.80%, Valid: 62.71% Test: 51.35%\n",
      "Run: 08, Epoch: 56, Loss: 0.4970, Train: 89.66%, Valid: 64.41% Test: 51.35%\n",
      "Run: 08, Epoch: 57, Loss: 0.4803, Train: 89.66%, Valid: 61.02% Test: 51.35%\n",
      "Run: 08, Epoch: 58, Loss: 0.4718, Train: 89.66%, Valid: 61.02% Test: 51.35%\n",
      "Run: 08, Epoch: 59, Loss: 0.4671, Train: 88.51%, Valid: 62.71% Test: 51.35%\n",
      "Run: 08, Epoch: 60, Loss: 0.4446, Train: 88.51%, Valid: 62.71% Test: 51.35%\n",
      "Run: 08, Epoch: 61, Loss: 0.5256, Train: 89.66%, Valid: 62.71% Test: 51.35%\n",
      "Run: 08, Epoch: 62, Loss: 0.4581, Train: 89.66%, Valid: 62.71% Test: 54.05%\n",
      "Run: 08, Epoch: 63, Loss: 0.4720, Train: 89.66%, Valid: 62.71% Test: 51.35%\n",
      "Run: 08, Epoch: 64, Loss: 0.5119, Train: 89.66%, Valid: 64.41% Test: 51.35%\n",
      "Run: 08, Epoch: 65, Loss: 0.4544, Train: 88.51%, Valid: 64.41% Test: 51.35%\n",
      "Run: 08, Epoch: 66, Loss: 0.4573, Train: 88.51%, Valid: 64.41% Test: 51.35%\n",
      "Run: 08, Epoch: 67, Loss: 0.4647, Train: 87.36%, Valid: 66.10% Test: 54.05%\n",
      "Run: 08, Epoch: 68, Loss: 0.4735, Train: 88.51%, Valid: 66.10% Test: 54.05%\n",
      "Run: 08, Epoch: 69, Loss: 0.4115, Train: 89.66%, Valid: 62.71% Test: 54.05%\n",
      "Run: 08, Epoch: 70, Loss: 0.4513, Train: 89.66%, Valid: 62.71% Test: 54.05%\n",
      "Run: 08, Epoch: 71, Loss: 0.4105, Train: 88.51%, Valid: 62.71% Test: 56.76%\n",
      "Run: 08, Epoch: 72, Loss: 0.3830, Train: 89.66%, Valid: 61.02% Test: 56.76%\n",
      "Run: 08, Epoch: 73, Loss: 0.4117, Train: 90.80%, Valid: 59.32% Test: 54.05%\n",
      "Run: 08, Epoch: 74, Loss: 0.4573, Train: 91.95%, Valid: 59.32% Test: 54.05%\n",
      "Run: 08, Epoch: 75, Loss: 0.5178, Train: 93.10%, Valid: 59.32% Test: 54.05%\n",
      "Run: 08, Epoch: 76, Loss: 0.4756, Train: 95.40%, Valid: 57.63% Test: 54.05%\n",
      "Run: 08, Epoch: 77, Loss: 0.4193, Train: 95.40%, Valid: 55.93% Test: 51.35%\n",
      "Run: 08, Epoch: 78, Loss: 0.4115, Train: 94.25%, Valid: 55.93% Test: 51.35%\n",
      "Run: 08, Epoch: 79, Loss: 0.3918, Train: 94.25%, Valid: 55.93% Test: 51.35%\n",
      "Run: 08, Epoch: 80, Loss: 0.4254, Train: 94.25%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 81, Loss: 0.4510, Train: 94.25%, Valid: 55.93% Test: 51.35%\n",
      "Run: 08, Epoch: 82, Loss: 0.3926, Train: 94.25%, Valid: 57.63% Test: 51.35%\n",
      "Run: 08, Epoch: 83, Loss: 0.4114, Train: 94.25%, Valid: 57.63% Test: 51.35%\n",
      "Run: 08, Epoch: 84, Loss: 0.3266, Train: 93.10%, Valid: 57.63% Test: 51.35%\n",
      "Run: 08, Epoch: 85, Loss: 0.4062, Train: 93.10%, Valid: 59.32% Test: 48.65%\n",
      "Run: 08, Epoch: 86, Loss: 0.3690, Train: 90.80%, Valid: 57.63% Test: 48.65%\n",
      "Run: 08, Epoch: 87, Loss: 0.3348, Train: 90.80%, Valid: 57.63% Test: 48.65%\n",
      "Run: 08, Epoch: 88, Loss: 0.3717, Train: 89.66%, Valid: 59.32% Test: 48.65%\n",
      "Run: 08, Epoch: 89, Loss: 0.3697, Train: 90.80%, Valid: 57.63% Test: 48.65%\n",
      "Run: 08, Epoch: 90, Loss: 0.3820, Train: 91.95%, Valid: 57.63% Test: 48.65%\n",
      "Run: 08, Epoch: 91, Loss: 0.4134, Train: 93.10%, Valid: 55.93% Test: 48.65%\n",
      "Run: 08, Epoch: 92, Loss: 0.4169, Train: 95.40%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 93, Loss: 0.3068, Train: 95.40%, Valid: 52.54% Test: 48.65%\n",
      "Run: 08, Epoch: 94, Loss: 0.3528, Train: 96.55%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 95, Loss: 0.3286, Train: 95.40%, Valid: 57.63% Test: 54.05%\n",
      "Run: 08, Epoch: 96, Loss: 0.3250, Train: 96.55%, Valid: 57.63% Test: 54.05%\n",
      "Run: 08, Epoch: 97, Loss: 0.3472, Train: 96.55%, Valid: 57.63% Test: 54.05%\n",
      "Run: 08, Epoch: 98, Loss: 0.3636, Train: 96.55%, Valid: 57.63% Test: 54.05%\n",
      "Run: 08, Epoch: 99, Loss: 0.3327, Train: 96.55%, Valid: 57.63% Test: 54.05%\n",
      "Run: 08, Epoch: 100, Loss: 0.2855, Train: 96.55%, Valid: 57.63% Test: 54.05%\n",
      "Run: 08, Epoch: 101, Loss: 0.3171, Train: 97.70%, Valid: 55.93% Test: 54.05%\n",
      "Run: 08, Epoch: 102, Loss: 0.3322, Train: 97.70%, Valid: 55.93% Test: 54.05%\n",
      "Run: 08, Epoch: 103, Loss: 0.2768, Train: 98.85%, Valid: 55.93% Test: 54.05%\n",
      "Run: 08, Epoch: 104, Loss: 0.4088, Train: 98.85%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 105, Loss: 0.3402, Train: 98.85%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 106, Loss: 0.3096, Train: 98.85%, Valid: 54.24% Test: 48.65%\n",
      "Run: 08, Epoch: 107, Loss: 0.3355, Train: 98.85%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 108, Loss: 0.2741, Train: 98.85%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 109, Loss: 0.2851, Train: 98.85%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 110, Loss: 0.2884, Train: 98.85%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 111, Loss: 0.3020, Train: 98.85%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 112, Loss: 0.3989, Train: 98.85%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 113, Loss: 0.2808, Train: 97.70%, Valid: 50.85% Test: 51.35%\n",
      "Run: 08, Epoch: 114, Loss: 0.2848, Train: 97.70%, Valid: 50.85% Test: 51.35%\n",
      "Run: 08, Epoch: 115, Loss: 0.3253, Train: 97.70%, Valid: 50.85% Test: 51.35%\n",
      "Run: 08, Epoch: 116, Loss: 0.3111, Train: 97.70%, Valid: 50.85% Test: 51.35%\n",
      "Run: 08, Epoch: 117, Loss: 0.2883, Train: 97.70%, Valid: 50.85% Test: 51.35%\n",
      "Run: 08, Epoch: 118, Loss: 0.2435, Train: 97.70%, Valid: 50.85% Test: 54.05%\n",
      "Run: 08, Epoch: 119, Loss: 0.3297, Train: 98.85%, Valid: 52.54% Test: 54.05%\n",
      "Run: 08, Epoch: 120, Loss: 0.2633, Train: 98.85%, Valid: 52.54% Test: 48.65%\n",
      "Run: 08, Epoch: 121, Loss: 0.3061, Train: 97.70%, Valid: 52.54% Test: 45.95%\n",
      "Run: 08, Epoch: 122, Loss: 0.2791, Train: 97.70%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 123, Loss: 0.3618, Train: 97.70%, Valid: 49.15% Test: 43.24%\n",
      "Run: 08, Epoch: 124, Loss: 0.2879, Train: 97.70%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 125, Loss: 0.3180, Train: 98.85%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 126, Loss: 0.2513, Train: 98.85%, Valid: 50.85% Test: 43.24%\n",
      "Run: 08, Epoch: 127, Loss: 0.2602, Train: 98.85%, Valid: 50.85% Test: 43.24%\n",
      "Run: 08, Epoch: 128, Loss: 0.2943, Train: 98.85%, Valid: 54.24% Test: 43.24%\n",
      "Run: 08, Epoch: 129, Loss: 0.2211, Train: 98.85%, Valid: 54.24% Test: 43.24%\n",
      "Run: 08, Epoch: 130, Loss: 0.2388, Train: 98.85%, Valid: 54.24% Test: 43.24%\n",
      "Run: 08, Epoch: 131, Loss: 0.2254, Train: 98.85%, Valid: 54.24% Test: 45.95%\n",
      "Run: 08, Epoch: 132, Loss: 0.2582, Train: 98.85%, Valid: 55.93% Test: 45.95%\n",
      "Run: 08, Epoch: 133, Loss: 0.2138, Train: 98.85%, Valid: 55.93% Test: 45.95%\n",
      "Run: 08, Epoch: 134, Loss: 0.2744, Train: 98.85%, Valid: 55.93% Test: 48.65%\n",
      "Run: 08, Epoch: 135, Loss: 0.2204, Train: 100.00%, Valid: 54.24% Test: 48.65%\n",
      "Run: 08, Epoch: 136, Loss: 0.2515, Train: 100.00%, Valid: 54.24% Test: 48.65%\n",
      "Run: 08, Epoch: 137, Loss: 0.2153, Train: 100.00%, Valid: 50.85% Test: 48.65%\n",
      "Run: 08, Epoch: 138, Loss: 0.2394, Train: 100.00%, Valid: 49.15% Test: 48.65%\n",
      "Run: 08, Epoch: 139, Loss: 0.2363, Train: 100.00%, Valid: 49.15% Test: 48.65%\n",
      "Run: 08, Epoch: 140, Loss: 0.2571, Train: 100.00%, Valid: 49.15% Test: 48.65%\n",
      "Run: 08, Epoch: 141, Loss: 0.2934, Train: 100.00%, Valid: 54.24% Test: 48.65%\n",
      "Run: 08, Epoch: 142, Loss: 0.2262, Train: 100.00%, Valid: 55.93% Test: 48.65%\n",
      "Run: 08, Epoch: 143, Loss: 0.2944, Train: 100.00%, Valid: 55.93% Test: 48.65%\n",
      "Run: 08, Epoch: 144, Loss: 0.2318, Train: 100.00%, Valid: 55.93% Test: 48.65%\n",
      "Run: 08, Epoch: 145, Loss: 0.1931, Train: 100.00%, Valid: 55.93% Test: 48.65%\n",
      "Run: 08, Epoch: 146, Loss: 0.2344, Train: 100.00%, Valid: 55.93% Test: 51.35%\n",
      "Run: 08, Epoch: 147, Loss: 0.2465, Train: 100.00%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 148, Loss: 0.2001, Train: 100.00%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 149, Loss: 0.1857, Train: 100.00%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 150, Loss: 0.1763, Train: 100.00%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 151, Loss: 0.2113, Train: 100.00%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 152, Loss: 0.2473, Train: 100.00%, Valid: 50.85% Test: 51.35%\n",
      "Run: 08, Epoch: 153, Loss: 0.2515, Train: 100.00%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 154, Loss: 0.1816, Train: 100.00%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 155, Loss: 0.2303, Train: 100.00%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 156, Loss: 0.2060, Train: 98.85%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 157, Loss: 0.2382, Train: 98.85%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 158, Loss: 0.3134, Train: 98.85%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 159, Loss: 0.2186, Train: 98.85%, Valid: 52.54% Test: 48.65%\n",
      "Run: 08, Epoch: 160, Loss: 0.2568, Train: 98.85%, Valid: 50.85% Test: 48.65%\n",
      "Run: 08, Epoch: 161, Loss: 0.2366, Train: 98.85%, Valid: 49.15% Test: 48.65%\n",
      "Run: 08, Epoch: 162, Loss: 0.1715, Train: 100.00%, Valid: 49.15% Test: 48.65%\n",
      "Run: 08, Epoch: 163, Loss: 0.1993, Train: 100.00%, Valid: 50.85% Test: 45.95%\n",
      "Run: 08, Epoch: 164, Loss: 0.2191, Train: 100.00%, Valid: 49.15% Test: 45.95%\n",
      "Run: 08, Epoch: 165, Loss: 0.2290, Train: 100.00%, Valid: 50.85% Test: 45.95%\n",
      "Run: 08, Epoch: 166, Loss: 0.1813, Train: 100.00%, Valid: 52.54% Test: 48.65%\n",
      "Run: 08, Epoch: 167, Loss: 0.1889, Train: 100.00%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 168, Loss: 0.2061, Train: 100.00%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 169, Loss: 0.1896, Train: 100.00%, Valid: 55.93% Test: 51.35%\n",
      "Run: 08, Epoch: 170, Loss: 0.1471, Train: 100.00%, Valid: 54.24% Test: 48.65%\n",
      "Run: 08, Epoch: 171, Loss: 0.2130, Train: 100.00%, Valid: 54.24% Test: 51.35%\n",
      "Run: 08, Epoch: 172, Loss: 0.2005, Train: 100.00%, Valid: 52.54% Test: 51.35%\n",
      "Run: 08, Epoch: 173, Loss: 0.2119, Train: 100.00%, Valid: 52.54% Test: 48.65%\n",
      "Run: 08, Epoch: 174, Loss: 0.2009, Train: 100.00%, Valid: 52.54% Test: 45.95%\n",
      "Run: 08, Epoch: 175, Loss: 0.2219, Train: 100.00%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 176, Loss: 0.1866, Train: 100.00%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 177, Loss: 0.2002, Train: 100.00%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 178, Loss: 0.1283, Train: 100.00%, Valid: 54.24% Test: 43.24%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 08, Epoch: 179, Loss: 0.1736, Train: 98.85%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 180, Loss: 0.1757, Train: 98.85%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 181, Loss: 0.1967, Train: 100.00%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 182, Loss: 0.1443, Train: 100.00%, Valid: 50.85% Test: 43.24%\n",
      "Run: 08, Epoch: 183, Loss: 0.1823, Train: 100.00%, Valid: 49.15% Test: 43.24%\n",
      "Run: 08, Epoch: 184, Loss: 0.1663, Train: 100.00%, Valid: 49.15% Test: 43.24%\n",
      "Run: 08, Epoch: 185, Loss: 0.1661, Train: 100.00%, Valid: 49.15% Test: 45.95%\n",
      "Run: 08, Epoch: 186, Loss: 0.2283, Train: 100.00%, Valid: 50.85% Test: 45.95%\n",
      "Run: 08, Epoch: 187, Loss: 0.2394, Train: 100.00%, Valid: 52.54% Test: 45.95%\n",
      "Run: 08, Epoch: 188, Loss: 0.1657, Train: 100.00%, Valid: 52.54% Test: 45.95%\n",
      "Run: 08, Epoch: 189, Loss: 0.1407, Train: 100.00%, Valid: 50.85% Test: 45.95%\n",
      "Run: 08, Epoch: 190, Loss: 0.1710, Train: 100.00%, Valid: 50.85% Test: 48.65%\n",
      "Run: 08, Epoch: 191, Loss: 0.1554, Train: 100.00%, Valid: 54.24% Test: 48.65%\n",
      "Run: 08, Epoch: 192, Loss: 0.1341, Train: 100.00%, Valid: 54.24% Test: 48.65%\n",
      "Run: 08, Epoch: 193, Loss: 0.1876, Train: 100.00%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 194, Loss: 0.2453, Train: 100.00%, Valid: 52.54% Test: 43.24%\n",
      "Run: 08, Epoch: 195, Loss: 0.1462, Train: 100.00%, Valid: 50.85% Test: 40.54%\n",
      "Run: 08, Epoch: 196, Loss: 0.1445, Train: 100.00%, Valid: 50.85% Test: 40.54%\n",
      "Run: 08, Epoch: 197, Loss: 0.1299, Train: 100.00%, Valid: 50.85% Test: 40.54%\n",
      "Run: 08, Epoch: 198, Loss: 0.1787, Train: 100.00%, Valid: 49.15% Test: 40.54%\n",
      "Run: 08, Epoch: 199, Loss: 0.1419, Train: 100.00%, Valid: 47.46% Test: 40.54%\n",
      "Run: 08, Epoch: 200, Loss: 0.1242, Train: 100.00%, Valid: 45.76% Test: 37.84%\n",
      "Run 08:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 67.80\n",
      "  Final Train: 67.82\n",
      "   Final Test: 62.16\n",
      "Run: 09, Epoch: 01, Loss: 1.8915, Train: 17.24%, Valid: 16.95% Test: 21.62%\n",
      "Run: 09, Epoch: 02, Loss: 1.8210, Train: 17.24%, Valid: 16.95% Test: 21.62%\n",
      "Run: 09, Epoch: 03, Loss: 1.7226, Train: 17.24%, Valid: 16.95% Test: 21.62%\n",
      "Run: 09, Epoch: 04, Loss: 1.6440, Train: 17.24%, Valid: 16.95% Test: 21.62%\n",
      "Run: 09, Epoch: 05, Loss: 1.5788, Train: 17.24%, Valid: 16.95% Test: 21.62%\n",
      "Run: 09, Epoch: 06, Loss: 1.5000, Train: 17.24%, Valid: 16.95% Test: 21.62%\n",
      "Run: 09, Epoch: 07, Loss: 1.4293, Train: 17.24%, Valid: 16.95% Test: 21.62%\n",
      "Run: 09, Epoch: 08, Loss: 1.3425, Train: 17.24%, Valid: 16.95% Test: 21.62%\n",
      "Run: 09, Epoch: 09, Loss: 1.3077, Train: 54.02%, Valid: 25.42% Test: 37.84%\n",
      "Run: 09, Epoch: 10, Loss: 1.2264, Train: 70.11%, Valid: 44.07% Test: 59.46%\n",
      "Run: 09, Epoch: 11, Loss: 1.2095, Train: 73.56%, Valid: 45.76% Test: 64.86%\n",
      "Run: 09, Epoch: 12, Loss: 1.1585, Train: 73.56%, Valid: 47.46% Test: 64.86%\n",
      "Run: 09, Epoch: 13, Loss: 1.1117, Train: 73.56%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 14, Loss: 1.0750, Train: 72.41%, Valid: 47.46% Test: 59.46%\n",
      "Run: 09, Epoch: 15, Loss: 1.0028, Train: 73.56%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 16, Loss: 0.9634, Train: 73.56%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 17, Loss: 0.9482, Train: 71.26%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 18, Loss: 0.9038, Train: 71.26%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 19, Loss: 0.9101, Train: 70.11%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 20, Loss: 0.8706, Train: 70.11%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 21, Loss: 0.8322, Train: 70.11%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 22, Loss: 0.8244, Train: 68.97%, Valid: 47.46% Test: 62.16%\n",
      "Run: 09, Epoch: 23, Loss: 0.7865, Train: 70.11%, Valid: 47.46% Test: 62.16%\n",
      "Run: 09, Epoch: 24, Loss: 0.7848, Train: 71.26%, Valid: 47.46% Test: 62.16%\n",
      "Run: 09, Epoch: 25, Loss: 0.7727, Train: 71.26%, Valid: 47.46% Test: 62.16%\n",
      "Run: 09, Epoch: 26, Loss: 0.7520, Train: 72.41%, Valid: 47.46% Test: 62.16%\n",
      "Run: 09, Epoch: 27, Loss: 0.7252, Train: 73.56%, Valid: 47.46% Test: 62.16%\n",
      "Run: 09, Epoch: 28, Loss: 0.7357, Train: 73.56%, Valid: 47.46% Test: 62.16%\n",
      "Run: 09, Epoch: 29, Loss: 0.6955, Train: 75.86%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 30, Loss: 0.6816, Train: 77.01%, Valid: 45.76% Test: 62.16%\n",
      "Run: 09, Epoch: 31, Loss: 0.6974, Train: 78.16%, Valid: 44.07% Test: 62.16%\n",
      "Run: 09, Epoch: 32, Loss: 0.6655, Train: 79.31%, Valid: 42.37% Test: 62.16%\n",
      "Run: 09, Epoch: 33, Loss: 0.6274, Train: 83.91%, Valid: 42.37% Test: 59.46%\n",
      "Run: 09, Epoch: 34, Loss: 0.6305, Train: 83.91%, Valid: 42.37% Test: 62.16%\n",
      "Run: 09, Epoch: 35, Loss: 0.6298, Train: 85.06%, Valid: 40.68% Test: 56.76%\n",
      "Run: 09, Epoch: 36, Loss: 0.6304, Train: 85.06%, Valid: 40.68% Test: 56.76%\n",
      "Run: 09, Epoch: 37, Loss: 0.6222, Train: 85.06%, Valid: 37.29% Test: 56.76%\n",
      "Run: 09, Epoch: 38, Loss: 0.6142, Train: 86.21%, Valid: 35.59% Test: 59.46%\n",
      "Run: 09, Epoch: 39, Loss: 0.5468, Train: 86.21%, Valid: 37.29% Test: 59.46%\n",
      "Run: 09, Epoch: 40, Loss: 0.6035, Train: 86.21%, Valid: 38.98% Test: 59.46%\n",
      "Run: 09, Epoch: 41, Loss: 0.5741, Train: 86.21%, Valid: 40.68% Test: 56.76%\n",
      "Run: 09, Epoch: 42, Loss: 0.5826, Train: 88.51%, Valid: 40.68% Test: 54.05%\n",
      "Run: 09, Epoch: 43, Loss: 0.5809, Train: 88.51%, Valid: 44.07% Test: 54.05%\n",
      "Run: 09, Epoch: 44, Loss: 0.5849, Train: 88.51%, Valid: 40.68% Test: 51.35%\n",
      "Run: 09, Epoch: 45, Loss: 0.5373, Train: 90.80%, Valid: 38.98% Test: 48.65%\n",
      "Run: 09, Epoch: 46, Loss: 0.5958, Train: 91.95%, Valid: 38.98% Test: 48.65%\n",
      "Run: 09, Epoch: 47, Loss: 0.5041, Train: 91.95%, Valid: 38.98% Test: 48.65%\n",
      "Run: 09, Epoch: 48, Loss: 0.5156, Train: 93.10%, Valid: 38.98% Test: 48.65%\n",
      "Run: 09, Epoch: 49, Loss: 0.5069, Train: 93.10%, Valid: 38.98% Test: 51.35%\n",
      "Run: 09, Epoch: 50, Loss: 0.5181, Train: 89.66%, Valid: 38.98% Test: 51.35%\n",
      "Run: 09, Epoch: 51, Loss: 0.5111, Train: 90.80%, Valid: 37.29% Test: 51.35%\n",
      "Run: 09, Epoch: 52, Loss: 0.5030, Train: 90.80%, Valid: 35.59% Test: 51.35%\n",
      "Run: 09, Epoch: 53, Loss: 0.5162, Train: 90.80%, Valid: 35.59% Test: 54.05%\n",
      "Run: 09, Epoch: 54, Loss: 0.4871, Train: 90.80%, Valid: 37.29% Test: 54.05%\n",
      "Run: 09, Epoch: 55, Loss: 0.4824, Train: 91.95%, Valid: 38.98% Test: 51.35%\n",
      "Run: 09, Epoch: 56, Loss: 0.4984, Train: 91.95%, Valid: 40.68% Test: 51.35%\n",
      "Run: 09, Epoch: 57, Loss: 0.4722, Train: 94.25%, Valid: 44.07% Test: 54.05%\n",
      "Run: 09, Epoch: 58, Loss: 0.4769, Train: 94.25%, Valid: 44.07% Test: 51.35%\n",
      "Run: 09, Epoch: 59, Loss: 0.4644, Train: 94.25%, Valid: 40.68% Test: 48.65%\n",
      "Run: 09, Epoch: 60, Loss: 0.4850, Train: 94.25%, Valid: 37.29% Test: 51.35%\n",
      "Run: 09, Epoch: 61, Loss: 0.4697, Train: 95.40%, Valid: 35.59% Test: 51.35%\n",
      "Run: 09, Epoch: 62, Loss: 0.4192, Train: 95.40%, Valid: 35.59% Test: 48.65%\n",
      "Run: 09, Epoch: 63, Loss: 0.4131, Train: 95.40%, Valid: 35.59% Test: 45.95%\n",
      "Run: 09, Epoch: 64, Loss: 0.4639, Train: 94.25%, Valid: 35.59% Test: 45.95%\n",
      "Run: 09, Epoch: 65, Loss: 0.4318, Train: 94.25%, Valid: 37.29% Test: 45.95%\n",
      "Run: 09, Epoch: 66, Loss: 0.4048, Train: 94.25%, Valid: 37.29% Test: 43.24%\n",
      "Run: 09, Epoch: 67, Loss: 0.4075, Train: 93.10%, Valid: 37.29% Test: 45.95%\n",
      "Run: 09, Epoch: 68, Loss: 0.4143, Train: 93.10%, Valid: 38.98% Test: 45.95%\n",
      "Run: 09, Epoch: 69, Loss: 0.4239, Train: 93.10%, Valid: 38.98% Test: 45.95%\n",
      "Run: 09, Epoch: 70, Loss: 0.4232, Train: 93.10%, Valid: 40.68% Test: 48.65%\n",
      "Run: 09, Epoch: 71, Loss: 0.3851, Train: 93.10%, Valid: 40.68% Test: 48.65%\n",
      "Run: 09, Epoch: 72, Loss: 0.4241, Train: 93.10%, Valid: 40.68% Test: 48.65%\n",
      "Run: 09, Epoch: 73, Loss: 0.4313, Train: 93.10%, Valid: 40.68% Test: 48.65%\n",
      "Run: 09, Epoch: 74, Loss: 0.3937, Train: 93.10%, Valid: 38.98% Test: 48.65%\n",
      "Run: 09, Epoch: 75, Loss: 0.4057, Train: 91.95%, Valid: 38.98% Test: 45.95%\n",
      "Run: 09, Epoch: 76, Loss: 0.3864, Train: 91.95%, Valid: 37.29% Test: 43.24%\n",
      "Run: 09, Epoch: 77, Loss: 0.3762, Train: 93.10%, Valid: 35.59% Test: 43.24%\n",
      "Run: 09, Epoch: 78, Loss: 0.3896, Train: 93.10%, Valid: 37.29% Test: 43.24%\n",
      "Run: 09, Epoch: 79, Loss: 0.3945, Train: 95.40%, Valid: 38.98% Test: 43.24%\n",
      "Run: 09, Epoch: 80, Loss: 0.4266, Train: 94.25%, Valid: 38.98% Test: 43.24%\n",
      "Run: 09, Epoch: 81, Loss: 0.4028, Train: 94.25%, Valid: 38.98% Test: 43.24%\n",
      "Run: 09, Epoch: 82, Loss: 0.3600, Train: 94.25%, Valid: 40.68% Test: 43.24%\n",
      "Run: 09, Epoch: 83, Loss: 0.3789, Train: 94.25%, Valid: 40.68% Test: 45.95%\n",
      "Run: 09, Epoch: 84, Loss: 0.3614, Train: 94.25%, Valid: 42.37% Test: 45.95%\n",
      "Run: 09, Epoch: 85, Loss: 0.3963, Train: 94.25%, Valid: 44.07% Test: 45.95%\n",
      "Run: 09, Epoch: 86, Loss: 0.4195, Train: 94.25%, Valid: 42.37% Test: 45.95%\n",
      "Run: 09, Epoch: 87, Loss: 0.3841, Train: 94.25%, Valid: 42.37% Test: 45.95%\n",
      "Run: 09, Epoch: 88, Loss: 0.3548, Train: 93.10%, Valid: 35.59% Test: 43.24%\n",
      "Run: 09, Epoch: 89, Loss: 0.3449, Train: 93.10%, Valid: 35.59% Test: 43.24%\n",
      "Run: 09, Epoch: 90, Loss: 0.3375, Train: 91.95%, Valid: 35.59% Test: 40.54%\n",
      "Run: 09, Epoch: 91, Loss: 0.3651, Train: 91.95%, Valid: 33.90% Test: 40.54%\n",
      "Run: 09, Epoch: 92, Loss: 0.3463, Train: 91.95%, Valid: 33.90% Test: 40.54%\n",
      "Run: 09, Epoch: 93, Loss: 0.3325, Train: 93.10%, Valid: 33.90% Test: 37.84%\n",
      "Run: 09, Epoch: 94, Loss: 0.3756, Train: 94.25%, Valid: 33.90% Test: 37.84%\n",
      "Run: 09, Epoch: 95, Loss: 0.3561, Train: 94.25%, Valid: 33.90% Test: 37.84%\n",
      "Run: 09, Epoch: 96, Loss: 0.3534, Train: 94.25%, Valid: 32.20% Test: 37.84%\n",
      "Run: 09, Epoch: 97, Loss: 0.3236, Train: 94.25%, Valid: 37.29% Test: 37.84%\n",
      "Run: 09, Epoch: 98, Loss: 0.3252, Train: 95.40%, Valid: 37.29% Test: 37.84%\n",
      "Run: 09, Epoch: 99, Loss: 0.3237, Train: 95.40%, Valid: 37.29% Test: 40.54%\n",
      "Run: 09, Epoch: 100, Loss: 0.3393, Train: 95.40%, Valid: 37.29% Test: 43.24%\n",
      "Run: 09, Epoch: 101, Loss: 0.3041, Train: 94.25%, Valid: 38.98% Test: 43.24%\n",
      "Run: 09, Epoch: 102, Loss: 0.3063, Train: 94.25%, Valid: 40.68% Test: 45.95%\n",
      "Run: 09, Epoch: 103, Loss: 0.3329, Train: 94.25%, Valid: 38.98% Test: 45.95%\n",
      "Run: 09, Epoch: 104, Loss: 0.3692, Train: 94.25%, Valid: 40.68% Test: 45.95%\n",
      "Run: 09, Epoch: 105, Loss: 0.3072, Train: 94.25%, Valid: 40.68% Test: 45.95%\n",
      "Run: 09, Epoch: 106, Loss: 0.3450, Train: 94.25%, Valid: 38.98% Test: 45.95%\n",
      "Run: 09, Epoch: 107, Loss: 0.3860, Train: 93.10%, Valid: 37.29% Test: 45.95%\n",
      "Run: 09, Epoch: 108, Loss: 0.3084, Train: 94.25%, Valid: 35.59% Test: 45.95%\n",
      "Run: 09, Epoch: 109, Loss: 0.3340, Train: 94.25%, Valid: 35.59% Test: 45.95%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 09, Epoch: 110, Loss: 0.3165, Train: 94.25%, Valid: 35.59% Test: 45.95%\n",
      "Run: 09, Epoch: 111, Loss: 0.3069, Train: 94.25%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 112, Loss: 0.3154, Train: 94.25%, Valid: 32.20% Test: 45.95%\n",
      "Run: 09, Epoch: 113, Loss: 0.2924, Train: 94.25%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 114, Loss: 0.2909, Train: 94.25%, Valid: 35.59% Test: 45.95%\n",
      "Run: 09, Epoch: 115, Loss: 0.2756, Train: 94.25%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 116, Loss: 0.2876, Train: 94.25%, Valid: 30.51% Test: 40.54%\n",
      "Run: 09, Epoch: 117, Loss: 0.3082, Train: 94.25%, Valid: 30.51% Test: 40.54%\n",
      "Run: 09, Epoch: 118, Loss: 0.3558, Train: 95.40%, Valid: 30.51% Test: 40.54%\n",
      "Run: 09, Epoch: 119, Loss: 0.3146, Train: 95.40%, Valid: 35.59% Test: 43.24%\n",
      "Run: 09, Epoch: 120, Loss: 0.3038, Train: 95.40%, Valid: 35.59% Test: 43.24%\n",
      "Run: 09, Epoch: 121, Loss: 0.2845, Train: 95.40%, Valid: 35.59% Test: 43.24%\n",
      "Run: 09, Epoch: 122, Loss: 0.2993, Train: 95.40%, Valid: 35.59% Test: 43.24%\n",
      "Run: 09, Epoch: 123, Loss: 0.3384, Train: 95.40%, Valid: 35.59% Test: 40.54%\n",
      "Run: 09, Epoch: 124, Loss: 0.2662, Train: 95.40%, Valid: 37.29% Test: 37.84%\n",
      "Run: 09, Epoch: 125, Loss: 0.2691, Train: 95.40%, Valid: 35.59% Test: 37.84%\n",
      "Run: 09, Epoch: 126, Loss: 0.2515, Train: 96.55%, Valid: 33.90% Test: 37.84%\n",
      "Run: 09, Epoch: 127, Loss: 0.3176, Train: 96.55%, Valid: 33.90% Test: 37.84%\n",
      "Run: 09, Epoch: 128, Loss: 0.2666, Train: 96.55%, Valid: 33.90% Test: 37.84%\n",
      "Run: 09, Epoch: 129, Loss: 0.3029, Train: 96.55%, Valid: 32.20% Test: 40.54%\n",
      "Run: 09, Epoch: 130, Loss: 0.3131, Train: 96.55%, Valid: 33.90% Test: 40.54%\n",
      "Run: 09, Epoch: 131, Loss: 0.2658, Train: 95.40%, Valid: 32.20% Test: 40.54%\n",
      "Run: 09, Epoch: 132, Loss: 0.2465, Train: 95.40%, Valid: 32.20% Test: 40.54%\n",
      "Run: 09, Epoch: 133, Loss: 0.2825, Train: 95.40%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 134, Loss: 0.3231, Train: 95.40%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 135, Loss: 0.2590, Train: 95.40%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 136, Loss: 0.2923, Train: 96.55%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 137, Loss: 0.2466, Train: 96.55%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 138, Loss: 0.2884, Train: 96.55%, Valid: 32.20% Test: 45.95%\n",
      "Run: 09, Epoch: 139, Loss: 0.2395, Train: 96.55%, Valid: 30.51% Test: 43.24%\n",
      "Run: 09, Epoch: 140, Loss: 0.2690, Train: 96.55%, Valid: 30.51% Test: 43.24%\n",
      "Run: 09, Epoch: 141, Loss: 0.2655, Train: 96.55%, Valid: 28.81% Test: 43.24%\n",
      "Run: 09, Epoch: 142, Loss: 0.2767, Train: 95.40%, Valid: 28.81% Test: 43.24%\n",
      "Run: 09, Epoch: 143, Loss: 0.2777, Train: 95.40%, Valid: 27.12% Test: 43.24%\n",
      "Run: 09, Epoch: 144, Loss: 0.2278, Train: 95.40%, Valid: 28.81% Test: 43.24%\n",
      "Run: 09, Epoch: 145, Loss: 0.2607, Train: 95.40%, Valid: 28.81% Test: 43.24%\n",
      "Run: 09, Epoch: 146, Loss: 0.2958, Train: 95.40%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 147, Loss: 0.2466, Train: 94.25%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 148, Loss: 0.2476, Train: 94.25%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 149, Loss: 0.2899, Train: 95.40%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 150, Loss: 0.2349, Train: 95.40%, Valid: 30.51% Test: 40.54%\n",
      "Run: 09, Epoch: 151, Loss: 0.2437, Train: 96.55%, Valid: 30.51% Test: 40.54%\n",
      "Run: 09, Epoch: 152, Loss: 0.2478, Train: 96.55%, Valid: 30.51% Test: 37.84%\n",
      "Run: 09, Epoch: 153, Loss: 0.3026, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 154, Loss: 0.2430, Train: 96.55%, Valid: 30.51% Test: 37.84%\n",
      "Run: 09, Epoch: 155, Loss: 0.2342, Train: 96.55%, Valid: 30.51% Test: 37.84%\n",
      "Run: 09, Epoch: 156, Loss: 0.2834, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 157, Loss: 0.2359, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 158, Loss: 0.2530, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 159, Loss: 0.2722, Train: 96.55%, Valid: 30.51% Test: 37.84%\n",
      "Run: 09, Epoch: 160, Loss: 0.2704, Train: 96.55%, Valid: 30.51% Test: 37.84%\n",
      "Run: 09, Epoch: 161, Loss: 0.2529, Train: 96.55%, Valid: 30.51% Test: 37.84%\n",
      "Run: 09, Epoch: 162, Loss: 0.2650, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 163, Loss: 0.2163, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 164, Loss: 0.2149, Train: 96.55%, Valid: 27.12% Test: 37.84%\n",
      "Run: 09, Epoch: 165, Loss: 0.2515, Train: 96.55%, Valid: 27.12% Test: 37.84%\n",
      "Run: 09, Epoch: 166, Loss: 0.2252, Train: 96.55%, Valid: 27.12% Test: 37.84%\n",
      "Run: 09, Epoch: 167, Loss: 0.2323, Train: 96.55%, Valid: 27.12% Test: 37.84%\n",
      "Run: 09, Epoch: 168, Loss: 0.2277, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 169, Loss: 0.2727, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 170, Loss: 0.1825, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 171, Loss: 0.2550, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 172, Loss: 0.2001, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 173, Loss: 0.2204, Train: 96.55%, Valid: 28.81% Test: 37.84%\n",
      "Run: 09, Epoch: 174, Loss: 0.2750, Train: 96.55%, Valid: 28.81% Test: 43.24%\n",
      "Run: 09, Epoch: 175, Loss: 0.2495, Train: 96.55%, Valid: 28.81% Test: 43.24%\n",
      "Run: 09, Epoch: 176, Loss: 0.2461, Train: 96.55%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 177, Loss: 0.2357, Train: 96.55%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 178, Loss: 0.2816, Train: 96.55%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 179, Loss: 0.2441, Train: 96.55%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 180, Loss: 0.2714, Train: 96.55%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 181, Loss: 0.2068, Train: 96.55%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 182, Loss: 0.2727, Train: 96.55%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 183, Loss: 0.1912, Train: 96.55%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 184, Loss: 0.2633, Train: 96.55%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 185, Loss: 0.2097, Train: 96.55%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 186, Loss: 0.2022, Train: 96.55%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 187, Loss: 0.2264, Train: 96.55%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 188, Loss: 0.2711, Train: 96.55%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 189, Loss: 0.2307, Train: 95.40%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 190, Loss: 0.1832, Train: 94.25%, Valid: 32.20% Test: 43.24%\n",
      "Run: 09, Epoch: 191, Loss: 0.2068, Train: 95.40%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 192, Loss: 0.2254, Train: 95.40%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 193, Loss: 0.2614, Train: 95.40%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 194, Loss: 0.2239, Train: 95.40%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 195, Loss: 0.1911, Train: 95.40%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 196, Loss: 0.2625, Train: 95.40%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 197, Loss: 0.2500, Train: 95.40%, Valid: 33.90% Test: 45.95%\n",
      "Run: 09, Epoch: 198, Loss: 0.2078, Train: 96.55%, Valid: 33.90% Test: 43.24%\n",
      "Run: 09, Epoch: 199, Loss: 0.2121, Train: 96.55%, Valid: 30.51% Test: 40.54%\n",
      "Run: 09, Epoch: 200, Loss: 0.2323, Train: 96.55%, Valid: 30.51% Test: 40.54%\n",
      "Run 09:\n",
      "Highest Train: 96.55\n",
      "Highest Valid: 47.46\n",
      "  Final Train: 73.56\n",
      "   Final Test: 64.86\n",
      "Run: 10, Epoch: 01, Loss: 1.6238, Train: 8.05%, Valid: 15.25% Test: 5.41%\n",
      "Run: 10, Epoch: 02, Loss: 1.5163, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 03, Loss: 1.4477, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 04, Loss: 1.3939, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 05, Loss: 1.3598, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 06, Loss: 1.2587, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 07, Loss: 1.2056, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 08, Loss: 1.1525, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 09, Loss: 1.0919, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 10, Loss: 1.0442, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 11, Loss: 1.0064, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 12, Loss: 1.0110, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 13, Loss: 0.9790, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 14, Loss: 0.9453, Train: 58.62%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 15, Loss: 0.8379, Train: 59.77%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 16, Loss: 0.8859, Train: 59.77%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 17, Loss: 0.8225, Train: 60.92%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 18, Loss: 0.8196, Train: 63.22%, Valid: 44.07% Test: 59.46%\n",
      "Run: 10, Epoch: 19, Loss: 0.8299, Train: 65.52%, Valid: 44.07% Test: 59.46%\n",
      "Run: 10, Epoch: 20, Loss: 0.8291, Train: 70.11%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 21, Loss: 0.8820, Train: 73.56%, Valid: 47.46% Test: 62.16%\n",
      "Run: 10, Epoch: 22, Loss: 0.7673, Train: 74.71%, Valid: 47.46% Test: 62.16%\n",
      "Run: 10, Epoch: 23, Loss: 0.7462, Train: 80.46%, Valid: 47.46% Test: 59.46%\n",
      "Run: 10, Epoch: 24, Loss: 0.7390, Train: 80.46%, Valid: 47.46% Test: 59.46%\n",
      "Run: 10, Epoch: 25, Loss: 0.7525, Train: 81.61%, Valid: 47.46% Test: 64.86%\n",
      "Run: 10, Epoch: 26, Loss: 0.7245, Train: 81.61%, Valid: 47.46% Test: 62.16%\n",
      "Run: 10, Epoch: 27, Loss: 0.7227, Train: 82.76%, Valid: 49.15% Test: 64.86%\n",
      "Run: 10, Epoch: 28, Loss: 0.6868, Train: 82.76%, Valid: 49.15% Test: 64.86%\n",
      "Run: 10, Epoch: 29, Loss: 0.6571, Train: 81.61%, Valid: 49.15% Test: 64.86%\n",
      "Run: 10, Epoch: 30, Loss: 0.7168, Train: 81.61%, Valid: 47.46% Test: 62.16%\n",
      "Run: 10, Epoch: 31, Loss: 0.6440, Train: 81.61%, Valid: 49.15% Test: 62.16%\n",
      "Run: 10, Epoch: 32, Loss: 0.7046, Train: 82.76%, Valid: 49.15% Test: 62.16%\n",
      "Run: 10, Epoch: 33, Loss: 0.6727, Train: 82.76%, Valid: 50.85% Test: 62.16%\n",
      "Run: 10, Epoch: 34, Loss: 0.5839, Train: 81.61%, Valid: 52.54% Test: 62.16%\n",
      "Run: 10, Epoch: 35, Loss: 0.7031, Train: 85.06%, Valid: 54.24% Test: 64.86%\n",
      "Run: 10, Epoch: 36, Loss: 0.6744, Train: 87.36%, Valid: 54.24% Test: 67.57%\n",
      "Run: 10, Epoch: 37, Loss: 0.6783, Train: 87.36%, Valid: 55.93% Test: 67.57%\n",
      "Run: 10, Epoch: 38, Loss: 0.5961, Train: 86.21%, Valid: 55.93% Test: 67.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 10, Epoch: 39, Loss: 0.6108, Train: 86.21%, Valid: 54.24% Test: 67.57%\n",
      "Run: 10, Epoch: 40, Loss: 0.6421, Train: 87.36%, Valid: 47.46% Test: 72.97%\n",
      "Run: 10, Epoch: 41, Loss: 0.6189, Train: 87.36%, Valid: 49.15% Test: 70.27%\n",
      "Run: 10, Epoch: 42, Loss: 0.5965, Train: 86.21%, Valid: 54.24% Test: 67.57%\n",
      "Run: 10, Epoch: 43, Loss: 0.5990, Train: 85.06%, Valid: 54.24% Test: 64.86%\n",
      "Run: 10, Epoch: 44, Loss: 0.6076, Train: 83.91%, Valid: 55.93% Test: 64.86%\n",
      "Run: 10, Epoch: 45, Loss: 0.5620, Train: 83.91%, Valid: 52.54% Test: 64.86%\n",
      "Run: 10, Epoch: 46, Loss: 0.5948, Train: 83.91%, Valid: 52.54% Test: 64.86%\n",
      "Run: 10, Epoch: 47, Loss: 0.6412, Train: 87.36%, Valid: 50.85% Test: 64.86%\n",
      "Run: 10, Epoch: 48, Loss: 0.6005, Train: 88.51%, Valid: 49.15% Test: 67.57%\n",
      "Run: 10, Epoch: 49, Loss: 0.5635, Train: 88.51%, Valid: 49.15% Test: 67.57%\n",
      "Run: 10, Epoch: 50, Loss: 0.5462, Train: 87.36%, Valid: 45.76% Test: 67.57%\n",
      "Run: 10, Epoch: 51, Loss: 0.5621, Train: 83.91%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 52, Loss: 0.5540, Train: 82.76%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 53, Loss: 0.5984, Train: 83.91%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 54, Loss: 0.5608, Train: 83.91%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 55, Loss: 0.5480, Train: 85.06%, Valid: 44.07% Test: 62.16%\n",
      "Run: 10, Epoch: 56, Loss: 0.5310, Train: 85.06%, Valid: 42.37% Test: 62.16%\n",
      "Run: 10, Epoch: 57, Loss: 0.5247, Train: 85.06%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 58, Loss: 0.5098, Train: 83.91%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 59, Loss: 0.5176, Train: 83.91%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 60, Loss: 0.4955, Train: 85.06%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 61, Loss: 0.5525, Train: 85.06%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 62, Loss: 0.5212, Train: 86.21%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 63, Loss: 0.4600, Train: 86.21%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 64, Loss: 0.5136, Train: 87.36%, Valid: 49.15% Test: 64.86%\n",
      "Run: 10, Epoch: 65, Loss: 0.5065, Train: 90.80%, Valid: 50.85% Test: 64.86%\n",
      "Run: 10, Epoch: 66, Loss: 0.5085, Train: 90.80%, Valid: 49.15% Test: 64.86%\n",
      "Run: 10, Epoch: 67, Loss: 0.4640, Train: 90.80%, Valid: 49.15% Test: 64.86%\n",
      "Run: 10, Epoch: 68, Loss: 0.5163, Train: 90.80%, Valid: 50.85% Test: 64.86%\n",
      "Run: 10, Epoch: 69, Loss: 0.4281, Train: 91.95%, Valid: 49.15% Test: 64.86%\n",
      "Run: 10, Epoch: 70, Loss: 0.4513, Train: 91.95%, Valid: 49.15% Test: 64.86%\n",
      "Run: 10, Epoch: 71, Loss: 0.4401, Train: 90.80%, Valid: 50.85% Test: 64.86%\n",
      "Run: 10, Epoch: 72, Loss: 0.4939, Train: 90.80%, Valid: 50.85% Test: 67.57%\n",
      "Run: 10, Epoch: 73, Loss: 0.4697, Train: 90.80%, Valid: 50.85% Test: 67.57%\n",
      "Run: 10, Epoch: 74, Loss: 0.4475, Train: 91.95%, Valid: 47.46% Test: 67.57%\n",
      "Run: 10, Epoch: 75, Loss: 0.3703, Train: 91.95%, Valid: 47.46% Test: 67.57%\n",
      "Run: 10, Epoch: 76, Loss: 0.3982, Train: 91.95%, Valid: 45.76% Test: 67.57%\n",
      "Run: 10, Epoch: 77, Loss: 0.3912, Train: 91.95%, Valid: 45.76% Test: 67.57%\n",
      "Run: 10, Epoch: 78, Loss: 0.3946, Train: 91.95%, Valid: 45.76% Test: 67.57%\n",
      "Run: 10, Epoch: 79, Loss: 0.4860, Train: 91.95%, Valid: 47.46% Test: 67.57%\n",
      "Run: 10, Epoch: 80, Loss: 0.4537, Train: 93.10%, Valid: 47.46% Test: 64.86%\n",
      "Run: 10, Epoch: 81, Loss: 0.4503, Train: 91.95%, Valid: 49.15% Test: 67.57%\n",
      "Run: 10, Epoch: 82, Loss: 0.4482, Train: 91.95%, Valid: 50.85% Test: 67.57%\n",
      "Run: 10, Epoch: 83, Loss: 0.4732, Train: 91.95%, Valid: 50.85% Test: 67.57%\n",
      "Run: 10, Epoch: 84, Loss: 0.4042, Train: 90.80%, Valid: 47.46% Test: 67.57%\n",
      "Run: 10, Epoch: 85, Loss: 0.3906, Train: 90.80%, Valid: 45.76% Test: 67.57%\n",
      "Run: 10, Epoch: 86, Loss: 0.3594, Train: 88.51%, Valid: 42.37% Test: 67.57%\n",
      "Run: 10, Epoch: 87, Loss: 0.3901, Train: 88.51%, Valid: 42.37% Test: 67.57%\n",
      "Run: 10, Epoch: 88, Loss: 0.4032, Train: 88.51%, Valid: 40.68% Test: 67.57%\n",
      "Run: 10, Epoch: 89, Loss: 0.3965, Train: 88.51%, Valid: 42.37% Test: 67.57%\n",
      "Run: 10, Epoch: 90, Loss: 0.3737, Train: 89.66%, Valid: 44.07% Test: 67.57%\n",
      "Run: 10, Epoch: 91, Loss: 0.3623, Train: 89.66%, Valid: 44.07% Test: 67.57%\n",
      "Run: 10, Epoch: 92, Loss: 0.3534, Train: 89.66%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 93, Loss: 0.4121, Train: 90.80%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 94, Loss: 0.4205, Train: 90.80%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 95, Loss: 0.3784, Train: 90.80%, Valid: 44.07% Test: 67.57%\n",
      "Run: 10, Epoch: 96, Loss: 0.3368, Train: 90.80%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 97, Loss: 0.3944, Train: 90.80%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 98, Loss: 0.3720, Train: 91.95%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 99, Loss: 0.3373, Train: 91.95%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 100, Loss: 0.3513, Train: 91.95%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 101, Loss: 0.4403, Train: 93.10%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 102, Loss: 0.4599, Train: 91.95%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 103, Loss: 0.4115, Train: 91.95%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 104, Loss: 0.3481, Train: 91.95%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 105, Loss: 0.3820, Train: 93.10%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 106, Loss: 0.3761, Train: 93.10%, Valid: 44.07% Test: 62.16%\n",
      "Run: 10, Epoch: 107, Loss: 0.3150, Train: 94.25%, Valid: 42.37% Test: 62.16%\n",
      "Run: 10, Epoch: 108, Loss: 0.3049, Train: 94.25%, Valid: 40.68% Test: 62.16%\n",
      "Run: 10, Epoch: 109, Loss: 0.4038, Train: 94.25%, Valid: 40.68% Test: 62.16%\n",
      "Run: 10, Epoch: 110, Loss: 0.3033, Train: 94.25%, Valid: 40.68% Test: 62.16%\n",
      "Run: 10, Epoch: 111, Loss: 0.3390, Train: 94.25%, Valid: 42.37% Test: 62.16%\n",
      "Run: 10, Epoch: 112, Loss: 0.4135, Train: 95.40%, Valid: 42.37% Test: 64.86%\n",
      "Run: 10, Epoch: 113, Loss: 0.3564, Train: 95.40%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 114, Loss: 0.3298, Train: 95.40%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 115, Loss: 0.3003, Train: 95.40%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 116, Loss: 0.3291, Train: 94.25%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 117, Loss: 0.3267, Train: 94.25%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 118, Loss: 0.3379, Train: 94.25%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 119, Loss: 0.3077, Train: 94.25%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 120, Loss: 0.3120, Train: 94.25%, Valid: 44.07% Test: 64.86%\n",
      "Run: 10, Epoch: 121, Loss: 0.2758, Train: 94.25%, Valid: 45.76% Test: 64.86%\n",
      "Run: 10, Epoch: 122, Loss: 0.3314, Train: 94.25%, Valid: 47.46% Test: 64.86%\n",
      "Run: 10, Epoch: 123, Loss: 0.3367, Train: 94.25%, Valid: 45.76% Test: 62.16%\n",
      "Run: 10, Epoch: 124, Loss: 0.3179, Train: 94.25%, Valid: 44.07% Test: 62.16%\n",
      "Run: 10, Epoch: 125, Loss: 0.3092, Train: 94.25%, Valid: 42.37% Test: 62.16%\n",
      "Run: 10, Epoch: 126, Loss: 0.2915, Train: 93.10%, Valid: 42.37% Test: 62.16%\n",
      "Run: 10, Epoch: 127, Loss: 0.3415, Train: 94.25%, Valid: 42.37% Test: 62.16%\n",
      "Run: 10, Epoch: 128, Loss: 0.3108, Train: 96.55%, Valid: 44.07% Test: 59.46%\n",
      "Run: 10, Epoch: 129, Loss: 0.2725, Train: 96.55%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 130, Loss: 0.3269, Train: 96.55%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 131, Loss: 0.3064, Train: 96.55%, Valid: 42.37% Test: 59.46%\n",
      "Run: 10, Epoch: 132, Loss: 0.3092, Train: 96.55%, Valid: 42.37% Test: 59.46%\n",
      "Run: 10, Epoch: 133, Loss: 0.3221, Train: 97.70%, Valid: 42.37% Test: 59.46%\n",
      "Run: 10, Epoch: 134, Loss: 0.2994, Train: 97.70%, Valid: 42.37% Test: 62.16%\n",
      "Run: 10, Epoch: 135, Loss: 0.3256, Train: 97.70%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 136, Loss: 0.3248, Train: 97.70%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 137, Loss: 0.2428, Train: 97.70%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 138, Loss: 0.2922, Train: 97.70%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 139, Loss: 0.2780, Train: 97.70%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 140, Loss: 0.2646, Train: 97.70%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 141, Loss: 0.3178, Train: 97.70%, Valid: 44.07% Test: 59.46%\n",
      "Run: 10, Epoch: 142, Loss: 0.2199, Train: 97.70%, Valid: 45.76% Test: 59.46%\n",
      "Run: 10, Epoch: 143, Loss: 0.2670, Train: 97.70%, Valid: 47.46% Test: 62.16%\n",
      "Run: 10, Epoch: 144, Loss: 0.2902, Train: 97.70%, Valid: 49.15% Test: 62.16%\n",
      "Run: 10, Epoch: 145, Loss: 0.3050, Train: 97.70%, Valid: 49.15% Test: 59.46%\n",
      "Run: 10, Epoch: 146, Loss: 0.3064, Train: 96.55%, Valid: 49.15% Test: 59.46%\n",
      "Run: 10, Epoch: 147, Loss: 0.2662, Train: 96.55%, Valid: 49.15% Test: 59.46%\n",
      "Run: 10, Epoch: 148, Loss: 0.3845, Train: 96.55%, Valid: 49.15% Test: 59.46%\n",
      "Run: 10, Epoch: 149, Loss: 0.2856, Train: 96.55%, Valid: 47.46% Test: 59.46%\n",
      "Run: 10, Epoch: 150, Loss: 0.3060, Train: 96.55%, Valid: 40.68% Test: 59.46%\n",
      "Run: 10, Epoch: 151, Loss: 0.3497, Train: 97.70%, Valid: 38.98% Test: 59.46%\n",
      "Run: 10, Epoch: 152, Loss: 0.3297, Train: 97.70%, Valid: 38.98% Test: 59.46%\n",
      "Run: 10, Epoch: 153, Loss: 0.2701, Train: 97.70%, Valid: 38.98% Test: 59.46%\n",
      "Run: 10, Epoch: 154, Loss: 0.2387, Train: 97.70%, Valid: 38.98% Test: 59.46%\n",
      "Run: 10, Epoch: 155, Loss: 0.2678, Train: 97.70%, Valid: 42.37% Test: 59.46%\n",
      "Run: 10, Epoch: 156, Loss: 0.2429, Train: 97.70%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 157, Loss: 0.2759, Train: 97.70%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 158, Loss: 0.3565, Train: 97.70%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 159, Loss: 0.3400, Train: 97.70%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 160, Loss: 0.2617, Train: 96.55%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 161, Loss: 0.2570, Train: 95.40%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 162, Loss: 0.2816, Train: 95.40%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 163, Loss: 0.2326, Train: 95.40%, Valid: 45.76% Test: 56.76%\n",
      "Run: 10, Epoch: 164, Loss: 0.3009, Train: 95.40%, Valid: 49.15% Test: 56.76%\n",
      "Run: 10, Epoch: 165, Loss: 0.3980, Train: 97.70%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 166, Loss: 0.2621, Train: 97.70%, Valid: 42.37% Test: 54.05%\n",
      "Run: 10, Epoch: 167, Loss: 0.2534, Train: 97.70%, Valid: 40.68% Test: 56.76%\n",
      "Run: 10, Epoch: 168, Loss: 0.1976, Train: 97.70%, Valid: 40.68% Test: 56.76%\n",
      "Run: 10, Epoch: 169, Loss: 0.2181, Train: 97.70%, Valid: 40.68% Test: 56.76%\n",
      "Run: 10, Epoch: 170, Loss: 0.3222, Train: 97.70%, Valid: 40.68% Test: 59.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 10, Epoch: 171, Loss: 0.2503, Train: 97.70%, Valid: 40.68% Test: 59.46%\n",
      "Run: 10, Epoch: 172, Loss: 0.2586, Train: 97.70%, Valid: 40.68% Test: 59.46%\n",
      "Run: 10, Epoch: 173, Loss: 0.2790, Train: 97.70%, Valid: 40.68% Test: 59.46%\n",
      "Run: 10, Epoch: 174, Loss: 0.3217, Train: 97.70%, Valid: 38.98% Test: 59.46%\n",
      "Run: 10, Epoch: 175, Loss: 0.2738, Train: 97.70%, Valid: 38.98% Test: 56.76%\n",
      "Run: 10, Epoch: 176, Loss: 0.2279, Train: 97.70%, Valid: 38.98% Test: 59.46%\n",
      "Run: 10, Epoch: 177, Loss: 0.2175, Train: 97.70%, Valid: 38.98% Test: 59.46%\n",
      "Run: 10, Epoch: 178, Loss: 0.2471, Train: 97.70%, Valid: 38.98% Test: 59.46%\n",
      "Run: 10, Epoch: 179, Loss: 0.2567, Train: 97.70%, Valid: 38.98% Test: 56.76%\n",
      "Run: 10, Epoch: 180, Loss: 0.2565, Train: 97.70%, Valid: 38.98% Test: 56.76%\n",
      "Run: 10, Epoch: 181, Loss: 0.2574, Train: 97.70%, Valid: 38.98% Test: 56.76%\n",
      "Run: 10, Epoch: 182, Loss: 0.1898, Train: 97.70%, Valid: 38.98% Test: 56.76%\n",
      "Run: 10, Epoch: 183, Loss: 0.2960, Train: 97.70%, Valid: 40.68% Test: 56.76%\n",
      "Run: 10, Epoch: 184, Loss: 0.2526, Train: 97.70%, Valid: 40.68% Test: 56.76%\n",
      "Run: 10, Epoch: 185, Loss: 0.2037, Train: 97.70%, Valid: 40.68% Test: 56.76%\n",
      "Run: 10, Epoch: 186, Loss: 0.1992, Train: 97.70%, Valid: 40.68% Test: 59.46%\n",
      "Run: 10, Epoch: 187, Loss: 0.2361, Train: 97.70%, Valid: 40.68% Test: 59.46%\n",
      "Run: 10, Epoch: 188, Loss: 0.1751, Train: 97.70%, Valid: 40.68% Test: 59.46%\n",
      "Run: 10, Epoch: 189, Loss: 0.3197, Train: 97.70%, Valid: 42.37% Test: 59.46%\n",
      "Run: 10, Epoch: 190, Loss: 0.2147, Train: 98.85%, Valid: 42.37% Test: 59.46%\n",
      "Run: 10, Epoch: 191, Loss: 0.2640, Train: 98.85%, Valid: 44.07% Test: 59.46%\n",
      "Run: 10, Epoch: 192, Loss: 0.1979, Train: 98.85%, Valid: 44.07% Test: 56.76%\n",
      "Run: 10, Epoch: 193, Loss: 0.1971, Train: 98.85%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 194, Loss: 0.3068, Train: 98.85%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 195, Loss: 0.3049, Train: 98.85%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 196, Loss: 0.2651, Train: 98.85%, Valid: 42.37% Test: 56.76%\n",
      "Run: 10, Epoch: 197, Loss: 0.2373, Train: 97.70%, Valid: 40.68% Test: 56.76%\n",
      "Run: 10, Epoch: 198, Loss: 0.1780, Train: 97.70%, Valid: 40.68% Test: 56.76%\n",
      "Run: 10, Epoch: 199, Loss: 0.2005, Train: 96.55%, Valid: 38.98% Test: 56.76%\n",
      "Run: 10, Epoch: 200, Loss: 0.2298, Train: 96.55%, Valid: 38.98% Test: 62.16%\n",
      "Run 10:\n",
      "Highest Train: 98.85\n",
      "Highest Valid: 55.93\n",
      "  Final Train: 87.36\n",
      "   Final Test: 67.57\n",
      "All runs:\n",
      "Highest Train: 98.16 ± 1.45\n",
      "Highest Valid: 58.98 ± 6.58\n",
      "  Final Train: 70.92 ± 13.07\n",
      "   Final Test: 57.57 ± 7.10\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_channels': 16, 'dropout': 0.5, 'epochs': 200, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-4, 'lr': 0.01,'hidden_channels_mlp': 20,'dropout_mlp': 0.5,'num_layers_mlp': 3}\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = Planetoid(root='/tmp/cora', name='Cora',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    X = data.topo\n",
    "    y_true = data.y\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    model = GAT(data.num_features, args.hidden_channels,10, args.num_layers,args.dropout,args.heads)\n",
    "    mlp_model = MLP(X.size(-1), args.hidden_channels_mlp, 10,args.num_layers_mlp, args.dropout_mlp)\n",
    "    #print(mlp_model.parameters())\n",
    "    mlp_2 = MLP2(20, 100, dataset.num_classes,3, 0.0)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        idx_train=[data.train_mask[i][run] for i in range(len(data.y))]\n",
    "        train_idx = np.where(idx_train)[0]\n",
    "        idx_val=[data.val_mask[i][run] for i in range(len(data.y))]\n",
    "        valid_idx = np.where(idx_val)[0]\n",
    "        idx_test=[data.test_mask[i][run] for i in range(len(data.y))]\n",
    "        test_idx = np.where(idx_test)[0]\n",
    "        \n",
    "        model.reset_parameters()\n",
    "        mlp_model.reset_parameters_mlp()\n",
    "        mlp_2.reset_parameters_mlp2()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        optimizer_mlp=torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "        optimizer_mlp2=torch.optim.Adam(mlp_2.parameters(), lr=0.001)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2)\n",
    "            result = test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1beb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
