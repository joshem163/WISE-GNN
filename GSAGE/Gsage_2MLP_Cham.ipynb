{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af88d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx import ego_graph\n",
    "\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "#from logger import Logger\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import WikipediaNetwork\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7babc9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2277, 2325], y=[2277], train_mask=[2277, 10], val_mask=[2277, 10], test_mask=[2277, 10], adj_t=[2277, 2277, nnz=36101])\n"
     ]
    }
   ],
   "source": [
    "dataset = WikipediaNetwork(root='/tmp/chameleon', name='chameleon',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "print(data)\n",
    "#split_idx = dataset.get_idx_split()\n",
    "#train_idx = split_idx['train'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91fdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10920\n",
      "7290\n",
      "4560\n"
     ]
    }
   ],
   "source": [
    "train_index = np.where(data.train_mask)[0]\n",
    "print(len(train_index))\n",
    "valid_index = np.where(data.val_mask)[0]\n",
    "print(len(valid_index))\n",
    "test_index = np.where(data.test_mask)[0]\n",
    "print(len(test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0b82f",
   "metadata": {},
   "source": [
    "# GSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9ef33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47468ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    #print(len(out))\n",
    "    #print(data.y.squeeze(1)[train_idx])\n",
    "    loss = F.nll_loss(out, data.y.squeeze()[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def ACC(Prediction, Label):\n",
    "    correct = Prediction.view(-1).eq(Label).sum().item()\n",
    "    total=len(Label)\n",
    "    return correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, train_idx,valid_idx,test_idx):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    y_pred=y_pred.view(-1)\n",
    "    train_acc=ACC(data.y[train_idx],y_pred[train_idx])\n",
    "    valid_acc=ACC(data.y[valid_idx],y_pred[valid_idx])\n",
    "    test_acc =ACC(data.y[test_idx],y_pred[test_idx])\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=[data.train_mask[i][0] for i in range(183)]\n",
    "train_index = np.where(idx)[0]\n",
    "print(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23796d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x17a73be50>\n",
      "Run: 01, Epoch: 01, Loss: 1.8193, Train: 48.44%, Valid: 41.56% Test: 42.11%\n",
      "Run: 01, Epoch: 02, Loss: 1.2171, Train: 54.21%, Valid: 45.82% Test: 47.59%\n",
      "Run: 01, Epoch: 03, Loss: 1.0920, Train: 55.31%, Valid: 46.50% Test: 47.37%\n",
      "Run: 01, Epoch: 04, Loss: 1.0285, Train: 57.78%, Valid: 47.60% Test: 48.46%\n",
      "Run: 01, Epoch: 05, Loss: 0.9675, Train: 64.56%, Valid: 52.67% Test: 51.10%\n",
      "Run: 01, Epoch: 06, Loss: 0.9117, Train: 70.88%, Valid: 56.10% Test: 54.82%\n",
      "Run: 01, Epoch: 07, Loss: 0.8672, Train: 74.08%, Valid: 58.30% Test: 57.02%\n",
      "Run: 01, Epoch: 08, Loss: 0.8248, Train: 76.83%, Valid: 58.44% Test: 57.02%\n",
      "Run: 01, Epoch: 09, Loss: 0.7855, Train: 78.85%, Valid: 59.53% Test: 57.02%\n",
      "Run: 01, Epoch: 10, Loss: 0.7368, Train: 81.87%, Valid: 59.81% Test: 56.80%\n",
      "Run: 01, Epoch: 11, Loss: 0.6989, Train: 84.43%, Valid: 60.08% Test: 56.80%\n",
      "Run: 01, Epoch: 12, Loss: 0.6507, Train: 85.07%, Valid: 60.91% Test: 58.55%\n",
      "Run: 01, Epoch: 13, Loss: 0.6257, Train: 85.90%, Valid: 61.04% Test: 57.89%\n",
      "Run: 01, Epoch: 14, Loss: 0.5931, Train: 87.27%, Valid: 61.59% Test: 58.99%\n",
      "Run: 01, Epoch: 15, Loss: 0.5643, Train: 88.64%, Valid: 61.87% Test: 59.87%\n",
      "Run: 01, Epoch: 16, Loss: 0.5523, Train: 90.02%, Valid: 62.41% Test: 60.75%\n",
      "Run: 01, Epoch: 17, Loss: 0.5138, Train: 91.21%, Valid: 62.00% Test: 60.96%\n",
      "Run: 01, Epoch: 18, Loss: 0.5023, Train: 92.03%, Valid: 62.28% Test: 62.72%\n",
      "Run: 01, Epoch: 19, Loss: 0.4749, Train: 93.22%, Valid: 62.69% Test: 62.72%\n",
      "Run: 01, Epoch: 20, Loss: 0.4265, Train: 93.96%, Valid: 63.37% Test: 62.28%\n",
      "Run: 01, Epoch: 21, Loss: 0.4064, Train: 94.23%, Valid: 63.79% Test: 61.84%\n",
      "Run: 01, Epoch: 22, Loss: 0.3995, Train: 94.69%, Valid: 62.96% Test: 61.84%\n",
      "Run: 01, Epoch: 23, Loss: 0.3649, Train: 95.15%, Valid: 63.79% Test: 61.40%\n",
      "Run: 01, Epoch: 24, Loss: 0.3657, Train: 95.15%, Valid: 64.88% Test: 61.62%\n",
      "Run: 01, Epoch: 25, Loss: 0.3455, Train: 95.79%, Valid: 64.75% Test: 61.18%\n",
      "Run: 01, Epoch: 26, Loss: 0.3136, Train: 96.25%, Valid: 64.06% Test: 61.18%\n",
      "Run: 01, Epoch: 27, Loss: 0.2931, Train: 96.79%, Valid: 64.47% Test: 60.96%\n",
      "Run: 01, Epoch: 28, Loss: 0.2750, Train: 96.98%, Valid: 64.20% Test: 61.40%\n",
      "Run: 01, Epoch: 29, Loss: 0.2634, Train: 96.98%, Valid: 64.47% Test: 61.18%\n",
      "Run: 01, Epoch: 30, Loss: 0.2728, Train: 97.16%, Valid: 64.75% Test: 61.84%\n",
      "Run: 01, Epoch: 31, Loss: 0.2385, Train: 97.44%, Valid: 64.75% Test: 62.28%\n",
      "Run: 01, Epoch: 32, Loss: 0.2417, Train: 97.25%, Valid: 64.61% Test: 62.94%\n",
      "Run: 01, Epoch: 33, Loss: 0.2363, Train: 97.89%, Valid: 65.16% Test: 62.28%\n",
      "Run: 01, Epoch: 34, Loss: 0.2261, Train: 98.08%, Valid: 65.57% Test: 61.84%\n",
      "Run: 01, Epoch: 35, Loss: 0.2102, Train: 98.35%, Valid: 65.43% Test: 61.62%\n",
      "Run: 01, Epoch: 36, Loss: 0.1898, Train: 98.44%, Valid: 65.29% Test: 61.84%\n",
      "Run: 01, Epoch: 37, Loss: 0.1998, Train: 98.44%, Valid: 64.88% Test: 61.40%\n",
      "Run: 01, Epoch: 38, Loss: 0.1790, Train: 98.63%, Valid: 64.61% Test: 61.62%\n",
      "Run: 01, Epoch: 39, Loss: 0.1635, Train: 98.72%, Valid: 64.47% Test: 61.62%\n",
      "Run: 01, Epoch: 40, Loss: 0.1739, Train: 98.90%, Valid: 64.75% Test: 62.50%\n",
      "Run: 01, Epoch: 41, Loss: 0.1594, Train: 98.90%, Valid: 64.88% Test: 61.84%\n",
      "Run: 01, Epoch: 42, Loss: 0.1577, Train: 98.99%, Valid: 64.75% Test: 62.06%\n",
      "Run: 01, Epoch: 43, Loss: 0.1350, Train: 99.08%, Valid: 64.61% Test: 62.28%\n",
      "Run: 01, Epoch: 44, Loss: 0.1346, Train: 98.99%, Valid: 63.79% Test: 61.40%\n",
      "Run: 01, Epoch: 45, Loss: 0.1378, Train: 99.08%, Valid: 63.79% Test: 61.40%\n",
      "Run: 01, Epoch: 46, Loss: 0.1161, Train: 99.27%, Valid: 64.06% Test: 62.50%\n",
      "Run: 01, Epoch: 47, Loss: 0.1156, Train: 99.36%, Valid: 64.20% Test: 61.84%\n",
      "Run: 01, Epoch: 48, Loss: 0.1181, Train: 99.36%, Valid: 64.33% Test: 61.62%\n",
      "Run: 01, Epoch: 49, Loss: 0.1153, Train: 99.36%, Valid: 63.92% Test: 61.18%\n",
      "Run: 01, Epoch: 50, Loss: 0.1186, Train: 99.36%, Valid: 64.20% Test: 61.62%\n",
      "Run: 01, Epoch: 51, Loss: 0.1233, Train: 99.45%, Valid: 64.61% Test: 61.84%\n",
      "Run: 01, Epoch: 52, Loss: 0.1094, Train: 99.54%, Valid: 64.47% Test: 61.18%\n",
      "Run: 01, Epoch: 53, Loss: 0.1044, Train: 99.82%, Valid: 64.33% Test: 61.40%\n",
      "Run: 01, Epoch: 54, Loss: 0.0949, Train: 99.82%, Valid: 64.06% Test: 61.62%\n",
      "Run: 01, Epoch: 55, Loss: 0.1042, Train: 99.82%, Valid: 64.33% Test: 61.84%\n",
      "Run: 01, Epoch: 56, Loss: 0.0904, Train: 99.82%, Valid: 64.06% Test: 61.84%\n",
      "Run: 01, Epoch: 57, Loss: 0.0929, Train: 99.82%, Valid: 64.06% Test: 61.84%\n",
      "Run: 01, Epoch: 58, Loss: 0.0868, Train: 99.82%, Valid: 64.06% Test: 62.06%\n",
      "Run: 01, Epoch: 59, Loss: 0.0892, Train: 99.82%, Valid: 64.06% Test: 61.62%\n",
      "Run: 01, Epoch: 60, Loss: 0.0838, Train: 99.82%, Valid: 63.79% Test: 61.18%\n",
      "Run: 01, Epoch: 61, Loss: 0.0754, Train: 99.82%, Valid: 63.79% Test: 61.18%\n",
      "Run: 01, Epoch: 62, Loss: 0.0808, Train: 99.82%, Valid: 63.92% Test: 61.40%\n",
      "Run: 01, Epoch: 63, Loss: 0.0918, Train: 99.82%, Valid: 63.10% Test: 60.96%\n",
      "Run: 01, Epoch: 64, Loss: 0.0729, Train: 99.82%, Valid: 63.10% Test: 60.96%\n",
      "Run: 01, Epoch: 65, Loss: 0.0726, Train: 99.82%, Valid: 63.10% Test: 62.50%\n",
      "Run: 01, Epoch: 66, Loss: 0.0632, Train: 99.82%, Valid: 62.83% Test: 62.28%\n",
      "Run: 01, Epoch: 67, Loss: 0.0590, Train: 99.82%, Valid: 62.41% Test: 62.28%\n",
      "Run: 01, Epoch: 68, Loss: 0.0623, Train: 99.82%, Valid: 62.83% Test: 62.06%\n",
      "Run: 01, Epoch: 69, Loss: 0.0764, Train: 99.82%, Valid: 62.69% Test: 62.28%\n",
      "Run: 01, Epoch: 70, Loss: 0.0615, Train: 99.82%, Valid: 62.41% Test: 61.84%\n",
      "Run: 01, Epoch: 71, Loss: 0.0668, Train: 99.91%, Valid: 62.83% Test: 61.84%\n",
      "Run: 01, Epoch: 72, Loss: 0.0691, Train: 99.91%, Valid: 62.96% Test: 61.40%\n",
      "Run: 01, Epoch: 73, Loss: 0.0672, Train: 99.91%, Valid: 62.96% Test: 61.18%\n",
      "Run: 01, Epoch: 74, Loss: 0.0557, Train: 99.91%, Valid: 62.96% Test: 60.53%\n",
      "Run: 01, Epoch: 75, Loss: 0.0537, Train: 99.91%, Valid: 62.69% Test: 60.96%\n",
      "Run: 01, Epoch: 76, Loss: 0.0545, Train: 99.91%, Valid: 62.55% Test: 61.18%\n",
      "Run: 01, Epoch: 77, Loss: 0.0637, Train: 99.91%, Valid: 62.41% Test: 60.75%\n",
      "Run: 01, Epoch: 78, Loss: 0.0480, Train: 99.91%, Valid: 62.41% Test: 61.18%\n",
      "Run: 01, Epoch: 79, Loss: 0.0579, Train: 99.91%, Valid: 62.41% Test: 61.18%\n",
      "Run: 01, Epoch: 80, Loss: 0.0560, Train: 99.91%, Valid: 62.14% Test: 61.18%\n",
      "Run: 01, Epoch: 81, Loss: 0.0536, Train: 99.91%, Valid: 62.00% Test: 60.96%\n",
      "Run: 01, Epoch: 82, Loss: 0.0524, Train: 99.91%, Valid: 62.00% Test: 60.96%\n",
      "Run: 01, Epoch: 83, Loss: 0.0589, Train: 99.91%, Valid: 61.73% Test: 61.18%\n",
      "Run: 01, Epoch: 84, Loss: 0.0499, Train: 99.91%, Valid: 62.14% Test: 60.96%\n",
      "Run: 01, Epoch: 85, Loss: 0.0514, Train: 99.91%, Valid: 62.14% Test: 61.62%\n",
      "Run: 01, Epoch: 86, Loss: 0.0467, Train: 99.91%, Valid: 62.14% Test: 61.40%\n",
      "Run: 01, Epoch: 87, Loss: 0.0478, Train: 99.91%, Valid: 61.73% Test: 61.62%\n",
      "Run: 01, Epoch: 88, Loss: 0.0437, Train: 99.91%, Valid: 61.73% Test: 61.62%\n",
      "Run: 01, Epoch: 89, Loss: 0.0461, Train: 99.91%, Valid: 61.73% Test: 61.84%\n",
      "Run: 01, Epoch: 90, Loss: 0.0425, Train: 99.91%, Valid: 61.87% Test: 61.18%\n",
      "Run: 01, Epoch: 91, Loss: 0.0449, Train: 99.91%, Valid: 61.59% Test: 61.84%\n",
      "Run: 01, Epoch: 92, Loss: 0.0571, Train: 99.91%, Valid: 61.73% Test: 61.62%\n",
      "Run: 01, Epoch: 93, Loss: 0.0493, Train: 99.91%, Valid: 61.45% Test: 61.62%\n",
      "Run: 01, Epoch: 94, Loss: 0.0443, Train: 99.91%, Valid: 61.59% Test: 61.40%\n",
      "Run: 01, Epoch: 95, Loss: 0.0494, Train: 99.91%, Valid: 61.87% Test: 61.62%\n",
      "Run: 01, Epoch: 96, Loss: 0.0442, Train: 99.91%, Valid: 62.14% Test: 61.62%\n",
      "Run: 01, Epoch: 97, Loss: 0.0483, Train: 99.91%, Valid: 62.55% Test: 61.40%\n",
      "Run: 01, Epoch: 98, Loss: 0.0459, Train: 99.91%, Valid: 62.41% Test: 61.40%\n",
      "Run: 01, Epoch: 99, Loss: 0.0424, Train: 99.91%, Valid: 62.41% Test: 61.40%\n",
      "Run: 01, Epoch: 100, Loss: 0.0397, Train: 99.91%, Valid: 62.69% Test: 62.06%\n",
      "Run 01:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 65.57\n",
      "  Final Train: 98.08\n",
      "   Final Test: 61.84\n",
      "Run: 02, Epoch: 01, Loss: 1.7921, Train: 44.69%, Valid: 41.56% Test: 43.42%\n",
      "Run: 02, Epoch: 02, Loss: 1.1949, Train: 63.64%, Valid: 49.93% Test: 53.95%\n",
      "Run: 02, Epoch: 03, Loss: 1.0860, Train: 71.43%, Valid: 52.81% Test: 56.14%\n",
      "Run: 02, Epoch: 04, Loss: 0.9955, Train: 73.90%, Valid: 52.54% Test: 57.68%\n",
      "Run: 02, Epoch: 05, Loss: 0.9472, Train: 73.99%, Valid: 52.54% Test: 58.99%\n",
      "Run: 02, Epoch: 06, Loss: 0.8670, Train: 75.18%, Valid: 52.26% Test: 58.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 02, Epoch: 07, Loss: 0.8098, Train: 76.92%, Valid: 53.09% Test: 57.89%\n",
      "Run: 02, Epoch: 08, Loss: 0.7580, Train: 79.03%, Valid: 54.18% Test: 57.89%\n",
      "Run: 02, Epoch: 09, Loss: 0.7106, Train: 82.51%, Valid: 55.97% Test: 59.87%\n",
      "Run: 02, Epoch: 10, Loss: 0.6767, Train: 84.43%, Valid: 55.97% Test: 59.43%\n",
      "Run: 02, Epoch: 11, Loss: 0.6332, Train: 86.08%, Valid: 57.34% Test: 60.75%\n",
      "Run: 02, Epoch: 12, Loss: 0.5993, Train: 87.64%, Valid: 57.61% Test: 61.84%\n",
      "Run: 02, Epoch: 13, Loss: 0.5473, Train: 88.28%, Valid: 57.48% Test: 60.09%\n",
      "Run: 02, Epoch: 14, Loss: 0.5185, Train: 89.56%, Valid: 57.48% Test: 60.96%\n",
      "Run: 02, Epoch: 15, Loss: 0.5040, Train: 90.75%, Valid: 57.61% Test: 61.62%\n",
      "Run: 02, Epoch: 16, Loss: 0.4797, Train: 90.66%, Valid: 57.34% Test: 61.40%\n",
      "Run: 02, Epoch: 17, Loss: 0.4277, Train: 91.76%, Valid: 57.75% Test: 62.50%\n",
      "Run: 02, Epoch: 18, Loss: 0.4154, Train: 93.59%, Valid: 60.08% Test: 64.25%\n",
      "Run: 02, Epoch: 19, Loss: 0.4009, Train: 94.87%, Valid: 60.63% Test: 65.13%\n",
      "Run: 02, Epoch: 20, Loss: 0.3655, Train: 95.51%, Valid: 61.32% Test: 65.57%\n",
      "Run: 02, Epoch: 21, Loss: 0.3414, Train: 96.25%, Valid: 61.04% Test: 66.45%\n",
      "Run: 02, Epoch: 22, Loss: 0.3355, Train: 96.25%, Valid: 61.18% Test: 66.45%\n",
      "Run: 02, Epoch: 23, Loss: 0.3249, Train: 96.61%, Valid: 61.73% Test: 66.89%\n",
      "Run: 02, Epoch: 24, Loss: 0.3108, Train: 96.89%, Valid: 61.73% Test: 66.45%\n",
      "Run: 02, Epoch: 25, Loss: 0.2890, Train: 97.44%, Valid: 61.45% Test: 66.23%\n",
      "Run: 02, Epoch: 26, Loss: 0.2745, Train: 97.34%, Valid: 60.91% Test: 66.23%\n",
      "Run: 02, Epoch: 27, Loss: 0.2601, Train: 97.53%, Valid: 60.77% Test: 66.67%\n",
      "Run: 02, Epoch: 28, Loss: 0.2423, Train: 97.89%, Valid: 60.63% Test: 65.57%\n",
      "Run: 02, Epoch: 29, Loss: 0.2328, Train: 98.08%, Valid: 60.77% Test: 66.23%\n",
      "Run: 02, Epoch: 30, Loss: 0.2266, Train: 98.35%, Valid: 60.91% Test: 66.23%\n",
      "Run: 02, Epoch: 31, Loss: 0.2013, Train: 98.35%, Valid: 61.18% Test: 65.35%\n",
      "Run: 02, Epoch: 32, Loss: 0.1947, Train: 98.53%, Valid: 61.32% Test: 65.57%\n",
      "Run: 02, Epoch: 33, Loss: 0.2005, Train: 98.72%, Valid: 61.18% Test: 65.79%\n",
      "Run: 02, Epoch: 34, Loss: 0.1776, Train: 98.72%, Valid: 60.91% Test: 65.79%\n",
      "Run: 02, Epoch: 35, Loss: 0.1688, Train: 98.90%, Valid: 60.49% Test: 66.01%\n",
      "Run: 02, Epoch: 36, Loss: 0.1876, Train: 98.90%, Valid: 60.22% Test: 66.23%\n",
      "Run: 02, Epoch: 37, Loss: 0.1681, Train: 98.90%, Valid: 59.95% Test: 65.35%\n",
      "Run: 02, Epoch: 38, Loss: 0.1683, Train: 98.90%, Valid: 60.08% Test: 65.35%\n",
      "Run: 02, Epoch: 39, Loss: 0.1517, Train: 99.27%, Valid: 59.67% Test: 65.57%\n",
      "Run: 02, Epoch: 40, Loss: 0.1521, Train: 99.27%, Valid: 59.12% Test: 64.69%\n",
      "Run: 02, Epoch: 41, Loss: 0.1624, Train: 99.18%, Valid: 58.85% Test: 64.04%\n",
      "Run: 02, Epoch: 42, Loss: 0.1445, Train: 99.08%, Valid: 58.57% Test: 63.82%\n",
      "Run: 02, Epoch: 43, Loss: 0.1536, Train: 99.18%, Valid: 58.44% Test: 63.82%\n",
      "Run: 02, Epoch: 44, Loss: 0.1263, Train: 99.36%, Valid: 58.44% Test: 64.04%\n",
      "Run: 02, Epoch: 45, Loss: 0.1134, Train: 99.36%, Valid: 58.85% Test: 64.47%\n",
      "Run: 02, Epoch: 46, Loss: 0.1232, Train: 99.45%, Valid: 59.12% Test: 64.47%\n",
      "Run: 02, Epoch: 47, Loss: 0.1054, Train: 99.73%, Valid: 59.12% Test: 65.13%\n",
      "Run: 02, Epoch: 48, Loss: 0.1043, Train: 99.63%, Valid: 58.98% Test: 64.69%\n",
      "Run: 02, Epoch: 49, Loss: 0.1112, Train: 99.63%, Valid: 59.12% Test: 64.69%\n",
      "Run: 02, Epoch: 50, Loss: 0.1112, Train: 99.63%, Valid: 58.85% Test: 64.25%\n",
      "Run: 02, Epoch: 51, Loss: 0.1069, Train: 99.63%, Valid: 58.57% Test: 64.47%\n",
      "Run: 02, Epoch: 52, Loss: 0.0946, Train: 99.63%, Valid: 58.57% Test: 64.25%\n",
      "Run: 02, Epoch: 53, Loss: 0.1105, Train: 99.63%, Valid: 58.16% Test: 64.25%\n",
      "Run: 02, Epoch: 54, Loss: 0.0912, Train: 99.73%, Valid: 58.02% Test: 64.25%\n",
      "Run: 02, Epoch: 55, Loss: 0.0828, Train: 99.63%, Valid: 58.02% Test: 63.82%\n",
      "Run: 02, Epoch: 56, Loss: 0.0804, Train: 99.63%, Valid: 57.75% Test: 64.04%\n",
      "Run: 02, Epoch: 57, Loss: 0.0764, Train: 99.54%, Valid: 57.75% Test: 63.82%\n",
      "Run: 02, Epoch: 58, Loss: 0.0847, Train: 99.63%, Valid: 58.02% Test: 63.38%\n",
      "Run: 02, Epoch: 59, Loss: 0.0808, Train: 99.73%, Valid: 58.02% Test: 63.60%\n",
      "Run: 02, Epoch: 60, Loss: 0.0763, Train: 99.73%, Valid: 58.02% Test: 64.04%\n",
      "Run: 02, Epoch: 61, Loss: 0.0763, Train: 99.73%, Valid: 58.02% Test: 64.04%\n",
      "Run: 02, Epoch: 62, Loss: 0.0836, Train: 99.73%, Valid: 57.89% Test: 64.47%\n",
      "Run: 02, Epoch: 63, Loss: 0.0767, Train: 99.73%, Valid: 57.75% Test: 64.69%\n",
      "Run: 02, Epoch: 64, Loss: 0.0637, Train: 99.73%, Valid: 58.02% Test: 64.91%\n",
      "Run: 02, Epoch: 65, Loss: 0.0785, Train: 99.73%, Valid: 58.44% Test: 65.79%\n",
      "Run: 02, Epoch: 66, Loss: 0.0641, Train: 99.73%, Valid: 58.30% Test: 65.57%\n",
      "Run: 02, Epoch: 67, Loss: 0.0749, Train: 99.73%, Valid: 58.16% Test: 65.57%\n",
      "Run: 02, Epoch: 68, Loss: 0.0658, Train: 99.73%, Valid: 58.30% Test: 65.57%\n",
      "Run: 02, Epoch: 69, Loss: 0.0635, Train: 99.73%, Valid: 58.44% Test: 65.57%\n",
      "Run: 02, Epoch: 70, Loss: 0.0568, Train: 99.82%, Valid: 58.30% Test: 65.35%\n",
      "Run: 02, Epoch: 71, Loss: 0.0658, Train: 99.82%, Valid: 58.57% Test: 65.35%\n",
      "Run: 02, Epoch: 72, Loss: 0.0538, Train: 99.82%, Valid: 58.57% Test: 65.79%\n",
      "Run: 02, Epoch: 73, Loss: 0.0659, Train: 99.82%, Valid: 58.57% Test: 65.79%\n",
      "Run: 02, Epoch: 74, Loss: 0.0598, Train: 99.73%, Valid: 58.71% Test: 65.79%\n",
      "Run: 02, Epoch: 75, Loss: 0.0624, Train: 99.73%, Valid: 58.44% Test: 66.01%\n",
      "Run: 02, Epoch: 76, Loss: 0.0546, Train: 99.73%, Valid: 58.98% Test: 65.57%\n",
      "Run: 02, Epoch: 77, Loss: 0.0463, Train: 99.73%, Valid: 58.71% Test: 65.35%\n",
      "Run: 02, Epoch: 78, Loss: 0.0488, Train: 99.73%, Valid: 58.71% Test: 65.35%\n",
      "Run: 02, Epoch: 79, Loss: 0.0469, Train: 99.73%, Valid: 58.57% Test: 65.57%\n",
      "Run: 02, Epoch: 80, Loss: 0.0619, Train: 99.82%, Valid: 58.16% Test: 65.57%\n",
      "Run: 02, Epoch: 81, Loss: 0.0579, Train: 99.82%, Valid: 58.30% Test: 65.79%\n",
      "Run: 02, Epoch: 82, Loss: 0.0539, Train: 99.91%, Valid: 58.71% Test: 65.35%\n",
      "Run: 02, Epoch: 83, Loss: 0.0483, Train: 99.91%, Valid: 58.98% Test: 66.01%\n",
      "Run: 02, Epoch: 84, Loss: 0.0467, Train: 99.91%, Valid: 58.71% Test: 66.01%\n",
      "Run: 02, Epoch: 85, Loss: 0.0467, Train: 99.91%, Valid: 58.71% Test: 65.79%\n",
      "Run: 02, Epoch: 86, Loss: 0.0467, Train: 99.91%, Valid: 58.44% Test: 65.79%\n",
      "Run: 02, Epoch: 87, Loss: 0.0538, Train: 99.91%, Valid: 58.16% Test: 65.57%\n",
      "Run: 02, Epoch: 88, Loss: 0.0502, Train: 99.82%, Valid: 58.16% Test: 65.35%\n",
      "Run: 02, Epoch: 89, Loss: 0.0355, Train: 99.82%, Valid: 58.16% Test: 65.35%\n",
      "Run: 02, Epoch: 90, Loss: 0.0501, Train: 99.82%, Valid: 58.44% Test: 65.57%\n",
      "Run: 02, Epoch: 91, Loss: 0.0527, Train: 99.82%, Valid: 58.16% Test: 66.01%\n",
      "Run: 02, Epoch: 92, Loss: 0.0486, Train: 99.91%, Valid: 58.30% Test: 66.23%\n",
      "Run: 02, Epoch: 93, Loss: 0.0517, Train: 99.91%, Valid: 58.44% Test: 66.23%\n",
      "Run: 02, Epoch: 94, Loss: 0.0497, Train: 99.91%, Valid: 58.30% Test: 66.23%\n",
      "Run: 02, Epoch: 95, Loss: 0.0436, Train: 99.91%, Valid: 58.57% Test: 66.67%\n",
      "Run: 02, Epoch: 96, Loss: 0.0400, Train: 99.91%, Valid: 58.71% Test: 66.67%\n",
      "Run: 02, Epoch: 97, Loss: 0.0394, Train: 99.91%, Valid: 58.71% Test: 66.23%\n",
      "Run: 02, Epoch: 98, Loss: 0.0316, Train: 99.91%, Valid: 58.85% Test: 66.45%\n",
      "Run: 02, Epoch: 99, Loss: 0.0365, Train: 99.91%, Valid: 58.85% Test: 66.67%\n",
      "Run: 02, Epoch: 100, Loss: 0.0438, Train: 99.91%, Valid: 58.57% Test: 66.67%\n",
      "Run 02:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 61.73\n",
      "  Final Train: 96.61\n",
      "   Final Test: 66.89\n",
      "Run: 03, Epoch: 01, Loss: 1.7530, Train: 31.87%, Valid: 25.10% Test: 26.75%\n",
      "Run: 03, Epoch: 02, Loss: 1.2411, Train: 43.96%, Valid: 36.90% Test: 37.94%\n",
      "Run: 03, Epoch: 03, Loss: 1.0898, Train: 52.20%, Valid: 44.31% Test: 42.54%\n",
      "Run: 03, Epoch: 04, Loss: 1.0093, Train: 55.95%, Valid: 45.82% Test: 47.37%\n",
      "Run: 03, Epoch: 05, Loss: 0.9454, Train: 61.90%, Valid: 49.38% Test: 50.44%\n",
      "Run: 03, Epoch: 06, Loss: 0.8926, Train: 68.13%, Valid: 53.50% Test: 50.66%\n",
      "Run: 03, Epoch: 07, Loss: 0.8429, Train: 73.90%, Valid: 54.60% Test: 52.41%\n",
      "Run: 03, Epoch: 08, Loss: 0.7870, Train: 75.37%, Valid: 56.65% Test: 54.17%\n",
      "Run: 03, Epoch: 09, Loss: 0.7383, Train: 78.66%, Valid: 57.89% Test: 56.14%\n",
      "Run: 03, Epoch: 10, Loss: 0.6939, Train: 79.67%, Valid: 57.34% Test: 55.48%\n",
      "Run: 03, Epoch: 11, Loss: 0.7007, Train: 82.88%, Valid: 58.30% Test: 57.24%\n",
      "Run: 03, Epoch: 12, Loss: 0.6374, Train: 85.44%, Valid: 59.95% Test: 57.68%\n",
      "Run: 03, Epoch: 13, Loss: 0.5946, Train: 86.81%, Valid: 60.91% Test: 57.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 03, Epoch: 14, Loss: 0.5797, Train: 86.63%, Valid: 58.71% Test: 55.70%\n",
      "Run: 03, Epoch: 15, Loss: 0.5337, Train: 86.26%, Valid: 58.98% Test: 55.26%\n",
      "Run: 03, Epoch: 16, Loss: 0.5161, Train: 87.27%, Valid: 59.26% Test: 55.48%\n",
      "Run: 03, Epoch: 17, Loss: 0.5020, Train: 88.92%, Valid: 59.53% Test: 57.02%\n",
      "Run: 03, Epoch: 18, Loss: 0.4666, Train: 91.30%, Valid: 60.91% Test: 59.21%\n",
      "Run: 03, Epoch: 19, Loss: 0.4400, Train: 93.04%, Valid: 61.59% Test: 59.21%\n",
      "Run: 03, Epoch: 20, Loss: 0.4083, Train: 93.22%, Valid: 61.32% Test: 57.46%\n",
      "Run: 03, Epoch: 21, Loss: 0.3820, Train: 93.50%, Valid: 61.45% Test: 57.68%\n",
      "Run: 03, Epoch: 22, Loss: 0.3606, Train: 94.51%, Valid: 62.28% Test: 57.89%\n",
      "Run: 03, Epoch: 23, Loss: 0.3666, Train: 95.51%, Valid: 62.83% Test: 59.21%\n",
      "Run: 03, Epoch: 24, Loss: 0.3427, Train: 95.51%, Valid: 62.83% Test: 59.43%\n",
      "Run: 03, Epoch: 25, Loss: 0.3242, Train: 96.15%, Valid: 62.14% Test: 59.65%\n",
      "Run: 03, Epoch: 26, Loss: 0.2835, Train: 96.43%, Valid: 62.28% Test: 58.99%\n",
      "Run: 03, Epoch: 27, Loss: 0.3103, Train: 96.79%, Valid: 62.69% Test: 58.77%\n",
      "Run: 03, Epoch: 28, Loss: 0.2670, Train: 97.44%, Valid: 62.55% Test: 57.89%\n",
      "Run: 03, Epoch: 29, Loss: 0.2832, Train: 97.80%, Valid: 63.10% Test: 58.11%\n",
      "Run: 03, Epoch: 30, Loss: 0.2297, Train: 97.80%, Valid: 62.14% Test: 58.77%\n",
      "Run: 03, Epoch: 31, Loss: 0.2474, Train: 97.89%, Valid: 62.00% Test: 58.99%\n",
      "Run: 03, Epoch: 32, Loss: 0.2071, Train: 97.89%, Valid: 62.69% Test: 58.99%\n",
      "Run: 03, Epoch: 33, Loss: 0.2054, Train: 97.89%, Valid: 62.41% Test: 59.43%\n",
      "Run: 03, Epoch: 34, Loss: 0.2193, Train: 98.08%, Valid: 62.83% Test: 60.09%\n",
      "Run: 03, Epoch: 35, Loss: 0.1845, Train: 98.44%, Valid: 63.24% Test: 59.65%\n",
      "Run: 03, Epoch: 36, Loss: 0.2102, Train: 98.53%, Valid: 63.37% Test: 59.43%\n",
      "Run: 03, Epoch: 37, Loss: 0.1909, Train: 98.53%, Valid: 63.65% Test: 59.43%\n",
      "Run: 03, Epoch: 38, Loss: 0.1630, Train: 98.72%, Valid: 63.37% Test: 59.65%\n",
      "Run: 03, Epoch: 39, Loss: 0.1469, Train: 98.81%, Valid: 63.92% Test: 59.43%\n",
      "Run: 03, Epoch: 40, Loss: 0.1534, Train: 98.99%, Valid: 63.92% Test: 59.43%\n",
      "Run: 03, Epoch: 41, Loss: 0.1608, Train: 99.08%, Valid: 63.79% Test: 59.21%\n",
      "Run: 03, Epoch: 42, Loss: 0.1424, Train: 99.08%, Valid: 63.65% Test: 59.43%\n",
      "Run: 03, Epoch: 43, Loss: 0.1361, Train: 99.08%, Valid: 63.92% Test: 59.21%\n",
      "Run: 03, Epoch: 44, Loss: 0.1450, Train: 99.08%, Valid: 63.79% Test: 58.99%\n",
      "Run: 03, Epoch: 45, Loss: 0.1466, Train: 99.18%, Valid: 63.51% Test: 59.21%\n",
      "Run: 03, Epoch: 46, Loss: 0.1317, Train: 99.27%, Valid: 63.37% Test: 59.87%\n",
      "Run: 03, Epoch: 47, Loss: 0.1307, Train: 99.36%, Valid: 63.10% Test: 59.65%\n",
      "Run: 03, Epoch: 48, Loss: 0.1221, Train: 99.36%, Valid: 63.37% Test: 59.21%\n",
      "Run: 03, Epoch: 49, Loss: 0.1172, Train: 99.36%, Valid: 63.37% Test: 59.43%\n",
      "Run: 03, Epoch: 50, Loss: 0.1189, Train: 99.45%, Valid: 63.51% Test: 59.43%\n",
      "Run: 03, Epoch: 51, Loss: 0.1065, Train: 99.45%, Valid: 63.79% Test: 59.43%\n",
      "Run: 03, Epoch: 52, Loss: 0.1071, Train: 99.54%, Valid: 63.51% Test: 59.43%\n",
      "Run: 03, Epoch: 53, Loss: 0.0994, Train: 99.54%, Valid: 63.51% Test: 59.21%\n",
      "Run: 03, Epoch: 54, Loss: 0.0983, Train: 99.54%, Valid: 63.65% Test: 59.43%\n",
      "Run: 03, Epoch: 55, Loss: 0.0896, Train: 99.54%, Valid: 63.51% Test: 58.77%\n",
      "Run: 03, Epoch: 56, Loss: 0.0920, Train: 99.54%, Valid: 63.10% Test: 59.43%\n",
      "Run: 03, Epoch: 57, Loss: 0.0862, Train: 99.54%, Valid: 63.24% Test: 58.99%\n",
      "Run: 03, Epoch: 58, Loss: 0.0908, Train: 99.54%, Valid: 63.79% Test: 59.21%\n",
      "Run: 03, Epoch: 59, Loss: 0.0867, Train: 99.54%, Valid: 63.65% Test: 59.87%\n",
      "Run: 03, Epoch: 60, Loss: 0.0798, Train: 99.54%, Valid: 63.51% Test: 59.87%\n",
      "Run: 03, Epoch: 61, Loss: 0.0828, Train: 99.63%, Valid: 63.51% Test: 59.65%\n",
      "Run: 03, Epoch: 62, Loss: 0.0904, Train: 99.54%, Valid: 63.79% Test: 59.43%\n",
      "Run: 03, Epoch: 63, Loss: 0.0831, Train: 99.73%, Valid: 63.37% Test: 58.77%\n",
      "Run: 03, Epoch: 64, Loss: 0.0854, Train: 99.73%, Valid: 63.65% Test: 58.77%\n",
      "Run: 03, Epoch: 65, Loss: 0.0794, Train: 99.73%, Valid: 63.65% Test: 58.11%\n",
      "Run: 03, Epoch: 66, Loss: 0.0691, Train: 99.73%, Valid: 63.37% Test: 58.11%\n",
      "Run: 03, Epoch: 67, Loss: 0.0787, Train: 99.73%, Valid: 63.24% Test: 58.55%\n",
      "Run: 03, Epoch: 68, Loss: 0.0620, Train: 99.73%, Valid: 63.51% Test: 58.33%\n",
      "Run: 03, Epoch: 69, Loss: 0.0744, Train: 99.73%, Valid: 63.37% Test: 58.55%\n",
      "Run: 03, Epoch: 70, Loss: 0.0876, Train: 99.73%, Valid: 63.65% Test: 58.33%\n",
      "Run: 03, Epoch: 71, Loss: 0.0810, Train: 99.82%, Valid: 63.51% Test: 58.55%\n",
      "Run: 03, Epoch: 72, Loss: 0.0615, Train: 99.82%, Valid: 63.79% Test: 58.33%\n",
      "Run: 03, Epoch: 73, Loss: 0.0868, Train: 99.91%, Valid: 63.92% Test: 58.11%\n",
      "Run: 03, Epoch: 74, Loss: 0.0662, Train: 99.91%, Valid: 63.51% Test: 58.33%\n",
      "Run: 03, Epoch: 75, Loss: 0.0536, Train: 99.91%, Valid: 62.96% Test: 57.68%\n",
      "Run: 03, Epoch: 76, Loss: 0.0603, Train: 99.82%, Valid: 62.69% Test: 57.68%\n",
      "Run: 03, Epoch: 77, Loss: 0.0623, Train: 99.82%, Valid: 62.83% Test: 57.89%\n",
      "Run: 03, Epoch: 78, Loss: 0.0758, Train: 99.82%, Valid: 63.24% Test: 58.11%\n",
      "Run: 03, Epoch: 79, Loss: 0.0504, Train: 99.82%, Valid: 62.96% Test: 58.55%\n",
      "Run: 03, Epoch: 80, Loss: 0.0596, Train: 99.82%, Valid: 62.96% Test: 58.99%\n",
      "Run: 03, Epoch: 81, Loss: 0.0535, Train: 99.82%, Valid: 62.96% Test: 58.77%\n",
      "Run: 03, Epoch: 82, Loss: 0.0602, Train: 99.91%, Valid: 62.96% Test: 58.77%\n",
      "Run: 03, Epoch: 83, Loss: 0.0475, Train: 99.91%, Valid: 63.10% Test: 58.11%\n",
      "Run: 03, Epoch: 84, Loss: 0.0471, Train: 99.91%, Valid: 63.10% Test: 58.11%\n",
      "Run: 03, Epoch: 85, Loss: 0.0573, Train: 99.91%, Valid: 63.51% Test: 57.89%\n",
      "Run: 03, Epoch: 86, Loss: 0.0433, Train: 99.82%, Valid: 63.79% Test: 57.68%\n",
      "Run: 03, Epoch: 87, Loss: 0.0463, Train: 99.82%, Valid: 63.79% Test: 57.68%\n",
      "Run: 03, Epoch: 88, Loss: 0.0576, Train: 99.82%, Valid: 63.51% Test: 57.24%\n",
      "Run: 03, Epoch: 89, Loss: 0.0519, Train: 99.82%, Valid: 63.65% Test: 57.24%\n",
      "Run: 03, Epoch: 90, Loss: 0.0550, Train: 99.82%, Valid: 63.65% Test: 57.68%\n",
      "Run: 03, Epoch: 91, Loss: 0.0439, Train: 99.91%, Valid: 63.79% Test: 57.89%\n",
      "Run: 03, Epoch: 92, Loss: 0.0432, Train: 99.91%, Valid: 63.51% Test: 58.33%\n",
      "Run: 03, Epoch: 93, Loss: 0.0579, Train: 99.91%, Valid: 63.51% Test: 58.77%\n",
      "Run: 03, Epoch: 94, Loss: 0.0489, Train: 99.91%, Valid: 64.06% Test: 58.99%\n",
      "Run: 03, Epoch: 95, Loss: 0.0453, Train: 99.91%, Valid: 63.79% Test: 58.55%\n",
      "Run: 03, Epoch: 96, Loss: 0.0484, Train: 99.91%, Valid: 64.20% Test: 58.55%\n",
      "Run: 03, Epoch: 97, Loss: 0.0437, Train: 99.91%, Valid: 64.06% Test: 58.33%\n",
      "Run: 03, Epoch: 98, Loss: 0.0377, Train: 99.91%, Valid: 63.65% Test: 58.33%\n",
      "Run: 03, Epoch: 99, Loss: 0.0439, Train: 99.91%, Valid: 63.37% Test: 58.55%\n",
      "Run: 03, Epoch: 100, Loss: 0.0387, Train: 99.91%, Valid: 62.96% Test: 58.33%\n",
      "Run 03:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 64.20\n",
      "  Final Train: 99.91\n",
      "   Final Test: 58.55\n",
      "Run: 04, Epoch: 01, Loss: 1.7095, Train: 34.89%, Valid: 32.92% Test: 36.40%\n",
      "Run: 04, Epoch: 02, Loss: 1.2281, Train: 47.25%, Valid: 38.96% Test: 42.11%\n",
      "Run: 04, Epoch: 03, Loss: 1.0922, Train: 52.29%, Valid: 41.15% Test: 45.18%\n",
      "Run: 04, Epoch: 04, Loss: 1.0332, Train: 59.43%, Valid: 45.40% Test: 48.46%\n",
      "Run: 04, Epoch: 05, Loss: 0.9705, Train: 61.72%, Valid: 48.56% Test: 51.75%\n",
      "Run: 04, Epoch: 06, Loss: 0.8901, Train: 64.74%, Valid: 50.48% Test: 51.75%\n",
      "Run: 04, Epoch: 07, Loss: 0.8360, Train: 67.49%, Valid: 51.44% Test: 52.63%\n",
      "Run: 04, Epoch: 08, Loss: 0.8011, Train: 70.97%, Valid: 54.18% Test: 53.95%\n",
      "Run: 04, Epoch: 09, Loss: 0.7537, Train: 75.46%, Valid: 56.10% Test: 56.80%\n",
      "Run: 04, Epoch: 10, Loss: 0.7028, Train: 78.39%, Valid: 56.10% Test: 57.46%\n",
      "Run: 04, Epoch: 11, Loss: 0.6661, Train: 81.41%, Valid: 57.89% Test: 59.43%\n",
      "Run: 04, Epoch: 12, Loss: 0.6292, Train: 83.79%, Valid: 57.75% Test: 60.31%\n",
      "Run: 04, Epoch: 13, Loss: 0.5954, Train: 85.26%, Valid: 58.44% Test: 60.53%\n",
      "Run: 04, Epoch: 14, Loss: 0.5584, Train: 87.09%, Valid: 59.26% Test: 61.62%\n",
      "Run: 04, Epoch: 15, Loss: 0.5235, Train: 88.92%, Valid: 60.36% Test: 62.06%\n",
      "Run: 04, Epoch: 16, Loss: 0.4928, Train: 89.84%, Valid: 60.08% Test: 62.28%\n",
      "Run: 04, Epoch: 17, Loss: 0.4722, Train: 90.66%, Valid: 61.18% Test: 62.94%\n",
      "Run: 04, Epoch: 18, Loss: 0.4482, Train: 91.48%, Valid: 61.73% Test: 63.16%\n",
      "Run: 04, Epoch: 19, Loss: 0.4287, Train: 92.03%, Valid: 63.24% Test: 63.60%\n",
      "Run: 04, Epoch: 20, Loss: 0.4002, Train: 93.04%, Valid: 62.96% Test: 64.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 04, Epoch: 21, Loss: 0.3786, Train: 93.77%, Valid: 62.55% Test: 64.47%\n",
      "Run: 04, Epoch: 22, Loss: 0.3552, Train: 94.78%, Valid: 62.83% Test: 64.04%\n",
      "Run: 04, Epoch: 23, Loss: 0.3495, Train: 95.33%, Valid: 62.41% Test: 64.69%\n",
      "Run: 04, Epoch: 24, Loss: 0.3254, Train: 95.33%, Valid: 61.87% Test: 65.79%\n",
      "Run: 04, Epoch: 25, Loss: 0.3157, Train: 96.06%, Valid: 61.32% Test: 65.35%\n",
      "Run: 04, Epoch: 26, Loss: 0.2867, Train: 95.79%, Valid: 61.73% Test: 65.13%\n",
      "Run: 04, Epoch: 27, Loss: 0.2749, Train: 96.79%, Valid: 62.28% Test: 65.13%\n",
      "Run: 04, Epoch: 28, Loss: 0.2839, Train: 97.07%, Valid: 62.00% Test: 64.91%\n",
      "Run: 04, Epoch: 29, Loss: 0.2456, Train: 97.25%, Valid: 62.14% Test: 64.91%\n",
      "Run: 04, Epoch: 30, Loss: 0.2413, Train: 97.62%, Valid: 62.83% Test: 65.13%\n",
      "Run: 04, Epoch: 31, Loss: 0.2357, Train: 97.80%, Valid: 62.55% Test: 65.35%\n",
      "Run: 04, Epoch: 32, Loss: 0.2257, Train: 97.89%, Valid: 62.00% Test: 65.35%\n",
      "Run: 04, Epoch: 33, Loss: 0.1900, Train: 97.89%, Valid: 61.45% Test: 64.47%\n",
      "Run: 04, Epoch: 34, Loss: 0.1931, Train: 97.80%, Valid: 61.73% Test: 64.47%\n",
      "Run: 04, Epoch: 35, Loss: 0.2161, Train: 97.99%, Valid: 61.87% Test: 64.47%\n",
      "Run: 04, Epoch: 36, Loss: 0.1760, Train: 98.17%, Valid: 62.00% Test: 64.69%\n",
      "Run: 04, Epoch: 37, Loss: 0.1740, Train: 98.26%, Valid: 62.83% Test: 64.91%\n",
      "Run: 04, Epoch: 38, Loss: 0.1677, Train: 98.35%, Valid: 62.69% Test: 64.69%\n",
      "Run: 04, Epoch: 39, Loss: 0.1594, Train: 98.53%, Valid: 62.96% Test: 64.47%\n",
      "Run: 04, Epoch: 40, Loss: 0.1630, Train: 98.99%, Valid: 63.10% Test: 64.25%\n",
      "Run: 04, Epoch: 41, Loss: 0.1530, Train: 99.08%, Valid: 62.96% Test: 63.60%\n",
      "Run: 04, Epoch: 42, Loss: 0.1470, Train: 99.27%, Valid: 62.96% Test: 62.94%\n",
      "Run: 04, Epoch: 43, Loss: 0.1597, Train: 99.45%, Valid: 63.79% Test: 63.82%\n",
      "Run: 04, Epoch: 44, Loss: 0.1411, Train: 99.54%, Valid: 63.92% Test: 64.25%\n",
      "Run: 04, Epoch: 45, Loss: 0.1220, Train: 99.45%, Valid: 63.51% Test: 64.91%\n",
      "Run: 04, Epoch: 46, Loss: 0.1114, Train: 99.45%, Valid: 63.37% Test: 65.13%\n",
      "Run: 04, Epoch: 47, Loss: 0.1154, Train: 99.54%, Valid: 63.92% Test: 65.79%\n",
      "Run: 04, Epoch: 48, Loss: 0.1221, Train: 99.54%, Valid: 64.06% Test: 65.79%\n",
      "Run: 04, Epoch: 49, Loss: 0.1259, Train: 99.54%, Valid: 64.06% Test: 66.01%\n",
      "Run: 04, Epoch: 50, Loss: 0.1076, Train: 99.54%, Valid: 64.06% Test: 66.23%\n",
      "Run: 04, Epoch: 51, Loss: 0.1075, Train: 99.63%, Valid: 63.24% Test: 65.35%\n",
      "Run: 04, Epoch: 52, Loss: 0.1136, Train: 99.63%, Valid: 62.96% Test: 64.91%\n",
      "Run: 04, Epoch: 53, Loss: 0.1123, Train: 99.63%, Valid: 63.24% Test: 64.91%\n",
      "Run: 04, Epoch: 54, Loss: 0.1028, Train: 99.63%, Valid: 63.37% Test: 64.47%\n",
      "Run: 04, Epoch: 55, Loss: 0.0931, Train: 99.63%, Valid: 63.37% Test: 64.91%\n",
      "Run: 04, Epoch: 56, Loss: 0.0984, Train: 99.63%, Valid: 63.65% Test: 64.69%\n",
      "Run: 04, Epoch: 57, Loss: 0.0888, Train: 99.63%, Valid: 63.65% Test: 64.69%\n",
      "Run: 04, Epoch: 58, Loss: 0.0818, Train: 99.54%, Valid: 63.92% Test: 64.69%\n",
      "Run: 04, Epoch: 59, Loss: 0.0937, Train: 99.63%, Valid: 63.92% Test: 64.47%\n",
      "Run: 04, Epoch: 60, Loss: 0.0763, Train: 99.63%, Valid: 64.20% Test: 64.47%\n",
      "Run: 04, Epoch: 61, Loss: 0.0755, Train: 99.63%, Valid: 64.06% Test: 63.82%\n",
      "Run: 04, Epoch: 62, Loss: 0.0742, Train: 99.63%, Valid: 64.20% Test: 63.82%\n",
      "Run: 04, Epoch: 63, Loss: 0.0803, Train: 99.63%, Valid: 63.51% Test: 63.60%\n",
      "Run: 04, Epoch: 64, Loss: 0.0728, Train: 99.63%, Valid: 63.51% Test: 63.60%\n",
      "Run: 04, Epoch: 65, Loss: 0.0748, Train: 99.63%, Valid: 63.65% Test: 63.60%\n",
      "Run: 04, Epoch: 66, Loss: 0.0780, Train: 99.63%, Valid: 63.65% Test: 63.38%\n",
      "Run: 04, Epoch: 67, Loss: 0.0687, Train: 99.63%, Valid: 64.20% Test: 63.38%\n",
      "Run: 04, Epoch: 68, Loss: 0.0762, Train: 99.63%, Valid: 64.33% Test: 63.82%\n",
      "Run: 04, Epoch: 69, Loss: 0.0626, Train: 99.63%, Valid: 64.20% Test: 64.04%\n",
      "Run: 04, Epoch: 70, Loss: 0.0664, Train: 99.63%, Valid: 63.79% Test: 63.60%\n",
      "Run: 04, Epoch: 71, Loss: 0.0666, Train: 99.63%, Valid: 63.79% Test: 63.38%\n",
      "Run: 04, Epoch: 72, Loss: 0.0664, Train: 99.63%, Valid: 63.92% Test: 63.82%\n",
      "Run: 04, Epoch: 73, Loss: 0.0574, Train: 99.73%, Valid: 64.06% Test: 64.04%\n",
      "Run: 04, Epoch: 74, Loss: 0.0611, Train: 99.73%, Valid: 64.20% Test: 64.04%\n",
      "Run: 04, Epoch: 75, Loss: 0.0565, Train: 99.73%, Valid: 64.33% Test: 63.82%\n",
      "Run: 04, Epoch: 76, Loss: 0.0551, Train: 99.82%, Valid: 64.47% Test: 63.60%\n",
      "Run: 04, Epoch: 77, Loss: 0.0553, Train: 99.82%, Valid: 64.20% Test: 64.04%\n",
      "Run: 04, Epoch: 78, Loss: 0.0667, Train: 99.82%, Valid: 64.20% Test: 64.04%\n",
      "Run: 04, Epoch: 79, Loss: 0.0535, Train: 99.82%, Valid: 63.79% Test: 64.25%\n",
      "Run: 04, Epoch: 80, Loss: 0.0595, Train: 99.91%, Valid: 63.92% Test: 64.04%\n",
      "Run: 04, Epoch: 81, Loss: 0.0565, Train: 99.91%, Valid: 64.33% Test: 64.04%\n",
      "Run: 04, Epoch: 82, Loss: 0.0677, Train: 99.82%, Valid: 64.61% Test: 63.82%\n",
      "Run: 04, Epoch: 83, Loss: 0.0607, Train: 99.82%, Valid: 64.33% Test: 63.60%\n",
      "Run: 04, Epoch: 84, Loss: 0.0627, Train: 99.82%, Valid: 64.33% Test: 64.04%\n",
      "Run: 04, Epoch: 85, Loss: 0.0566, Train: 99.82%, Valid: 64.20% Test: 64.04%\n",
      "Run: 04, Epoch: 86, Loss: 0.0470, Train: 99.82%, Valid: 64.06% Test: 64.47%\n",
      "Run: 04, Epoch: 87, Loss: 0.0576, Train: 99.82%, Valid: 64.47% Test: 64.25%\n",
      "Run: 04, Epoch: 88, Loss: 0.0485, Train: 99.91%, Valid: 64.61% Test: 64.04%\n",
      "Run: 04, Epoch: 89, Loss: 0.0512, Train: 99.91%, Valid: 64.61% Test: 64.04%\n",
      "Run: 04, Epoch: 90, Loss: 0.0427, Train: 99.91%, Valid: 64.33% Test: 64.47%\n",
      "Run: 04, Epoch: 91, Loss: 0.0524, Train: 99.82%, Valid: 64.33% Test: 64.47%\n",
      "Run: 04, Epoch: 92, Loss: 0.0498, Train: 99.82%, Valid: 64.47% Test: 64.47%\n",
      "Run: 04, Epoch: 93, Loss: 0.0513, Train: 99.82%, Valid: 64.61% Test: 64.91%\n",
      "Run: 04, Epoch: 94, Loss: 0.0536, Train: 99.82%, Valid: 64.61% Test: 64.91%\n",
      "Run: 04, Epoch: 95, Loss: 0.0448, Train: 99.82%, Valid: 64.47% Test: 65.13%\n",
      "Run: 04, Epoch: 96, Loss: 0.0431, Train: 99.82%, Valid: 64.61% Test: 64.91%\n",
      "Run: 04, Epoch: 97, Loss: 0.0383, Train: 99.82%, Valid: 64.33% Test: 65.35%\n",
      "Run: 04, Epoch: 98, Loss: 0.0413, Train: 99.82%, Valid: 64.33% Test: 65.57%\n",
      "Run: 04, Epoch: 99, Loss: 0.0415, Train: 99.82%, Valid: 64.47% Test: 66.01%\n",
      "Run: 04, Epoch: 100, Loss: 0.0481, Train: 99.82%, Valid: 64.20% Test: 65.79%\n",
      "Run 04:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 64.61\n",
      "  Final Train: 99.82\n",
      "   Final Test: 63.82\n",
      "Run: 05, Epoch: 01, Loss: 1.7311, Train: 47.34%, Valid: 39.78% Test: 46.27%\n",
      "Run: 05, Epoch: 02, Loss: 1.2429, Train: 57.23%, Valid: 44.31% Test: 49.34%\n",
      "Run: 05, Epoch: 03, Loss: 1.1071, Train: 64.56%, Valid: 48.42% Test: 50.22%\n",
      "Run: 05, Epoch: 04, Loss: 1.0168, Train: 65.66%, Valid: 49.66% Test: 50.22%\n",
      "Run: 05, Epoch: 05, Loss: 0.9753, Train: 67.86%, Valid: 48.29% Test: 49.34%\n",
      "Run: 05, Epoch: 06, Loss: 0.9296, Train: 71.15%, Valid: 49.66% Test: 50.88%\n",
      "Run: 05, Epoch: 07, Loss: 0.8772, Train: 72.34%, Valid: 50.21% Test: 51.97%\n",
      "Run: 05, Epoch: 08, Loss: 0.8357, Train: 75.27%, Valid: 52.95% Test: 53.73%\n",
      "Run: 05, Epoch: 09, Loss: 0.7476, Train: 79.40%, Valid: 55.28% Test: 56.36%\n",
      "Run: 05, Epoch: 10, Loss: 0.7334, Train: 81.23%, Valid: 56.52% Test: 58.11%\n",
      "Run: 05, Epoch: 11, Loss: 0.7067, Train: 82.69%, Valid: 58.30% Test: 60.53%\n",
      "Run: 05, Epoch: 12, Loss: 0.6650, Train: 83.97%, Valid: 59.26% Test: 61.84%\n",
      "Run: 05, Epoch: 13, Loss: 0.6142, Train: 85.62%, Valid: 60.36% Test: 63.38%\n",
      "Run: 05, Epoch: 14, Loss: 0.5689, Train: 86.90%, Valid: 59.81% Test: 63.38%\n",
      "Run: 05, Epoch: 15, Loss: 0.5540, Train: 86.72%, Valid: 59.95% Test: 63.82%\n",
      "Run: 05, Epoch: 16, Loss: 0.5376, Train: 88.19%, Valid: 60.08% Test: 62.94%\n",
      "Run: 05, Epoch: 17, Loss: 0.5368, Train: 89.56%, Valid: 61.32% Test: 64.04%\n",
      "Run: 05, Epoch: 18, Loss: 0.4759, Train: 91.48%, Valid: 61.73% Test: 64.69%\n",
      "Run: 05, Epoch: 19, Loss: 0.4488, Train: 92.77%, Valid: 61.18% Test: 65.79%\n",
      "Run: 05, Epoch: 20, Loss: 0.4241, Train: 93.68%, Valid: 60.91% Test: 66.01%\n",
      "Run: 05, Epoch: 21, Loss: 0.4052, Train: 94.78%, Valid: 60.08% Test: 66.89%\n",
      "Run: 05, Epoch: 22, Loss: 0.3925, Train: 95.42%, Valid: 61.18% Test: 66.89%\n",
      "Run: 05, Epoch: 23, Loss: 0.3813, Train: 95.79%, Valid: 61.73% Test: 66.89%\n",
      "Run: 05, Epoch: 24, Loss: 0.3481, Train: 96.06%, Valid: 61.59% Test: 66.89%\n",
      "Run: 05, Epoch: 25, Loss: 0.3312, Train: 96.43%, Valid: 61.73% Test: 66.23%\n",
      "Run: 05, Epoch: 26, Loss: 0.3271, Train: 96.43%, Valid: 61.59% Test: 66.01%\n",
      "Run: 05, Epoch: 27, Loss: 0.3184, Train: 96.79%, Valid: 61.32% Test: 66.23%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 05, Epoch: 28, Loss: 0.2782, Train: 96.89%, Valid: 61.32% Test: 66.01%\n",
      "Run: 05, Epoch: 29, Loss: 0.2608, Train: 97.16%, Valid: 62.00% Test: 65.57%\n",
      "Run: 05, Epoch: 30, Loss: 0.2685, Train: 97.34%, Valid: 62.14% Test: 65.79%\n",
      "Run: 05, Epoch: 31, Loss: 0.2641, Train: 97.89%, Valid: 62.55% Test: 66.45%\n",
      "Run: 05, Epoch: 32, Loss: 0.2575, Train: 97.99%, Valid: 62.28% Test: 66.67%\n",
      "Run: 05, Epoch: 33, Loss: 0.2080, Train: 97.80%, Valid: 62.83% Test: 66.89%\n",
      "Run: 05, Epoch: 34, Loss: 0.2112, Train: 97.80%, Valid: 62.83% Test: 66.67%\n",
      "Run: 05, Epoch: 35, Loss: 0.1872, Train: 98.08%, Valid: 63.24% Test: 66.67%\n",
      "Run: 05, Epoch: 36, Loss: 0.1869, Train: 98.17%, Valid: 63.24% Test: 66.45%\n",
      "Run: 05, Epoch: 37, Loss: 0.1767, Train: 98.26%, Valid: 63.24% Test: 66.45%\n",
      "Run: 05, Epoch: 38, Loss: 0.1890, Train: 98.17%, Valid: 62.96% Test: 66.45%\n",
      "Run: 05, Epoch: 39, Loss: 0.1671, Train: 98.35%, Valid: 62.55% Test: 66.23%\n",
      "Run: 05, Epoch: 40, Loss: 0.1475, Train: 98.72%, Valid: 62.83% Test: 66.01%\n",
      "Run: 05, Epoch: 41, Loss: 0.1680, Train: 98.72%, Valid: 62.55% Test: 66.23%\n",
      "Run: 05, Epoch: 42, Loss: 0.1452, Train: 98.99%, Valid: 62.55% Test: 66.67%\n",
      "Run: 05, Epoch: 43, Loss: 0.1495, Train: 99.08%, Valid: 62.55% Test: 66.67%\n",
      "Run: 05, Epoch: 44, Loss: 0.1531, Train: 99.18%, Valid: 62.55% Test: 66.01%\n",
      "Run: 05, Epoch: 45, Loss: 0.1175, Train: 99.27%, Valid: 62.41% Test: 66.23%\n",
      "Run: 05, Epoch: 46, Loss: 0.1399, Train: 99.36%, Valid: 62.14% Test: 65.79%\n",
      "Run: 05, Epoch: 47, Loss: 0.1145, Train: 99.36%, Valid: 62.00% Test: 66.01%\n",
      "Run: 05, Epoch: 48, Loss: 0.1088, Train: 99.36%, Valid: 62.14% Test: 65.79%\n",
      "Run: 05, Epoch: 49, Loss: 0.1108, Train: 99.45%, Valid: 62.28% Test: 65.57%\n",
      "Run: 05, Epoch: 50, Loss: 0.1245, Train: 99.45%, Valid: 61.87% Test: 65.35%\n",
      "Run: 05, Epoch: 51, Loss: 0.1131, Train: 99.45%, Valid: 61.87% Test: 65.13%\n",
      "Run: 05, Epoch: 52, Loss: 0.1009, Train: 99.36%, Valid: 61.45% Test: 65.35%\n",
      "Run: 05, Epoch: 53, Loss: 0.1095, Train: 99.63%, Valid: 61.59% Test: 65.35%\n",
      "Run: 05, Epoch: 54, Loss: 0.0899, Train: 99.63%, Valid: 61.32% Test: 64.69%\n",
      "Run: 05, Epoch: 55, Loss: 0.1110, Train: 99.73%, Valid: 61.32% Test: 64.25%\n",
      "Run: 05, Epoch: 56, Loss: 0.0981, Train: 99.73%, Valid: 61.32% Test: 64.04%\n",
      "Run: 05, Epoch: 57, Loss: 0.1002, Train: 99.73%, Valid: 61.45% Test: 64.04%\n",
      "Run: 05, Epoch: 58, Loss: 0.0950, Train: 99.73%, Valid: 62.28% Test: 64.04%\n",
      "Run: 05, Epoch: 59, Loss: 0.0860, Train: 99.73%, Valid: 62.14% Test: 64.04%\n",
      "Run: 05, Epoch: 60, Loss: 0.0904, Train: 99.73%, Valid: 61.87% Test: 63.82%\n",
      "Run: 05, Epoch: 61, Loss: 0.0878, Train: 99.73%, Valid: 61.45% Test: 63.82%\n",
      "Run: 05, Epoch: 62, Loss: 0.0848, Train: 99.73%, Valid: 61.18% Test: 63.60%\n",
      "Run: 05, Epoch: 63, Loss: 0.0852, Train: 99.73%, Valid: 61.18% Test: 63.82%\n",
      "Run: 05, Epoch: 64, Loss: 0.0870, Train: 99.73%, Valid: 60.91% Test: 63.82%\n",
      "Run: 05, Epoch: 65, Loss: 0.0796, Train: 99.82%, Valid: 61.32% Test: 64.04%\n",
      "Run: 05, Epoch: 66, Loss: 0.0744, Train: 99.82%, Valid: 61.32% Test: 63.60%\n",
      "Run: 05, Epoch: 67, Loss: 0.0758, Train: 99.82%, Valid: 61.73% Test: 63.60%\n",
      "Run: 05, Epoch: 68, Loss: 0.0694, Train: 99.82%, Valid: 61.45% Test: 64.04%\n",
      "Run: 05, Epoch: 69, Loss: 0.0726, Train: 99.82%, Valid: 61.32% Test: 64.04%\n",
      "Run: 05, Epoch: 70, Loss: 0.0612, Train: 99.82%, Valid: 61.18% Test: 64.04%\n",
      "Run: 05, Epoch: 71, Loss: 0.0638, Train: 99.82%, Valid: 61.32% Test: 63.82%\n",
      "Run: 05, Epoch: 72, Loss: 0.0667, Train: 99.82%, Valid: 60.77% Test: 63.60%\n",
      "Run: 05, Epoch: 73, Loss: 0.0653, Train: 99.82%, Valid: 61.87% Test: 63.60%\n",
      "Run: 05, Epoch: 74, Loss: 0.0583, Train: 99.82%, Valid: 61.45% Test: 64.04%\n",
      "Run: 05, Epoch: 75, Loss: 0.0668, Train: 99.82%, Valid: 61.32% Test: 63.38%\n",
      "Run: 05, Epoch: 76, Loss: 0.0582, Train: 99.82%, Valid: 61.04% Test: 63.60%\n",
      "Run: 05, Epoch: 77, Loss: 0.0593, Train: 99.82%, Valid: 61.45% Test: 62.94%\n",
      "Run: 05, Epoch: 78, Loss: 0.0609, Train: 99.82%, Valid: 61.04% Test: 63.38%\n",
      "Run: 05, Epoch: 79, Loss: 0.0564, Train: 99.82%, Valid: 61.04% Test: 63.60%\n",
      "Run: 05, Epoch: 80, Loss: 0.0422, Train: 99.82%, Valid: 61.18% Test: 63.60%\n",
      "Run: 05, Epoch: 81, Loss: 0.0530, Train: 99.82%, Valid: 60.91% Test: 63.38%\n",
      "Run: 05, Epoch: 82, Loss: 0.0506, Train: 99.82%, Valid: 60.49% Test: 63.82%\n",
      "Run: 05, Epoch: 83, Loss: 0.0562, Train: 99.82%, Valid: 60.77% Test: 63.82%\n",
      "Run: 05, Epoch: 84, Loss: 0.0509, Train: 99.82%, Valid: 60.63% Test: 63.38%\n",
      "Run: 05, Epoch: 85, Loss: 0.0666, Train: 99.82%, Valid: 60.77% Test: 63.82%\n",
      "Run: 05, Epoch: 86, Loss: 0.0473, Train: 99.82%, Valid: 60.63% Test: 63.60%\n",
      "Run: 05, Epoch: 87, Loss: 0.0483, Train: 99.82%, Valid: 60.91% Test: 63.60%\n",
      "Run: 05, Epoch: 88, Loss: 0.0401, Train: 99.82%, Valid: 60.91% Test: 63.38%\n",
      "Run: 05, Epoch: 89, Loss: 0.0432, Train: 99.82%, Valid: 61.18% Test: 63.60%\n",
      "Run: 05, Epoch: 90, Loss: 0.0547, Train: 99.82%, Valid: 60.63% Test: 63.38%\n",
      "Run: 05, Epoch: 91, Loss: 0.0385, Train: 99.82%, Valid: 60.63% Test: 63.60%\n",
      "Run: 05, Epoch: 92, Loss: 0.0359, Train: 99.82%, Valid: 60.08% Test: 64.25%\n",
      "Run: 05, Epoch: 93, Loss: 0.0440, Train: 99.82%, Valid: 60.36% Test: 64.04%\n",
      "Run: 05, Epoch: 94, Loss: 0.0514, Train: 99.82%, Valid: 60.36% Test: 63.60%\n",
      "Run: 05, Epoch: 95, Loss: 0.0383, Train: 99.82%, Valid: 60.22% Test: 63.82%\n",
      "Run: 05, Epoch: 96, Loss: 0.0477, Train: 99.82%, Valid: 60.36% Test: 63.60%\n",
      "Run: 05, Epoch: 97, Loss: 0.0373, Train: 99.82%, Valid: 60.49% Test: 63.60%\n",
      "Run: 05, Epoch: 98, Loss: 0.0365, Train: 99.91%, Valid: 60.63% Test: 63.38%\n",
      "Run: 05, Epoch: 99, Loss: 0.0406, Train: 99.91%, Valid: 60.36% Test: 63.38%\n",
      "Run: 05, Epoch: 100, Loss: 0.0513, Train: 99.91%, Valid: 60.49% Test: 63.38%\n",
      "Run 05:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 63.24\n",
      "  Final Train: 98.08\n",
      "   Final Test: 66.67\n",
      "Run: 06, Epoch: 01, Loss: 1.8337, Train: 35.35%, Valid: 34.43% Test: 32.89%\n",
      "Run: 06, Epoch: 02, Loss: 1.2203, Train: 43.96%, Valid: 35.94% Test: 35.53%\n",
      "Run: 06, Epoch: 03, Loss: 1.1379, Train: 48.90%, Valid: 40.74% Test: 38.82%\n",
      "Run: 06, Epoch: 04, Loss: 1.0661, Train: 55.31%, Valid: 46.64% Test: 43.42%\n",
      "Run: 06, Epoch: 05, Loss: 1.0125, Train: 61.36%, Valid: 50.34% Test: 45.83%\n",
      "Run: 06, Epoch: 06, Loss: 0.9544, Train: 65.02%, Valid: 52.26% Test: 49.78%\n",
      "Run: 06, Epoch: 07, Loss: 0.9082, Train: 70.15%, Valid: 55.42% Test: 52.41%\n",
      "Run: 06, Epoch: 08, Loss: 0.8673, Train: 72.99%, Valid: 55.69% Test: 50.44%\n",
      "Run: 06, Epoch: 09, Loss: 0.8171, Train: 75.46%, Valid: 57.06% Test: 51.75%\n",
      "Run: 06, Epoch: 10, Loss: 0.8026, Train: 78.30%, Valid: 58.16% Test: 55.04%\n",
      "Run: 06, Epoch: 11, Loss: 0.7567, Train: 80.95%, Valid: 58.98% Test: 56.36%\n",
      "Run: 06, Epoch: 12, Loss: 0.7248, Train: 83.24%, Valid: 60.49% Test: 58.55%\n",
      "Run: 06, Epoch: 13, Loss: 0.6851, Train: 86.08%, Valid: 60.63% Test: 60.75%\n",
      "Run: 06, Epoch: 14, Loss: 0.6312, Train: 88.00%, Valid: 61.04% Test: 60.53%\n",
      "Run: 06, Epoch: 15, Loss: 0.6347, Train: 88.64%, Valid: 61.04% Test: 60.31%\n",
      "Run: 06, Epoch: 16, Loss: 0.5897, Train: 88.92%, Valid: 61.87% Test: 61.62%\n",
      "Run: 06, Epoch: 17, Loss: 0.5642, Train: 90.02%, Valid: 62.14% Test: 61.62%\n",
      "Run: 06, Epoch: 18, Loss: 0.5565, Train: 90.66%, Valid: 62.41% Test: 62.06%\n",
      "Run: 06, Epoch: 19, Loss: 0.4999, Train: 91.39%, Valid: 62.69% Test: 62.28%\n",
      "Run: 06, Epoch: 20, Loss: 0.4764, Train: 92.67%, Valid: 63.10% Test: 62.06%\n",
      "Run: 06, Epoch: 21, Loss: 0.4614, Train: 92.77%, Valid: 63.37% Test: 62.50%\n",
      "Run: 06, Epoch: 22, Loss: 0.4462, Train: 93.59%, Valid: 62.41% Test: 62.28%\n",
      "Run: 06, Epoch: 23, Loss: 0.4137, Train: 94.51%, Valid: 62.69% Test: 61.84%\n",
      "Run: 06, Epoch: 24, Loss: 0.4043, Train: 94.87%, Valid: 63.24% Test: 61.62%\n",
      "Run: 06, Epoch: 25, Loss: 0.3703, Train: 95.42%, Valid: 63.79% Test: 61.84%\n",
      "Run: 06, Epoch: 26, Loss: 0.3758, Train: 95.88%, Valid: 64.88% Test: 62.06%\n",
      "Run: 06, Epoch: 27, Loss: 0.3567, Train: 95.79%, Valid: 64.33% Test: 63.38%\n",
      "Run: 06, Epoch: 28, Loss: 0.3162, Train: 96.06%, Valid: 64.33% Test: 62.50%\n",
      "Run: 06, Epoch: 29, Loss: 0.3230, Train: 96.06%, Valid: 63.79% Test: 61.62%\n",
      "Run: 06, Epoch: 30, Loss: 0.3241, Train: 96.34%, Valid: 63.92% Test: 62.28%\n",
      "Run: 06, Epoch: 31, Loss: 0.3008, Train: 96.61%, Valid: 63.37% Test: 62.72%\n",
      "Run: 06, Epoch: 32, Loss: 0.2790, Train: 97.25%, Valid: 63.79% Test: 64.25%\n",
      "Run: 06, Epoch: 33, Loss: 0.2705, Train: 97.16%, Valid: 63.92% Test: 63.60%\n",
      "Run: 06, Epoch: 34, Loss: 0.2329, Train: 97.07%, Valid: 63.65% Test: 64.25%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 06, Epoch: 35, Loss: 0.2539, Train: 97.62%, Valid: 63.65% Test: 64.25%\n",
      "Run: 06, Epoch: 36, Loss: 0.2407, Train: 97.89%, Valid: 63.65% Test: 64.91%\n",
      "Run: 06, Epoch: 37, Loss: 0.2447, Train: 98.08%, Valid: 63.65% Test: 65.35%\n",
      "Run: 06, Epoch: 38, Loss: 0.2217, Train: 98.26%, Valid: 63.65% Test: 64.69%\n",
      "Run: 06, Epoch: 39, Loss: 0.1973, Train: 98.26%, Valid: 63.65% Test: 64.25%\n",
      "Run: 06, Epoch: 40, Loss: 0.1905, Train: 98.35%, Valid: 63.37% Test: 64.04%\n",
      "Run: 06, Epoch: 41, Loss: 0.1888, Train: 98.53%, Valid: 63.79% Test: 64.91%\n",
      "Run: 06, Epoch: 42, Loss: 0.1758, Train: 98.53%, Valid: 63.92% Test: 65.13%\n",
      "Run: 06, Epoch: 43, Loss: 0.1695, Train: 98.53%, Valid: 63.65% Test: 65.13%\n",
      "Run: 06, Epoch: 44, Loss: 0.1584, Train: 98.72%, Valid: 63.65% Test: 64.91%\n",
      "Run: 06, Epoch: 45, Loss: 0.1592, Train: 98.72%, Valid: 63.79% Test: 64.25%\n",
      "Run: 06, Epoch: 46, Loss: 0.1653, Train: 98.99%, Valid: 64.06% Test: 64.25%\n",
      "Run: 06, Epoch: 47, Loss: 0.1494, Train: 98.99%, Valid: 64.33% Test: 64.04%\n",
      "Run: 06, Epoch: 48, Loss: 0.1447, Train: 99.08%, Valid: 64.47% Test: 64.25%\n",
      "Run: 06, Epoch: 49, Loss: 0.1444, Train: 99.08%, Valid: 64.33% Test: 63.82%\n",
      "Run: 06, Epoch: 50, Loss: 0.1342, Train: 99.27%, Valid: 63.92% Test: 63.82%\n",
      "Run: 06, Epoch: 51, Loss: 0.1236, Train: 99.36%, Valid: 64.20% Test: 64.04%\n",
      "Run: 06, Epoch: 52, Loss: 0.1276, Train: 99.36%, Valid: 63.92% Test: 63.82%\n",
      "Run: 06, Epoch: 53, Loss: 0.1356, Train: 99.36%, Valid: 64.20% Test: 63.82%\n",
      "Run: 06, Epoch: 54, Loss: 0.1154, Train: 99.36%, Valid: 63.92% Test: 64.47%\n",
      "Run: 06, Epoch: 55, Loss: 0.1138, Train: 99.45%, Valid: 64.06% Test: 64.04%\n",
      "Run: 06, Epoch: 56, Loss: 0.1228, Train: 99.45%, Valid: 63.65% Test: 63.82%\n",
      "Run: 06, Epoch: 57, Loss: 0.1107, Train: 99.45%, Valid: 63.65% Test: 64.25%\n",
      "Run: 06, Epoch: 58, Loss: 0.1090, Train: 99.54%, Valid: 63.51% Test: 64.47%\n",
      "Run: 06, Epoch: 59, Loss: 0.1234, Train: 99.54%, Valid: 63.65% Test: 64.69%\n",
      "Run: 06, Epoch: 60, Loss: 0.1046, Train: 99.54%, Valid: 63.37% Test: 64.47%\n",
      "Run: 06, Epoch: 61, Loss: 0.1057, Train: 99.54%, Valid: 63.10% Test: 64.04%\n",
      "Run: 06, Epoch: 62, Loss: 0.0958, Train: 99.63%, Valid: 63.10% Test: 64.47%\n",
      "Run: 06, Epoch: 63, Loss: 0.0886, Train: 99.63%, Valid: 63.37% Test: 64.04%\n",
      "Run: 06, Epoch: 64, Loss: 0.0908, Train: 99.63%, Valid: 63.37% Test: 64.04%\n",
      "Run: 06, Epoch: 65, Loss: 0.1162, Train: 99.73%, Valid: 63.10% Test: 63.82%\n",
      "Run: 06, Epoch: 66, Loss: 0.0866, Train: 99.73%, Valid: 63.10% Test: 63.82%\n",
      "Run: 06, Epoch: 67, Loss: 0.0859, Train: 99.73%, Valid: 63.24% Test: 63.38%\n",
      "Run: 06, Epoch: 68, Loss: 0.0921, Train: 99.73%, Valid: 63.51% Test: 63.16%\n",
      "Run: 06, Epoch: 69, Loss: 0.0809, Train: 99.73%, Valid: 63.24% Test: 62.94%\n",
      "Run: 06, Epoch: 70, Loss: 0.0818, Train: 99.73%, Valid: 63.10% Test: 62.94%\n",
      "Run: 06, Epoch: 71, Loss: 0.0827, Train: 99.73%, Valid: 63.24% Test: 63.38%\n",
      "Run: 06, Epoch: 72, Loss: 0.0852, Train: 99.73%, Valid: 63.10% Test: 64.04%\n",
      "Run: 06, Epoch: 73, Loss: 0.0732, Train: 99.73%, Valid: 63.51% Test: 64.04%\n",
      "Run: 06, Epoch: 74, Loss: 0.0715, Train: 99.73%, Valid: 63.51% Test: 63.82%\n",
      "Run: 06, Epoch: 75, Loss: 0.0769, Train: 99.73%, Valid: 63.51% Test: 64.25%\n",
      "Run: 06, Epoch: 76, Loss: 0.0676, Train: 99.82%, Valid: 63.65% Test: 63.82%\n",
      "Run: 06, Epoch: 77, Loss: 0.0759, Train: 99.91%, Valid: 63.92% Test: 63.82%\n",
      "Run: 06, Epoch: 78, Loss: 0.0628, Train: 99.91%, Valid: 63.79% Test: 63.82%\n",
      "Run: 06, Epoch: 79, Loss: 0.0662, Train: 99.91%, Valid: 63.79% Test: 64.04%\n",
      "Run: 06, Epoch: 80, Loss: 0.0649, Train: 99.91%, Valid: 63.51% Test: 63.60%\n",
      "Run: 06, Epoch: 81, Loss: 0.0712, Train: 99.91%, Valid: 63.37% Test: 63.60%\n",
      "Run: 06, Epoch: 82, Loss: 0.0663, Train: 99.91%, Valid: 63.37% Test: 63.38%\n",
      "Run: 06, Epoch: 83, Loss: 0.0563, Train: 99.82%, Valid: 63.24% Test: 63.82%\n",
      "Run: 06, Epoch: 84, Loss: 0.0631, Train: 99.82%, Valid: 63.24% Test: 63.60%\n",
      "Run: 06, Epoch: 85, Loss: 0.0513, Train: 99.82%, Valid: 63.24% Test: 63.82%\n",
      "Run: 06, Epoch: 86, Loss: 0.0539, Train: 99.82%, Valid: 63.24% Test: 63.16%\n",
      "Run: 06, Epoch: 87, Loss: 0.0512, Train: 99.82%, Valid: 63.24% Test: 63.82%\n",
      "Run: 06, Epoch: 88, Loss: 0.0649, Train: 99.82%, Valid: 63.24% Test: 64.04%\n",
      "Run: 06, Epoch: 89, Loss: 0.0649, Train: 99.91%, Valid: 63.65% Test: 64.04%\n",
      "Run: 06, Epoch: 90, Loss: 0.0594, Train: 99.91%, Valid: 63.51% Test: 64.04%\n",
      "Run: 06, Epoch: 91, Loss: 0.0622, Train: 99.91%, Valid: 63.51% Test: 63.82%\n",
      "Run: 06, Epoch: 92, Loss: 0.0503, Train: 99.91%, Valid: 63.65% Test: 64.04%\n",
      "Run: 06, Epoch: 93, Loss: 0.0602, Train: 99.91%, Valid: 64.06% Test: 63.38%\n",
      "Run: 06, Epoch: 94, Loss: 0.0490, Train: 99.91%, Valid: 63.51% Test: 63.38%\n",
      "Run: 06, Epoch: 95, Loss: 0.0557, Train: 99.91%, Valid: 63.92% Test: 63.16%\n",
      "Run: 06, Epoch: 96, Loss: 0.0501, Train: 99.91%, Valid: 63.65% Test: 63.16%\n",
      "Run: 06, Epoch: 97, Loss: 0.0470, Train: 99.91%, Valid: 63.65% Test: 62.94%\n",
      "Run: 06, Epoch: 98, Loss: 0.0558, Train: 99.91%, Valid: 63.92% Test: 62.94%\n",
      "Run: 06, Epoch: 99, Loss: 0.0507, Train: 99.91%, Valid: 63.79% Test: 63.16%\n",
      "Run: 06, Epoch: 100, Loss: 0.0466, Train: 99.91%, Valid: 63.51% Test: 63.60%\n",
      "Run 06:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 64.88\n",
      "  Final Train: 95.88\n",
      "   Final Test: 62.06\n",
      "Run: 07, Epoch: 01, Loss: 1.8092, Train: 39.38%, Valid: 38.82% Test: 31.36%\n",
      "Run: 07, Epoch: 02, Loss: 1.2382, Train: 42.22%, Valid: 38.55% Test: 31.80%\n",
      "Run: 07, Epoch: 03, Loss: 1.0987, Train: 48.26%, Valid: 40.60% Test: 37.72%\n",
      "Run: 07, Epoch: 04, Loss: 1.0387, Train: 53.48%, Valid: 43.76% Test: 39.47%\n",
      "Run: 07, Epoch: 05, Loss: 0.9847, Train: 58.24%, Valid: 47.87% Test: 41.89%\n",
      "Run: 07, Epoch: 06, Loss: 0.9222, Train: 64.29%, Valid: 51.58% Test: 44.96%\n",
      "Run: 07, Epoch: 07, Loss: 0.8703, Train: 68.59%, Valid: 52.40% Test: 47.59%\n",
      "Run: 07, Epoch: 08, Loss: 0.8114, Train: 73.26%, Valid: 55.14% Test: 50.00%\n",
      "Run: 07, Epoch: 09, Loss: 0.7959, Train: 76.28%, Valid: 57.06% Test: 52.63%\n",
      "Run: 07, Epoch: 10, Loss: 0.7503, Train: 77.93%, Valid: 57.75% Test: 52.63%\n",
      "Run: 07, Epoch: 11, Loss: 0.7064, Train: 80.86%, Valid: 58.85% Test: 53.29%\n",
      "Run: 07, Epoch: 12, Loss: 0.6675, Train: 83.61%, Valid: 60.22% Test: 54.17%\n",
      "Run: 07, Epoch: 13, Loss: 0.6171, Train: 84.16%, Valid: 61.18% Test: 56.14%\n",
      "Run: 07, Epoch: 14, Loss: 0.5964, Train: 85.53%, Valid: 61.18% Test: 58.55%\n",
      "Run: 07, Epoch: 15, Loss: 0.5547, Train: 87.36%, Valid: 60.63% Test: 58.55%\n",
      "Run: 07, Epoch: 16, Loss: 0.5324, Train: 88.74%, Valid: 61.04% Test: 58.99%\n",
      "Run: 07, Epoch: 17, Loss: 0.4983, Train: 89.84%, Valid: 62.00% Test: 59.87%\n",
      "Run: 07, Epoch: 18, Loss: 0.4862, Train: 91.76%, Valid: 63.37% Test: 60.96%\n",
      "Run: 07, Epoch: 19, Loss: 0.4571, Train: 92.40%, Valid: 63.79% Test: 62.06%\n",
      "Run: 07, Epoch: 20, Loss: 0.4362, Train: 93.50%, Valid: 63.37% Test: 62.50%\n",
      "Run: 07, Epoch: 21, Loss: 0.3950, Train: 94.32%, Valid: 63.51% Test: 62.28%\n",
      "Run: 07, Epoch: 22, Loss: 0.3906, Train: 94.87%, Valid: 63.65% Test: 61.84%\n",
      "Run: 07, Epoch: 23, Loss: 0.3701, Train: 94.78%, Valid: 64.33% Test: 62.28%\n",
      "Run: 07, Epoch: 24, Loss: 0.3382, Train: 95.15%, Valid: 63.65% Test: 62.28%\n",
      "Run: 07, Epoch: 25, Loss: 0.3351, Train: 95.05%, Valid: 63.79% Test: 62.06%\n",
      "Run: 07, Epoch: 26, Loss: 0.3108, Train: 95.51%, Valid: 63.65% Test: 62.50%\n",
      "Run: 07, Epoch: 27, Loss: 0.2931, Train: 96.06%, Valid: 63.65% Test: 62.06%\n",
      "Run: 07, Epoch: 28, Loss: 0.2897, Train: 96.89%, Valid: 63.79% Test: 63.16%\n",
      "Run: 07, Epoch: 29, Loss: 0.2540, Train: 97.25%, Valid: 63.51% Test: 63.38%\n",
      "Run: 07, Epoch: 30, Loss: 0.2738, Train: 97.34%, Valid: 63.37% Test: 63.82%\n",
      "Run: 07, Epoch: 31, Loss: 0.2599, Train: 97.44%, Valid: 63.65% Test: 63.82%\n",
      "Run: 07, Epoch: 32, Loss: 0.2615, Train: 97.89%, Valid: 63.79% Test: 62.94%\n",
      "Run: 07, Epoch: 33, Loss: 0.2225, Train: 97.99%, Valid: 64.20% Test: 63.16%\n",
      "Run: 07, Epoch: 34, Loss: 0.2137, Train: 98.08%, Valid: 64.47% Test: 62.50%\n",
      "Run: 07, Epoch: 35, Loss: 0.1984, Train: 98.17%, Valid: 64.47% Test: 61.62%\n",
      "Run: 07, Epoch: 36, Loss: 0.1888, Train: 98.35%, Valid: 64.33% Test: 61.40%\n",
      "Run: 07, Epoch: 37, Loss: 0.2072, Train: 98.63%, Valid: 64.61% Test: 62.50%\n",
      "Run: 07, Epoch: 38, Loss: 0.1693, Train: 98.72%, Valid: 64.75% Test: 62.94%\n",
      "Run: 07, Epoch: 39, Loss: 0.1703, Train: 98.63%, Valid: 63.92% Test: 62.06%\n",
      "Run: 07, Epoch: 40, Loss: 0.1613, Train: 98.53%, Valid: 63.79% Test: 61.84%\n",
      "Run: 07, Epoch: 41, Loss: 0.1491, Train: 98.63%, Valid: 63.65% Test: 60.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 07, Epoch: 42, Loss: 0.1515, Train: 98.72%, Valid: 63.79% Test: 60.31%\n",
      "Run: 07, Epoch: 43, Loss: 0.1623, Train: 98.63%, Valid: 63.51% Test: 60.09%\n",
      "Run: 07, Epoch: 44, Loss: 0.1648, Train: 98.72%, Valid: 63.37% Test: 61.62%\n",
      "Run: 07, Epoch: 45, Loss: 0.1294, Train: 99.08%, Valid: 63.92% Test: 61.84%\n",
      "Run: 07, Epoch: 46, Loss: 0.1275, Train: 99.08%, Valid: 63.92% Test: 61.84%\n",
      "Run: 07, Epoch: 47, Loss: 0.1484, Train: 99.08%, Valid: 64.20% Test: 62.28%\n",
      "Run: 07, Epoch: 48, Loss: 0.1236, Train: 99.08%, Valid: 64.20% Test: 62.50%\n",
      "Run: 07, Epoch: 49, Loss: 0.1292, Train: 99.18%, Valid: 63.92% Test: 62.94%\n",
      "Run: 07, Epoch: 50, Loss: 0.1040, Train: 99.27%, Valid: 63.79% Test: 62.94%\n",
      "Run: 07, Epoch: 51, Loss: 0.1149, Train: 99.36%, Valid: 63.79% Test: 62.94%\n",
      "Run: 07, Epoch: 52, Loss: 0.1225, Train: 99.36%, Valid: 63.37% Test: 63.38%\n",
      "Run: 07, Epoch: 53, Loss: 0.1065, Train: 99.54%, Valid: 63.37% Test: 62.94%\n",
      "Run: 07, Epoch: 54, Loss: 0.1025, Train: 99.54%, Valid: 63.79% Test: 62.72%\n",
      "Run: 07, Epoch: 55, Loss: 0.1106, Train: 99.54%, Valid: 64.33% Test: 62.28%\n",
      "Run: 07, Epoch: 56, Loss: 0.1104, Train: 99.45%, Valid: 64.20% Test: 62.94%\n",
      "Run: 07, Epoch: 57, Loss: 0.0897, Train: 99.63%, Valid: 64.33% Test: 62.72%\n",
      "Run: 07, Epoch: 58, Loss: 0.0990, Train: 99.63%, Valid: 64.61% Test: 62.06%\n",
      "Run: 07, Epoch: 59, Loss: 0.0753, Train: 99.73%, Valid: 65.02% Test: 62.28%\n",
      "Run: 07, Epoch: 60, Loss: 0.0778, Train: 99.73%, Valid: 65.02% Test: 62.06%\n",
      "Run: 07, Epoch: 61, Loss: 0.0861, Train: 99.73%, Valid: 64.61% Test: 62.06%\n",
      "Run: 07, Epoch: 62, Loss: 0.0751, Train: 99.73%, Valid: 64.47% Test: 62.06%\n",
      "Run: 07, Epoch: 63, Loss: 0.0815, Train: 99.63%, Valid: 63.92% Test: 62.28%\n",
      "Run: 07, Epoch: 64, Loss: 0.0738, Train: 99.63%, Valid: 63.79% Test: 62.72%\n",
      "Run: 07, Epoch: 65, Loss: 0.0734, Train: 99.73%, Valid: 64.06% Test: 62.94%\n",
      "Run: 07, Epoch: 66, Loss: 0.0710, Train: 99.73%, Valid: 64.20% Test: 62.72%\n",
      "Run: 07, Epoch: 67, Loss: 0.0641, Train: 99.73%, Valid: 64.06% Test: 62.28%\n",
      "Run: 07, Epoch: 68, Loss: 0.0871, Train: 99.82%, Valid: 64.33% Test: 63.16%\n",
      "Run: 07, Epoch: 69, Loss: 0.0880, Train: 99.82%, Valid: 64.20% Test: 63.16%\n",
      "Run: 07, Epoch: 70, Loss: 0.0620, Train: 99.82%, Valid: 63.65% Test: 63.16%\n",
      "Run: 07, Epoch: 71, Loss: 0.0690, Train: 99.73%, Valid: 63.65% Test: 63.60%\n",
      "Run: 07, Epoch: 72, Loss: 0.0627, Train: 99.73%, Valid: 63.51% Test: 63.38%\n",
      "Run: 07, Epoch: 73, Loss: 0.0656, Train: 99.73%, Valid: 63.65% Test: 63.82%\n",
      "Run: 07, Epoch: 74, Loss: 0.0667, Train: 99.82%, Valid: 63.65% Test: 63.82%\n",
      "Run: 07, Epoch: 75, Loss: 0.0622, Train: 99.82%, Valid: 64.06% Test: 63.82%\n",
      "Run: 07, Epoch: 76, Loss: 0.0629, Train: 99.91%, Valid: 63.79% Test: 64.25%\n",
      "Run: 07, Epoch: 77, Loss: 0.0528, Train: 99.82%, Valid: 63.92% Test: 64.47%\n",
      "Run: 07, Epoch: 78, Loss: 0.0584, Train: 99.82%, Valid: 63.51% Test: 64.69%\n",
      "Run: 07, Epoch: 79, Loss: 0.0615, Train: 99.82%, Valid: 62.96% Test: 64.69%\n",
      "Run: 07, Epoch: 80, Loss: 0.0585, Train: 99.82%, Valid: 63.24% Test: 64.25%\n",
      "Run: 07, Epoch: 81, Loss: 0.0656, Train: 99.91%, Valid: 63.24% Test: 64.47%\n",
      "Run: 07, Epoch: 82, Loss: 0.0598, Train: 99.91%, Valid: 63.65% Test: 64.04%\n",
      "Run: 07, Epoch: 83, Loss: 0.0526, Train: 99.91%, Valid: 63.37% Test: 64.04%\n",
      "Run: 07, Epoch: 84, Loss: 0.0406, Train: 99.91%, Valid: 63.51% Test: 64.25%\n",
      "Run: 07, Epoch: 85, Loss: 0.0464, Train: 99.91%, Valid: 63.65% Test: 64.25%\n",
      "Run: 07, Epoch: 86, Loss: 0.0505, Train: 99.91%, Valid: 63.51% Test: 64.04%\n",
      "Run: 07, Epoch: 87, Loss: 0.0578, Train: 99.91%, Valid: 64.20% Test: 63.82%\n",
      "Run: 07, Epoch: 88, Loss: 0.0393, Train: 99.91%, Valid: 64.75% Test: 63.38%\n",
      "Run: 07, Epoch: 89, Loss: 0.0550, Train: 100.00%, Valid: 64.61% Test: 63.82%\n",
      "Run: 07, Epoch: 90, Loss: 0.0446, Train: 100.00%, Valid: 64.61% Test: 64.04%\n",
      "Run: 07, Epoch: 91, Loss: 0.0500, Train: 100.00%, Valid: 64.47% Test: 63.60%\n",
      "Run: 07, Epoch: 92, Loss: 0.0391, Train: 100.00%, Valid: 64.33% Test: 63.38%\n",
      "Run: 07, Epoch: 93, Loss: 0.0465, Train: 100.00%, Valid: 63.65% Test: 63.60%\n",
      "Run: 07, Epoch: 94, Loss: 0.0502, Train: 100.00%, Valid: 63.92% Test: 63.38%\n",
      "Run: 07, Epoch: 95, Loss: 0.0397, Train: 100.00%, Valid: 63.65% Test: 63.60%\n",
      "Run: 07, Epoch: 96, Loss: 0.0437, Train: 100.00%, Valid: 63.65% Test: 63.16%\n",
      "Run: 07, Epoch: 97, Loss: 0.0423, Train: 100.00%, Valid: 63.51% Test: 63.16%\n",
      "Run: 07, Epoch: 98, Loss: 0.0418, Train: 100.00%, Valid: 63.24% Test: 63.60%\n",
      "Run: 07, Epoch: 99, Loss: 0.0398, Train: 100.00%, Valid: 63.65% Test: 63.60%\n",
      "Run: 07, Epoch: 100, Loss: 0.0405, Train: 100.00%, Valid: 63.24% Test: 63.38%\n",
      "Run 07:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 65.02\n",
      "  Final Train: 99.73\n",
      "   Final Test: 62.28\n",
      "Run: 08, Epoch: 01, Loss: 1.7866, Train: 34.71%, Valid: 28.53% Test: 24.78%\n",
      "Run: 08, Epoch: 02, Loss: 1.2618, Train: 49.36%, Valid: 37.04% Test: 32.24%\n",
      "Run: 08, Epoch: 03, Loss: 1.1047, Train: 55.31%, Valid: 41.43% Test: 37.28%\n",
      "Run: 08, Epoch: 04, Loss: 1.0507, Train: 60.71%, Valid: 44.31% Test: 41.45%\n",
      "Run: 08, Epoch: 05, Loss: 1.0234, Train: 64.93%, Valid: 47.19% Test: 45.61%\n",
      "Run: 08, Epoch: 06, Loss: 0.9139, Train: 68.59%, Valid: 50.62% Test: 48.46%\n",
      "Run: 08, Epoch: 07, Loss: 0.8789, Train: 71.79%, Valid: 52.26% Test: 50.00%\n",
      "Run: 08, Epoch: 08, Loss: 0.8339, Train: 74.27%, Valid: 52.95% Test: 49.56%\n",
      "Run: 08, Epoch: 09, Loss: 0.7798, Train: 76.56%, Valid: 53.22% Test: 49.56%\n",
      "Run: 08, Epoch: 10, Loss: 0.7583, Train: 78.39%, Valid: 54.46% Test: 49.78%\n",
      "Run: 08, Epoch: 11, Loss: 0.7148, Train: 79.40%, Valid: 54.46% Test: 49.78%\n",
      "Run: 08, Epoch: 12, Loss: 0.6586, Train: 80.49%, Valid: 55.56% Test: 51.10%\n",
      "Run: 08, Epoch: 13, Loss: 0.6460, Train: 82.42%, Valid: 55.83% Test: 51.75%\n",
      "Run: 08, Epoch: 14, Loss: 0.6092, Train: 85.16%, Valid: 56.52% Test: 53.51%\n",
      "Run: 08, Epoch: 15, Loss: 0.5693, Train: 86.26%, Valid: 57.61% Test: 53.95%\n",
      "Run: 08, Epoch: 16, Loss: 0.5676, Train: 87.36%, Valid: 58.44% Test: 56.80%\n",
      "Run: 08, Epoch: 17, Loss: 0.5223, Train: 88.37%, Valid: 59.40% Test: 56.58%\n",
      "Run: 08, Epoch: 18, Loss: 0.4887, Train: 89.10%, Valid: 59.67% Test: 57.02%\n",
      "Run: 08, Epoch: 19, Loss: 0.4999, Train: 89.84%, Valid: 59.53% Test: 57.24%\n",
      "Run: 08, Epoch: 20, Loss: 0.4475, Train: 90.02%, Valid: 60.49% Test: 57.89%\n",
      "Run: 08, Epoch: 21, Loss: 0.4203, Train: 90.93%, Valid: 60.91% Test: 58.55%\n",
      "Run: 08, Epoch: 22, Loss: 0.4063, Train: 91.67%, Valid: 60.63% Test: 58.55%\n",
      "Run: 08, Epoch: 23, Loss: 0.4089, Train: 92.03%, Valid: 60.91% Test: 58.77%\n",
      "Run: 08, Epoch: 24, Loss: 0.3854, Train: 93.22%, Valid: 61.73% Test: 58.99%\n",
      "Run: 08, Epoch: 25, Loss: 0.3502, Train: 93.77%, Valid: 61.87% Test: 59.43%\n",
      "Run: 08, Epoch: 26, Loss: 0.3238, Train: 94.41%, Valid: 62.28% Test: 58.77%\n",
      "Run: 08, Epoch: 27, Loss: 0.3182, Train: 94.78%, Valid: 62.69% Test: 58.55%\n",
      "Run: 08, Epoch: 28, Loss: 0.3041, Train: 95.24%, Valid: 62.41% Test: 57.89%\n",
      "Run: 08, Epoch: 29, Loss: 0.2930, Train: 95.70%, Valid: 62.28% Test: 58.33%\n",
      "Run: 08, Epoch: 30, Loss: 0.2859, Train: 96.34%, Valid: 62.28% Test: 57.89%\n",
      "Run: 08, Epoch: 31, Loss: 0.2705, Train: 96.52%, Valid: 62.96% Test: 58.33%\n",
      "Run: 08, Epoch: 32, Loss: 0.2455, Train: 96.79%, Valid: 63.10% Test: 58.33%\n",
      "Run: 08, Epoch: 33, Loss: 0.2309, Train: 96.98%, Valid: 62.83% Test: 58.11%\n",
      "Run: 08, Epoch: 34, Loss: 0.2289, Train: 97.44%, Valid: 63.51% Test: 58.11%\n",
      "Run: 08, Epoch: 35, Loss: 0.2279, Train: 97.62%, Valid: 63.79% Test: 58.11%\n",
      "Run: 08, Epoch: 36, Loss: 0.2083, Train: 97.62%, Valid: 64.20% Test: 58.99%\n",
      "Run: 08, Epoch: 37, Loss: 0.2129, Train: 97.71%, Valid: 64.20% Test: 59.43%\n",
      "Run: 08, Epoch: 38, Loss: 0.1971, Train: 98.08%, Valid: 64.75% Test: 59.65%\n",
      "Run: 08, Epoch: 39, Loss: 0.1908, Train: 98.35%, Valid: 64.47% Test: 60.09%\n",
      "Run: 08, Epoch: 40, Loss: 0.1946, Train: 98.35%, Valid: 64.06% Test: 59.65%\n",
      "Run: 08, Epoch: 41, Loss: 0.1908, Train: 98.44%, Valid: 64.06% Test: 60.09%\n",
      "Run: 08, Epoch: 42, Loss: 0.1552, Train: 98.44%, Valid: 63.79% Test: 60.31%\n",
      "Run: 08, Epoch: 43, Loss: 0.1552, Train: 98.72%, Valid: 63.51% Test: 60.53%\n",
      "Run: 08, Epoch: 44, Loss: 0.1593, Train: 98.72%, Valid: 64.33% Test: 61.18%\n",
      "Run: 08, Epoch: 45, Loss: 0.1424, Train: 98.72%, Valid: 64.06% Test: 61.84%\n",
      "Run: 08, Epoch: 46, Loss: 0.1633, Train: 98.81%, Valid: 64.20% Test: 61.62%\n",
      "Run: 08, Epoch: 47, Loss: 0.1380, Train: 98.81%, Valid: 63.92% Test: 61.40%\n",
      "Run: 08, Epoch: 48, Loss: 0.1467, Train: 99.08%, Valid: 64.06% Test: 61.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 08, Epoch: 49, Loss: 0.1320, Train: 99.18%, Valid: 63.92% Test: 61.40%\n",
      "Run: 08, Epoch: 50, Loss: 0.1188, Train: 99.36%, Valid: 63.51% Test: 60.96%\n",
      "Run: 08, Epoch: 51, Loss: 0.1267, Train: 99.36%, Valid: 63.24% Test: 60.53%\n",
      "Run: 08, Epoch: 52, Loss: 0.1268, Train: 99.45%, Valid: 63.10% Test: 60.96%\n",
      "Run: 08, Epoch: 53, Loss: 0.1068, Train: 99.45%, Valid: 62.41% Test: 60.96%\n",
      "Run: 08, Epoch: 54, Loss: 0.1084, Train: 99.63%, Valid: 62.55% Test: 61.40%\n",
      "Run: 08, Epoch: 55, Loss: 0.1136, Train: 99.63%, Valid: 62.55% Test: 61.18%\n",
      "Run: 08, Epoch: 56, Loss: 0.0972, Train: 99.45%, Valid: 62.83% Test: 61.84%\n",
      "Run: 08, Epoch: 57, Loss: 0.1002, Train: 99.54%, Valid: 62.96% Test: 61.40%\n",
      "Run: 08, Epoch: 58, Loss: 0.1064, Train: 99.54%, Valid: 62.83% Test: 60.96%\n",
      "Run: 08, Epoch: 59, Loss: 0.0920, Train: 99.54%, Valid: 62.00% Test: 60.31%\n",
      "Run: 08, Epoch: 60, Loss: 0.0927, Train: 99.54%, Valid: 61.73% Test: 60.53%\n",
      "Run: 08, Epoch: 61, Loss: 0.0903, Train: 99.63%, Valid: 61.87% Test: 60.31%\n",
      "Run: 08, Epoch: 62, Loss: 0.0948, Train: 99.63%, Valid: 61.87% Test: 60.96%\n",
      "Run: 08, Epoch: 63, Loss: 0.1002, Train: 99.73%, Valid: 62.00% Test: 60.53%\n",
      "Run: 08, Epoch: 64, Loss: 0.0758, Train: 99.73%, Valid: 61.87% Test: 60.75%\n",
      "Run: 08, Epoch: 65, Loss: 0.0924, Train: 99.73%, Valid: 61.45% Test: 60.75%\n",
      "Run: 08, Epoch: 66, Loss: 0.0984, Train: 99.73%, Valid: 62.00% Test: 60.96%\n",
      "Run: 08, Epoch: 67, Loss: 0.0806, Train: 99.73%, Valid: 62.00% Test: 60.96%\n",
      "Run: 08, Epoch: 68, Loss: 0.0772, Train: 99.82%, Valid: 61.59% Test: 61.40%\n",
      "Run: 08, Epoch: 69, Loss: 0.0676, Train: 99.91%, Valid: 61.59% Test: 61.18%\n",
      "Run: 08, Epoch: 70, Loss: 0.0694, Train: 99.73%, Valid: 61.45% Test: 61.40%\n",
      "Run: 08, Epoch: 71, Loss: 0.0729, Train: 99.73%, Valid: 61.45% Test: 61.18%\n",
      "Run: 08, Epoch: 72, Loss: 0.0722, Train: 99.73%, Valid: 61.73% Test: 60.96%\n",
      "Run: 08, Epoch: 73, Loss: 0.0630, Train: 99.73%, Valid: 61.73% Test: 60.53%\n",
      "Run: 08, Epoch: 74, Loss: 0.0779, Train: 99.73%, Valid: 61.87% Test: 60.31%\n",
      "Run: 08, Epoch: 75, Loss: 0.0618, Train: 99.73%, Valid: 62.28% Test: 60.53%\n",
      "Run: 08, Epoch: 76, Loss: 0.0523, Train: 99.82%, Valid: 62.14% Test: 60.96%\n",
      "Run: 08, Epoch: 77, Loss: 0.0643, Train: 99.82%, Valid: 62.00% Test: 60.75%\n",
      "Run: 08, Epoch: 78, Loss: 0.0664, Train: 99.91%, Valid: 61.45% Test: 60.96%\n",
      "Run: 08, Epoch: 79, Loss: 0.0579, Train: 99.91%, Valid: 61.59% Test: 60.96%\n",
      "Run: 08, Epoch: 80, Loss: 0.0606, Train: 99.91%, Valid: 61.87% Test: 61.40%\n",
      "Run: 08, Epoch: 81, Loss: 0.0551, Train: 99.91%, Valid: 61.87% Test: 61.62%\n",
      "Run: 08, Epoch: 82, Loss: 0.0616, Train: 99.91%, Valid: 61.87% Test: 60.75%\n",
      "Run: 08, Epoch: 83, Loss: 0.0625, Train: 99.91%, Valid: 61.32% Test: 60.53%\n",
      "Run: 08, Epoch: 84, Loss: 0.0599, Train: 99.91%, Valid: 61.45% Test: 60.75%\n",
      "Run: 08, Epoch: 85, Loss: 0.0646, Train: 99.91%, Valid: 61.59% Test: 60.53%\n",
      "Run: 08, Epoch: 86, Loss: 0.0485, Train: 99.91%, Valid: 62.14% Test: 60.53%\n",
      "Run: 08, Epoch: 87, Loss: 0.0558, Train: 99.91%, Valid: 62.14% Test: 60.09%\n",
      "Run: 08, Epoch: 88, Loss: 0.0450, Train: 99.91%, Valid: 61.87% Test: 60.31%\n",
      "Run: 08, Epoch: 89, Loss: 0.0501, Train: 99.91%, Valid: 61.87% Test: 60.75%\n",
      "Run: 08, Epoch: 90, Loss: 0.0518, Train: 99.91%, Valid: 62.14% Test: 60.75%\n",
      "Run: 08, Epoch: 91, Loss: 0.0447, Train: 99.91%, Valid: 61.87% Test: 60.75%\n",
      "Run: 08, Epoch: 92, Loss: 0.0505, Train: 99.91%, Valid: 62.14% Test: 60.96%\n",
      "Run: 08, Epoch: 93, Loss: 0.0383, Train: 99.91%, Valid: 61.73% Test: 61.40%\n",
      "Run: 08, Epoch: 94, Loss: 0.0580, Train: 99.91%, Valid: 61.73% Test: 61.62%\n",
      "Run: 08, Epoch: 95, Loss: 0.0418, Train: 99.91%, Valid: 61.59% Test: 61.84%\n",
      "Run: 08, Epoch: 96, Loss: 0.0512, Train: 99.91%, Valid: 61.87% Test: 61.62%\n",
      "Run: 08, Epoch: 97, Loss: 0.0422, Train: 99.91%, Valid: 62.28% Test: 61.84%\n",
      "Run: 08, Epoch: 98, Loss: 0.0384, Train: 99.91%, Valid: 61.87% Test: 61.62%\n",
      "Run: 08, Epoch: 99, Loss: 0.0490, Train: 99.91%, Valid: 61.59% Test: 61.40%\n",
      "Run: 08, Epoch: 100, Loss: 0.0469, Train: 99.91%, Valid: 61.59% Test: 61.62%\n",
      "Run 08:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 64.75\n",
      "  Final Train: 98.08\n",
      "   Final Test: 59.65\n",
      "Run: 09, Epoch: 01, Loss: 1.7836, Train: 41.58%, Valid: 38.55% Test: 39.91%\n",
      "Run: 09, Epoch: 02, Loss: 1.2461, Train: 48.17%, Valid: 39.37% Test: 43.86%\n",
      "Run: 09, Epoch: 03, Loss: 1.1029, Train: 54.30%, Valid: 42.94% Test: 46.93%\n",
      "Run: 09, Epoch: 04, Loss: 1.0474, Train: 61.90%, Valid: 46.09% Test: 49.12%\n",
      "Run: 09, Epoch: 05, Loss: 0.9845, Train: 67.58%, Valid: 51.99% Test: 52.85%\n",
      "Run: 09, Epoch: 06, Loss: 0.9130, Train: 70.42%, Valid: 53.91% Test: 53.29%\n",
      "Run: 09, Epoch: 07, Loss: 0.8568, Train: 72.44%, Valid: 54.32% Test: 54.39%\n",
      "Run: 09, Epoch: 08, Loss: 0.8221, Train: 73.90%, Valid: 54.73% Test: 54.17%\n",
      "Run: 09, Epoch: 09, Loss: 0.7779, Train: 76.37%, Valid: 55.97% Test: 56.14%\n",
      "Run: 09, Epoch: 10, Loss: 0.7425, Train: 79.95%, Valid: 57.34% Test: 56.14%\n",
      "Run: 09, Epoch: 11, Loss: 0.7170, Train: 81.96%, Valid: 58.85% Test: 57.89%\n",
      "Run: 09, Epoch: 12, Loss: 0.6533, Train: 83.97%, Valid: 58.71% Test: 59.21%\n",
      "Run: 09, Epoch: 13, Loss: 0.6174, Train: 87.00%, Valid: 59.53% Test: 60.75%\n",
      "Run: 09, Epoch: 14, Loss: 0.5792, Train: 89.10%, Valid: 59.67% Test: 60.96%\n",
      "Run: 09, Epoch: 15, Loss: 0.5401, Train: 90.84%, Valid: 61.32% Test: 61.62%\n",
      "Run: 09, Epoch: 16, Loss: 0.5282, Train: 90.93%, Valid: 60.36% Test: 61.18%\n",
      "Run: 09, Epoch: 17, Loss: 0.4829, Train: 91.48%, Valid: 61.32% Test: 61.62%\n",
      "Run: 09, Epoch: 18, Loss: 0.4774, Train: 93.13%, Valid: 61.87% Test: 61.84%\n",
      "Run: 09, Epoch: 19, Loss: 0.4542, Train: 93.96%, Valid: 62.41% Test: 62.28%\n",
      "Run: 09, Epoch: 20, Loss: 0.4248, Train: 93.86%, Valid: 62.28% Test: 62.06%\n",
      "Run: 09, Epoch: 21, Loss: 0.4097, Train: 94.23%, Valid: 62.14% Test: 62.94%\n",
      "Run: 09, Epoch: 22, Loss: 0.3751, Train: 94.78%, Valid: 62.55% Test: 63.16%\n",
      "Run: 09, Epoch: 23, Loss: 0.3547, Train: 95.33%, Valid: 62.28% Test: 63.16%\n",
      "Run: 09, Epoch: 24, Loss: 0.3365, Train: 95.70%, Valid: 62.69% Test: 63.16%\n",
      "Run: 09, Epoch: 25, Loss: 0.3200, Train: 96.34%, Valid: 63.37% Test: 63.38%\n",
      "Run: 09, Epoch: 26, Loss: 0.2939, Train: 96.79%, Valid: 63.92% Test: 64.04%\n",
      "Run: 09, Epoch: 27, Loss: 0.2672, Train: 97.16%, Valid: 64.61% Test: 63.82%\n",
      "Run: 09, Epoch: 28, Loss: 0.2758, Train: 97.53%, Valid: 64.20% Test: 63.38%\n",
      "Run: 09, Epoch: 29, Loss: 0.2793, Train: 97.80%, Valid: 64.33% Test: 63.38%\n",
      "Run: 09, Epoch: 30, Loss: 0.2317, Train: 97.80%, Valid: 64.20% Test: 63.60%\n",
      "Run: 09, Epoch: 31, Loss: 0.2301, Train: 98.17%, Valid: 64.06% Test: 62.94%\n",
      "Run: 09, Epoch: 32, Loss: 0.2228, Train: 98.17%, Valid: 64.61% Test: 63.38%\n",
      "Run: 09, Epoch: 33, Loss: 0.2105, Train: 98.08%, Valid: 64.33% Test: 63.60%\n",
      "Run: 09, Epoch: 34, Loss: 0.2079, Train: 98.08%, Valid: 63.79% Test: 63.38%\n",
      "Run: 09, Epoch: 35, Loss: 0.1933, Train: 98.08%, Valid: 63.92% Test: 62.06%\n",
      "Run: 09, Epoch: 36, Loss: 0.1819, Train: 98.26%, Valid: 63.37% Test: 62.28%\n",
      "Run: 09, Epoch: 37, Loss: 0.1697, Train: 98.53%, Valid: 63.37% Test: 62.06%\n",
      "Run: 09, Epoch: 38, Loss: 0.1800, Train: 98.63%, Valid: 63.37% Test: 61.62%\n",
      "Run: 09, Epoch: 39, Loss: 0.1640, Train: 98.81%, Valid: 63.79% Test: 61.62%\n",
      "Run: 09, Epoch: 40, Loss: 0.1300, Train: 98.90%, Valid: 63.65% Test: 62.06%\n",
      "Run: 09, Epoch: 41, Loss: 0.1699, Train: 99.08%, Valid: 63.92% Test: 61.84%\n",
      "Run: 09, Epoch: 42, Loss: 0.1384, Train: 99.36%, Valid: 64.20% Test: 61.40%\n",
      "Run: 09, Epoch: 43, Loss: 0.1357, Train: 99.36%, Valid: 63.92% Test: 61.40%\n",
      "Run: 09, Epoch: 44, Loss: 0.1508, Train: 99.45%, Valid: 63.51% Test: 61.40%\n",
      "Run: 09, Epoch: 45, Loss: 0.1251, Train: 99.36%, Valid: 63.24% Test: 61.18%\n",
      "Run: 09, Epoch: 46, Loss: 0.1338, Train: 99.27%, Valid: 63.37% Test: 60.96%\n",
      "Run: 09, Epoch: 47, Loss: 0.1302, Train: 99.36%, Valid: 63.10% Test: 60.75%\n",
      "Run: 09, Epoch: 48, Loss: 0.1054, Train: 99.54%, Valid: 63.10% Test: 61.18%\n",
      "Run: 09, Epoch: 49, Loss: 0.0951, Train: 99.45%, Valid: 63.24% Test: 61.18%\n",
      "Run: 09, Epoch: 50, Loss: 0.0959, Train: 99.63%, Valid: 62.96% Test: 61.18%\n",
      "Run: 09, Epoch: 51, Loss: 0.1004, Train: 99.63%, Valid: 62.69% Test: 60.96%\n",
      "Run: 09, Epoch: 52, Loss: 0.1074, Train: 99.73%, Valid: 62.55% Test: 60.75%\n",
      "Run: 09, Epoch: 53, Loss: 0.1075, Train: 99.73%, Valid: 62.69% Test: 60.96%\n",
      "Run: 09, Epoch: 54, Loss: 0.1078, Train: 99.73%, Valid: 62.69% Test: 60.31%\n",
      "Run: 09, Epoch: 55, Loss: 0.0972, Train: 99.73%, Valid: 62.55% Test: 60.53%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 09, Epoch: 56, Loss: 0.0940, Train: 99.73%, Valid: 62.55% Test: 60.75%\n",
      "Run: 09, Epoch: 57, Loss: 0.0908, Train: 99.73%, Valid: 62.41% Test: 60.75%\n",
      "Run: 09, Epoch: 58, Loss: 0.0836, Train: 99.73%, Valid: 62.69% Test: 60.75%\n",
      "Run: 09, Epoch: 59, Loss: 0.0978, Train: 99.73%, Valid: 63.37% Test: 60.75%\n",
      "Run: 09, Epoch: 60, Loss: 0.0783, Train: 99.73%, Valid: 62.83% Test: 60.96%\n",
      "Run: 09, Epoch: 61, Loss: 0.0908, Train: 99.73%, Valid: 63.37% Test: 60.31%\n",
      "Run: 09, Epoch: 62, Loss: 0.0796, Train: 99.73%, Valid: 63.37% Test: 60.31%\n",
      "Run: 09, Epoch: 63, Loss: 0.0597, Train: 99.63%, Valid: 63.92% Test: 60.75%\n",
      "Run: 09, Epoch: 64, Loss: 0.0858, Train: 99.63%, Valid: 64.20% Test: 60.96%\n",
      "Run: 09, Epoch: 65, Loss: 0.0814, Train: 99.63%, Valid: 64.20% Test: 60.53%\n",
      "Run: 09, Epoch: 66, Loss: 0.0705, Train: 99.63%, Valid: 64.47% Test: 60.96%\n",
      "Run: 09, Epoch: 67, Loss: 0.0669, Train: 99.63%, Valid: 63.92% Test: 61.40%\n",
      "Run: 09, Epoch: 68, Loss: 0.0695, Train: 99.63%, Valid: 64.33% Test: 61.40%\n",
      "Run: 09, Epoch: 69, Loss: 0.0745, Train: 99.63%, Valid: 63.92% Test: 61.40%\n",
      "Run: 09, Epoch: 70, Loss: 0.0713, Train: 99.63%, Valid: 63.24% Test: 61.40%\n",
      "Run: 09, Epoch: 71, Loss: 0.0758, Train: 99.63%, Valid: 64.33% Test: 61.18%\n",
      "Run: 09, Epoch: 72, Loss: 0.0568, Train: 99.63%, Valid: 63.65% Test: 61.18%\n",
      "Run: 09, Epoch: 73, Loss: 0.0731, Train: 99.73%, Valid: 63.65% Test: 61.40%\n",
      "Run: 09, Epoch: 74, Loss: 0.0685, Train: 99.73%, Valid: 62.96% Test: 61.18%\n",
      "Run: 09, Epoch: 75, Loss: 0.0764, Train: 99.73%, Valid: 63.10% Test: 61.18%\n",
      "Run: 09, Epoch: 76, Loss: 0.0721, Train: 99.73%, Valid: 63.37% Test: 61.62%\n",
      "Run: 09, Epoch: 77, Loss: 0.0612, Train: 99.73%, Valid: 63.51% Test: 61.84%\n",
      "Run: 09, Epoch: 78, Loss: 0.0584, Train: 99.73%, Valid: 62.69% Test: 61.40%\n",
      "Run: 09, Epoch: 79, Loss: 0.0649, Train: 99.73%, Valid: 62.83% Test: 61.84%\n",
      "Run: 09, Epoch: 80, Loss: 0.0458, Train: 99.73%, Valid: 63.24% Test: 61.40%\n",
      "Run: 09, Epoch: 81, Loss: 0.0612, Train: 99.73%, Valid: 63.65% Test: 61.40%\n",
      "Run: 09, Epoch: 82, Loss: 0.0542, Train: 99.73%, Valid: 63.24% Test: 61.84%\n",
      "Run: 09, Epoch: 83, Loss: 0.0559, Train: 99.73%, Valid: 63.37% Test: 61.84%\n",
      "Run: 09, Epoch: 84, Loss: 0.0480, Train: 99.73%, Valid: 63.51% Test: 61.84%\n",
      "Run: 09, Epoch: 85, Loss: 0.0655, Train: 99.73%, Valid: 63.51% Test: 61.84%\n",
      "Run: 09, Epoch: 86, Loss: 0.0559, Train: 99.73%, Valid: 63.37% Test: 62.06%\n",
      "Run: 09, Epoch: 87, Loss: 0.0532, Train: 99.73%, Valid: 62.55% Test: 62.06%\n",
      "Run: 09, Epoch: 88, Loss: 0.0591, Train: 99.73%, Valid: 62.83% Test: 62.50%\n",
      "Run: 09, Epoch: 89, Loss: 0.0537, Train: 99.73%, Valid: 62.41% Test: 62.06%\n",
      "Run: 09, Epoch: 90, Loss: 0.0603, Train: 99.82%, Valid: 62.28% Test: 62.28%\n",
      "Run: 09, Epoch: 91, Loss: 0.0502, Train: 99.82%, Valid: 62.28% Test: 62.06%\n",
      "Run: 09, Epoch: 92, Loss: 0.0529, Train: 99.82%, Valid: 62.41% Test: 61.62%\n",
      "Run: 09, Epoch: 93, Loss: 0.0556, Train: 99.82%, Valid: 62.96% Test: 61.62%\n",
      "Run: 09, Epoch: 94, Loss: 0.0478, Train: 99.82%, Valid: 62.96% Test: 61.84%\n",
      "Run: 09, Epoch: 95, Loss: 0.0448, Train: 99.82%, Valid: 62.96% Test: 61.84%\n",
      "Run: 09, Epoch: 96, Loss: 0.0436, Train: 99.82%, Valid: 63.10% Test: 61.62%\n",
      "Run: 09, Epoch: 97, Loss: 0.0556, Train: 99.82%, Valid: 61.87% Test: 61.18%\n",
      "Run: 09, Epoch: 98, Loss: 0.0512, Train: 99.82%, Valid: 61.73% Test: 61.18%\n",
      "Run: 09, Epoch: 99, Loss: 0.0648, Train: 99.82%, Valid: 62.00% Test: 60.75%\n",
      "Run: 09, Epoch: 100, Loss: 0.0420, Train: 99.82%, Valid: 61.73% Test: 60.75%\n",
      "Run 09:\n",
      "Highest Train: 99.82\n",
      "Highest Valid: 64.61\n",
      "  Final Train: 97.16\n",
      "   Final Test: 63.82\n",
      "Run: 10, Epoch: 01, Loss: 1.8398, Train: 34.07%, Valid: 32.37% Test: 34.65%\n",
      "Run: 10, Epoch: 02, Loss: 1.2618, Train: 43.50%, Valid: 38.55% Test: 36.84%\n",
      "Run: 10, Epoch: 03, Loss: 1.1221, Train: 46.61%, Valid: 39.64% Test: 40.35%\n",
      "Run: 10, Epoch: 04, Loss: 1.0500, Train: 52.93%, Valid: 42.25% Test: 44.74%\n",
      "Run: 10, Epoch: 05, Loss: 0.9619, Train: 58.24%, Valid: 46.91% Test: 46.49%\n",
      "Run: 10, Epoch: 06, Loss: 0.9061, Train: 65.02%, Valid: 49.38% Test: 49.78%\n",
      "Run: 10, Epoch: 07, Loss: 0.8496, Train: 70.51%, Valid: 51.58% Test: 52.85%\n",
      "Run: 10, Epoch: 08, Loss: 0.8174, Train: 74.91%, Valid: 52.95% Test: 54.61%\n",
      "Run: 10, Epoch: 09, Loss: 0.7780, Train: 78.75%, Valid: 55.01% Test: 57.02%\n",
      "Run: 10, Epoch: 10, Loss: 0.7178, Train: 81.87%, Valid: 56.38% Test: 57.24%\n",
      "Run: 10, Epoch: 11, Loss: 0.6575, Train: 83.79%, Valid: 56.79% Test: 57.89%\n",
      "Run: 10, Epoch: 12, Loss: 0.6496, Train: 84.43%, Valid: 56.79% Test: 58.33%\n",
      "Run: 10, Epoch: 13, Loss: 0.5955, Train: 85.26%, Valid: 58.02% Test: 60.09%\n",
      "Run: 10, Epoch: 14, Loss: 0.5487, Train: 86.54%, Valid: 58.30% Test: 59.43%\n",
      "Run: 10, Epoch: 15, Loss: 0.5257, Train: 88.37%, Valid: 59.12% Test: 59.65%\n",
      "Run: 10, Epoch: 16, Loss: 0.4734, Train: 90.57%, Valid: 59.81% Test: 59.65%\n",
      "Run: 10, Epoch: 17, Loss: 0.4811, Train: 92.77%, Valid: 59.53% Test: 62.28%\n",
      "Run: 10, Epoch: 18, Loss: 0.4275, Train: 93.86%, Valid: 60.49% Test: 63.38%\n",
      "Run: 10, Epoch: 19, Loss: 0.4137, Train: 94.51%, Valid: 60.36% Test: 62.28%\n",
      "Run: 10, Epoch: 20, Loss: 0.3899, Train: 95.15%, Valid: 60.08% Test: 63.60%\n",
      "Run: 10, Epoch: 21, Loss: 0.3565, Train: 95.51%, Valid: 60.63% Test: 63.16%\n",
      "Run: 10, Epoch: 22, Loss: 0.3552, Train: 95.88%, Valid: 61.04% Test: 63.16%\n",
      "Run: 10, Epoch: 23, Loss: 0.3237, Train: 96.70%, Valid: 60.91% Test: 62.94%\n",
      "Run: 10, Epoch: 24, Loss: 0.3000, Train: 96.98%, Valid: 60.91% Test: 62.06%\n",
      "Run: 10, Epoch: 25, Loss: 0.2819, Train: 97.34%, Valid: 60.77% Test: 61.84%\n",
      "Run: 10, Epoch: 26, Loss: 0.2700, Train: 97.53%, Valid: 61.59% Test: 61.84%\n",
      "Run: 10, Epoch: 27, Loss: 0.2661, Train: 97.89%, Valid: 62.41% Test: 62.06%\n",
      "Run: 10, Epoch: 28, Loss: 0.2429, Train: 98.35%, Valid: 62.00% Test: 62.28%\n",
      "Run: 10, Epoch: 29, Loss: 0.2274, Train: 98.63%, Valid: 62.14% Test: 61.40%\n",
      "Run: 10, Epoch: 30, Loss: 0.2385, Train: 98.81%, Valid: 61.73% Test: 61.40%\n",
      "Run: 10, Epoch: 31, Loss: 0.1931, Train: 98.72%, Valid: 62.55% Test: 60.75%\n",
      "Run: 10, Epoch: 32, Loss: 0.1922, Train: 98.72%, Valid: 61.87% Test: 61.62%\n",
      "Run: 10, Epoch: 33, Loss: 0.1965, Train: 98.99%, Valid: 62.14% Test: 61.62%\n",
      "Run: 10, Epoch: 34, Loss: 0.1845, Train: 99.08%, Valid: 62.55% Test: 61.18%\n",
      "Run: 10, Epoch: 35, Loss: 0.1716, Train: 99.08%, Valid: 61.87% Test: 60.75%\n",
      "Run: 10, Epoch: 36, Loss: 0.1560, Train: 99.18%, Valid: 61.32% Test: 60.53%\n",
      "Run: 10, Epoch: 37, Loss: 0.1617, Train: 99.18%, Valid: 60.49% Test: 60.96%\n",
      "Run: 10, Epoch: 38, Loss: 0.1422, Train: 99.18%, Valid: 60.49% Test: 61.18%\n",
      "Run: 10, Epoch: 39, Loss: 0.1460, Train: 99.18%, Valid: 61.04% Test: 60.75%\n",
      "Run: 10, Epoch: 40, Loss: 0.1261, Train: 99.08%, Valid: 60.91% Test: 60.75%\n",
      "Run: 10, Epoch: 41, Loss: 0.1226, Train: 99.27%, Valid: 61.04% Test: 60.31%\n",
      "Run: 10, Epoch: 42, Loss: 0.1164, Train: 99.18%, Valid: 62.14% Test: 60.09%\n",
      "Run: 10, Epoch: 43, Loss: 0.1019, Train: 99.27%, Valid: 62.41% Test: 59.87%\n",
      "Run: 10, Epoch: 44, Loss: 0.1064, Train: 99.27%, Valid: 62.28% Test: 60.09%\n",
      "Run: 10, Epoch: 45, Loss: 0.1078, Train: 99.36%, Valid: 62.28% Test: 59.87%\n",
      "Run: 10, Epoch: 46, Loss: 0.0975, Train: 99.36%, Valid: 62.14% Test: 60.09%\n",
      "Run: 10, Epoch: 47, Loss: 0.0985, Train: 99.54%, Valid: 62.69% Test: 60.53%\n",
      "Run: 10, Epoch: 48, Loss: 0.0922, Train: 99.54%, Valid: 62.69% Test: 60.75%\n",
      "Run: 10, Epoch: 49, Loss: 0.0947, Train: 99.54%, Valid: 62.00% Test: 60.53%\n",
      "Run: 10, Epoch: 50, Loss: 0.0780, Train: 99.63%, Valid: 61.73% Test: 60.53%\n",
      "Run: 10, Epoch: 51, Loss: 0.0807, Train: 99.63%, Valid: 60.91% Test: 60.53%\n",
      "Run: 10, Epoch: 52, Loss: 0.0993, Train: 99.73%, Valid: 60.49% Test: 60.96%\n",
      "Run: 10, Epoch: 53, Loss: 0.0971, Train: 99.73%, Valid: 59.95% Test: 60.53%\n",
      "Run: 10, Epoch: 54, Loss: 0.0705, Train: 99.73%, Valid: 59.81% Test: 60.96%\n",
      "Run: 10, Epoch: 55, Loss: 0.0868, Train: 99.73%, Valid: 60.08% Test: 60.96%\n",
      "Run: 10, Epoch: 56, Loss: 0.0806, Train: 99.73%, Valid: 60.63% Test: 59.87%\n",
      "Run: 10, Epoch: 57, Loss: 0.0626, Train: 99.82%, Valid: 60.22% Test: 59.65%\n",
      "Run: 10, Epoch: 58, Loss: 0.0774, Train: 99.82%, Valid: 60.49% Test: 59.87%\n",
      "Run: 10, Epoch: 59, Loss: 0.0636, Train: 99.82%, Valid: 60.49% Test: 60.31%\n",
      "Run: 10, Epoch: 60, Loss: 0.0671, Train: 99.82%, Valid: 60.36% Test: 60.31%\n",
      "Run: 10, Epoch: 61, Loss: 0.0687, Train: 99.82%, Valid: 59.95% Test: 60.09%\n",
      "Run: 10, Epoch: 62, Loss: 0.0643, Train: 99.82%, Valid: 59.81% Test: 60.53%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 10, Epoch: 63, Loss: 0.0523, Train: 99.82%, Valid: 59.53% Test: 60.31%\n",
      "Run: 10, Epoch: 64, Loss: 0.0546, Train: 99.82%, Valid: 59.81% Test: 60.53%\n",
      "Run: 10, Epoch: 65, Loss: 0.0525, Train: 99.82%, Valid: 60.22% Test: 60.31%\n",
      "Run: 10, Epoch: 66, Loss: 0.0549, Train: 99.82%, Valid: 60.63% Test: 59.87%\n",
      "Run: 10, Epoch: 67, Loss: 0.0513, Train: 99.82%, Valid: 60.49% Test: 60.09%\n",
      "Run: 10, Epoch: 68, Loss: 0.0565, Train: 99.82%, Valid: 60.36% Test: 60.31%\n",
      "Run: 10, Epoch: 69, Loss: 0.0550, Train: 99.82%, Valid: 60.49% Test: 60.53%\n",
      "Run: 10, Epoch: 70, Loss: 0.0647, Train: 99.82%, Valid: 60.22% Test: 60.53%\n",
      "Run: 10, Epoch: 71, Loss: 0.0433, Train: 99.82%, Valid: 60.22% Test: 60.53%\n",
      "Run: 10, Epoch: 72, Loss: 0.0536, Train: 99.82%, Valid: 60.22% Test: 60.31%\n",
      "Run: 10, Epoch: 73, Loss: 0.0639, Train: 99.82%, Valid: 60.22% Test: 60.09%\n",
      "Run: 10, Epoch: 74, Loss: 0.0519, Train: 99.82%, Valid: 60.49% Test: 59.21%\n",
      "Run: 10, Epoch: 75, Loss: 0.0524, Train: 99.82%, Valid: 60.49% Test: 59.43%\n",
      "Run: 10, Epoch: 76, Loss: 0.0640, Train: 99.82%, Valid: 60.36% Test: 59.65%\n",
      "Run: 10, Epoch: 77, Loss: 0.0539, Train: 99.82%, Valid: 60.36% Test: 60.31%\n",
      "Run: 10, Epoch: 78, Loss: 0.0531, Train: 99.82%, Valid: 60.49% Test: 60.31%\n",
      "Run: 10, Epoch: 79, Loss: 0.0390, Train: 99.82%, Valid: 60.36% Test: 60.31%\n",
      "Run: 10, Epoch: 80, Loss: 0.0482, Train: 99.82%, Valid: 60.36% Test: 60.09%\n",
      "Run: 10, Epoch: 81, Loss: 0.0477, Train: 99.82%, Valid: 60.63% Test: 60.09%\n",
      "Run: 10, Epoch: 82, Loss: 0.0479, Train: 99.82%, Valid: 60.36% Test: 59.87%\n",
      "Run: 10, Epoch: 83, Loss: 0.0483, Train: 99.82%, Valid: 59.95% Test: 60.09%\n",
      "Run: 10, Epoch: 84, Loss: 0.0479, Train: 99.82%, Valid: 59.95% Test: 60.09%\n",
      "Run: 10, Epoch: 85, Loss: 0.0392, Train: 99.82%, Valid: 60.08% Test: 59.65%\n",
      "Run: 10, Epoch: 86, Loss: 0.0415, Train: 99.82%, Valid: 60.08% Test: 59.87%\n",
      "Run: 10, Epoch: 87, Loss: 0.0473, Train: 99.82%, Valid: 60.77% Test: 60.09%\n",
      "Run: 10, Epoch: 88, Loss: 0.0419, Train: 99.82%, Valid: 61.32% Test: 59.87%\n",
      "Run: 10, Epoch: 89, Loss: 0.0438, Train: 99.82%, Valid: 61.04% Test: 60.09%\n",
      "Run: 10, Epoch: 90, Loss: 0.0351, Train: 99.82%, Valid: 60.77% Test: 60.31%\n",
      "Run: 10, Epoch: 91, Loss: 0.0376, Train: 99.82%, Valid: 60.91% Test: 59.87%\n",
      "Run: 10, Epoch: 92, Loss: 0.0369, Train: 99.82%, Valid: 61.32% Test: 60.31%\n",
      "Run: 10, Epoch: 93, Loss: 0.0448, Train: 99.82%, Valid: 61.59% Test: 60.53%\n",
      "Run: 10, Epoch: 94, Loss: 0.0368, Train: 99.82%, Valid: 61.45% Test: 60.75%\n",
      "Run: 10, Epoch: 95, Loss: 0.0406, Train: 99.82%, Valid: 61.04% Test: 60.96%\n",
      "Run: 10, Epoch: 96, Loss: 0.0362, Train: 99.82%, Valid: 61.32% Test: 61.40%\n",
      "Run: 10, Epoch: 97, Loss: 0.0346, Train: 99.82%, Valid: 61.04% Test: 61.18%\n",
      "Run: 10, Epoch: 98, Loss: 0.0446, Train: 99.82%, Valid: 60.91% Test: 61.18%\n",
      "Run: 10, Epoch: 99, Loss: 0.0392, Train: 99.82%, Valid: 60.91% Test: 61.40%\n",
      "Run: 10, Epoch: 100, Loss: 0.0417, Train: 99.82%, Valid: 60.91% Test: 61.40%\n",
      "Run 10:\n",
      "Highest Train: 99.82\n",
      "Highest Valid: 62.69\n",
      "  Final Train: 99.54\n",
      "   Final Test: 60.53\n",
      "All runs:\n",
      "Highest Train: 99.90 ± 0.05\n",
      "Highest Valid: 64.13 ± 1.20\n",
      "  Final Train: 98.29 ± 1.44\n",
      "   Final Test: 62.61 ± 2.75\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
    "         'batch_size': 32, 'hidden_channels': 32, 'dropout': 0.5, 'epochs': 100, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-6, 'lr': 0.01}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    dataset = WikipediaNetwork(root='/tmp/chameleon', name='chameleon',transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    #idx_train=[data.train_mask[i][0] for i in range(len(data.y))]\n",
    "    #train_idx = np.where(idx_train)[0]\n",
    "    #idx_val=[data.val_mask[i][0] for i in range(len(data.y))]\n",
    "    #valid_idx = np.where(idx_val)[0]\n",
    "    #idx_test=[data.test_mask[i][0] for i in range(len(data.y))]\n",
    "    #test_idx = np.where(idx_test)[0]\n",
    "    \n",
    "    model = SAGE(data.num_features, args.hidden_channels,\n",
    "                    dataset.num_classes, args.num_layers,\n",
    "                    args.dropout)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        idx_train=[data.train_mask[i][run] for i in range(len(data.y))]\n",
    "        train_idx = np.where(idx_train)[0]\n",
    "        idx_val=[data.val_mask[i][run] for i in range(len(data.y))]\n",
    "        valid_idx = np.where(idx_val)[0]\n",
    "        idx_test=[data.test_mask[i][run] for i in range(len(data.y))]\n",
    "        test_idx = np.where(idx_test)[0]\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52f151",
   "metadata": {},
   "source": [
    "# Wise Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a09514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2277, 2325], y=[2277], train_mask=[2277, 10], val_mask=[2277, 10], test_mask=[2277, 10], adj_t=[2277, 2277, nnz=36101])\n"
     ]
    }
   ],
   "source": [
    "dataset = WikipediaNetwork(root='/tmp/chameleon', name='chameleon',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f82a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2316</th>\n",
       "      <th>2317</th>\n",
       "      <th>2318</th>\n",
       "      <th>2319</th>\n",
       "      <th>2320</th>\n",
       "      <th>2321</th>\n",
       "      <th>2322</th>\n",
       "      <th>2323</th>\n",
       "      <th>2324</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2326 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  2316  2317  2318  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "   2319  2320  2321  2322  2323  2324  class  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0      0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "3   0.0   0.0   0.0   0.0   1.0   0.0      4  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0      2  \n",
       "\n",
       "[5 rows x 2326 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Domain_Fec=pd.DataFrame(data.x.numpy())\n",
    "label=pd.DataFrame(data.y.numpy(),columns =['class'])\n",
    "Data=pd.concat([Domain_Fec,label], axis=1)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2642b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_nodes=len(data.y)\n",
    "fe_len=len(data.x[0])\n",
    "catagories=Data['class'].to_numpy()\n",
    "data_by_class = {cls: Data.loc[Data['class'] == cls].drop(['class'], axis=1) for cls in range(max(catagories) + 1)}\n",
    "basis = [[max(df[i]) for i in range(len(df.columns))] for df in data_by_class.values()]\n",
    "sel_basis = [[int(list(df[i].to_numpy()).count(1) >= int(len(df[i].index)*0.01)) \n",
    "              for i in range(len(df.columns))]\n",
    "             for df in data_by_class.values()]\n",
    "feature_names = [ii for ii in range(fe_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae7c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity(array1, array2):\n",
    "    intersection = np.sum(np.logical_and(array1, array2))\n",
    "    #union = np.sum(np.logical_or(array1, array2))\n",
    "    #jaccard_similarity = intersection / union\n",
    "    \n",
    "    #return jaccard_similarity\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12133154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes long time\n",
    "Fec=[]\n",
    "for i in range(23):\n",
    "    vec=[]\n",
    "    f=Data.loc[i, feature_names].values.flatten().tolist()\n",
    "    count=np.zeros(7)\n",
    "    for j in range(1433):\n",
    "        for i in range(max(catagories)+1):\n",
    "            if f[j]==1 and basis[i][j]==1:\n",
    "                count[i]=count[i]+1;\n",
    "\n",
    "    for i in range(max(catagories)+1):\n",
    "        vec.append(count[i])\n",
    "    f.clear()\n",
    "    Fec.append(vec)\n",
    "print(Fec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4db5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Fec=[]\n",
    "for i in range(Number_nodes):\n",
    "#for i in range(2):\n",
    "    vec=[]\n",
    "    f=Data.loc[i, feature_names].values.flatten().tolist()\n",
    "    vec.append(Similarity(f,basis[0]))\n",
    "    vec.append(Similarity(f,basis[1]))\n",
    "    vec.append(Similarity(f,basis[2]))\n",
    "    vec.append(Similarity(f,basis[3]))\n",
    "    vec.append(Similarity(f,basis[4]))\n",
    "    #print(f)\n",
    "    f.clear()\n",
    "    Fec.append(vec)\n",
    "SFec=[]\n",
    "for i in range(Number_nodes):\n",
    "#for i in range(2):\n",
    "    Svec=[]\n",
    "    f=Data.loc[i, feature_names].values.flatten().tolist()\n",
    "    Svec.append(Similarity(f,sel_basis[0]))\n",
    "    Svec.append(Similarity(f,sel_basis[1]))\n",
    "    Svec.append(Similarity(f,sel_basis[2]))\n",
    "    Svec.append(Similarity(f,sel_basis[3]))\n",
    "    Svec.append(Similarity(f,sel_basis[4]))\n",
    "    #print(f)\n",
    "    f.clear()\n",
    "    SFec.append(Svec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "054ee569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.,  6.,  6.,  ...,  5.,  5.,  7.],\n",
      "        [10., 12., 18.,  ...,  8., 16., 15.],\n",
      "        [19., 24., 39.,  ..., 19., 33., 28.],\n",
      "        ...,\n",
      "        [ 3.,  2.,  1.,  ...,  0.,  2.,  2.],\n",
      "        [16., 20., 32.,  ..., 17., 22., 38.],\n",
      "        [ 4.,  4.,  3.,  ...,  2.,  1.,  2.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inc_fe=torch.tensor(Fec)\n",
    "sel_fe=torch.tensor(SFec)\n",
    "CC_domain=torch.cat((Inc_fe, sel_fe), 1).float()\n",
    "print(CC_domain)\n",
    "CC_domain.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f7d51",
   "metadata": {},
   "source": [
    "# W-GSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55c6fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2277, 10], y=[2277], train_mask=[2277, 10], val_mask=[2277, 10], test_mask=[2277, 10], adj_t=[2277, 2277, nnz=36101])\n"
     ]
    }
   ],
   "source": [
    "data.x=CC_domain\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4763ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x161e57ee0>\n",
      "Run: 01, Epoch: 01, Loss: 1.9146, Train: 21.70%, Valid: 23.73% Test: 22.15%\n",
      "Run: 01, Epoch: 02, Loss: 1.7175, Train: 21.52%, Valid: 20.30% Test: 23.25%\n",
      "Run: 01, Epoch: 03, Loss: 1.5985, Train: 22.25%, Valid: 19.75% Test: 23.46%\n",
      "Run: 01, Epoch: 04, Loss: 1.4756, Train: 25.00%, Valid: 22.50% Test: 26.32%\n",
      "Run: 01, Epoch: 05, Loss: 1.4087, Train: 27.38%, Valid: 26.34% Test: 31.80%\n",
      "Run: 01, Epoch: 06, Loss: 1.3876, Train: 32.23%, Valid: 30.86% Test: 34.87%\n",
      "Run: 01, Epoch: 07, Loss: 1.3560, Train: 35.53%, Valid: 34.43% Test: 38.38%\n",
      "Run: 01, Epoch: 08, Loss: 1.3494, Train: 38.55%, Valid: 36.63% Test: 38.82%\n",
      "Run: 01, Epoch: 09, Loss: 1.3199, Train: 40.48%, Valid: 37.72% Test: 41.01%\n",
      "Run: 01, Epoch: 10, Loss: 1.3151, Train: 40.75%, Valid: 37.17% Test: 42.54%\n",
      "Run: 01, Epoch: 11, Loss: 1.2756, Train: 42.40%, Valid: 37.72% Test: 43.20%\n",
      "Run: 01, Epoch: 12, Loss: 1.2680, Train: 43.32%, Valid: 38.00% Test: 44.52%\n",
      "Run: 01, Epoch: 13, Loss: 1.2348, Train: 45.24%, Valid: 41.43% Test: 46.49%\n",
      "Run: 01, Epoch: 14, Loss: 1.2390, Train: 46.52%, Valid: 43.62% Test: 46.93%\n",
      "Run: 01, Epoch: 15, Loss: 1.2295, Train: 47.89%, Valid: 45.27% Test: 47.59%\n",
      "Run: 01, Epoch: 16, Loss: 1.2022, Train: 47.53%, Valid: 44.72% Test: 47.15%\n",
      "Run: 01, Epoch: 17, Loss: 1.1992, Train: 49.73%, Valid: 46.50% Test: 47.15%\n",
      "Run: 01, Epoch: 18, Loss: 1.1689, Train: 52.38%, Valid: 48.29% Test: 48.46%\n",
      "Run: 01, Epoch: 19, Loss: 1.1706, Train: 55.22%, Valid: 51.03% Test: 53.95%\n",
      "Run: 01, Epoch: 20, Loss: 1.1377, Train: 54.49%, Valid: 54.73% Test: 53.95%\n",
      "Run: 01, Epoch: 21, Loss: 1.1495, Train: 54.30%, Valid: 54.05% Test: 51.54%\n",
      "Run: 01, Epoch: 22, Loss: 1.1619, Train: 54.95%, Valid: 53.64% Test: 50.66%\n",
      "Run: 01, Epoch: 23, Loss: 1.1324, Train: 54.85%, Valid: 52.95% Test: 51.75%\n",
      "Run: 01, Epoch: 24, Loss: 1.1238, Train: 56.04%, Valid: 55.56% Test: 53.51%\n",
      "Run: 01, Epoch: 25, Loss: 1.1173, Train: 58.06%, Valid: 56.38% Test: 55.70%\n",
      "Run: 01, Epoch: 26, Loss: 1.0855, Train: 59.43%, Valid: 58.85% Test: 58.99%\n",
      "Run: 01, Epoch: 27, Loss: 1.0778, Train: 62.00%, Valid: 59.95% Test: 60.75%\n",
      "Run: 01, Epoch: 28, Loss: 1.0784, Train: 61.72%, Valid: 58.85% Test: 60.53%\n",
      "Run: 01, Epoch: 29, Loss: 1.0352, Train: 61.72%, Valid: 58.57% Test: 60.53%\n",
      "Run: 01, Epoch: 30, Loss: 1.0616, Train: 61.81%, Valid: 59.53% Test: 60.75%\n",
      "Run: 01, Epoch: 31, Loss: 1.0293, Train: 60.99%, Valid: 58.02% Test: 62.06%\n",
      "Run: 01, Epoch: 32, Loss: 1.0376, Train: 62.64%, Valid: 60.08% Test: 62.50%\n",
      "Run: 01, Epoch: 33, Loss: 1.0423, Train: 64.01%, Valid: 60.77% Test: 63.60%\n",
      "Run: 01, Epoch: 34, Loss: 1.0262, Train: 64.38%, Valid: 62.14% Test: 65.13%\n",
      "Run: 01, Epoch: 35, Loss: 1.0031, Train: 65.66%, Valid: 63.65% Test: 66.01%\n",
      "Run: 01, Epoch: 36, Loss: 1.0099, Train: 65.20%, Valid: 63.65% Test: 65.13%\n",
      "Run: 01, Epoch: 37, Loss: 0.9924, Train: 65.38%, Valid: 65.29% Test: 64.91%\n",
      "Run: 01, Epoch: 38, Loss: 0.9852, Train: 65.02%, Valid: 64.61% Test: 64.47%\n",
      "Run: 01, Epoch: 39, Loss: 0.9767, Train: 64.74%, Valid: 63.79% Test: 65.57%\n",
      "Run: 01, Epoch: 40, Loss: 0.9627, Train: 64.65%, Valid: 63.51% Test: 65.13%\n",
      "Run: 01, Epoch: 41, Loss: 0.9584, Train: 68.32%, Valid: 66.53% Test: 67.76%\n",
      "Run: 01, Epoch: 42, Loss: 0.9768, Train: 70.51%, Valid: 67.76% Test: 69.52%\n",
      "Run: 01, Epoch: 43, Loss: 0.9936, Train: 71.52%, Valid: 67.76% Test: 69.74%\n",
      "Run: 01, Epoch: 44, Loss: 0.9536, Train: 70.24%, Valid: 68.31% Test: 69.30%\n",
      "Run: 01, Epoch: 45, Loss: 0.9502, Train: 69.87%, Valid: 68.18% Test: 68.42%\n",
      "Run: 01, Epoch: 46, Loss: 0.9471, Train: 71.70%, Valid: 68.59% Test: 69.08%\n",
      "Run: 01, Epoch: 47, Loss: 0.9032, Train: 71.43%, Valid: 69.82% Test: 68.64%\n",
      "Run: 01, Epoch: 48, Loss: 0.9013, Train: 73.08%, Valid: 71.74% Test: 71.27%\n",
      "Run: 01, Epoch: 49, Loss: 0.9018, Train: 72.71%, Valid: 71.88% Test: 72.15%\n",
      "Run: 01, Epoch: 50, Loss: 0.9093, Train: 70.24%, Valid: 69.68% Test: 71.93%\n",
      "Run: 01, Epoch: 51, Loss: 0.8958, Train: 69.69%, Valid: 68.45% Test: 71.05%\n",
      "Run: 01, Epoch: 52, Loss: 0.8988, Train: 72.44%, Valid: 70.78% Test: 71.05%\n",
      "Run: 01, Epoch: 53, Loss: 0.8991, Train: 73.99%, Valid: 72.15% Test: 72.15%\n",
      "Run: 01, Epoch: 54, Loss: 0.8907, Train: 70.97%, Valid: 70.78% Test: 71.93%\n",
      "Run: 01, Epoch: 55, Loss: 0.8808, Train: 71.61%, Valid: 71.74% Test: 72.37%\n",
      "Run: 01, Epoch: 56, Loss: 0.8753, Train: 71.79%, Valid: 70.23% Test: 69.96%\n",
      "Run: 01, Epoch: 57, Loss: 0.8532, Train: 70.51%, Valid: 68.86% Test: 68.64%\n",
      "Run: 01, Epoch: 58, Loss: 0.8519, Train: 70.88%, Valid: 69.41% Test: 70.61%\n",
      "Run: 01, Epoch: 59, Loss: 0.8384, Train: 72.16%, Valid: 70.37% Test: 72.37%\n",
      "Run: 01, Epoch: 60, Loss: 0.8555, Train: 72.71%, Valid: 71.47% Test: 72.81%\n",
      "Run: 01, Epoch: 61, Loss: 0.8419, Train: 74.36%, Valid: 72.98% Test: 75.66%\n",
      "Run: 01, Epoch: 62, Loss: 0.8381, Train: 76.01%, Valid: 74.07% Test: 75.88%\n",
      "Run: 01, Epoch: 63, Loss: 0.8100, Train: 76.83%, Valid: 75.45% Test: 76.32%\n",
      "Run: 01, Epoch: 64, Loss: 0.8628, Train: 76.92%, Valid: 75.45% Test: 74.78%\n",
      "Run: 01, Epoch: 65, Loss: 0.8140, Train: 75.09%, Valid: 74.35% Test: 74.12%\n",
      "Run: 01, Epoch: 66, Loss: 0.8516, Train: 75.46%, Valid: 74.76% Test: 74.56%\n",
      "Run: 01, Epoch: 67, Loss: 0.8224, Train: 76.37%, Valid: 76.41% Test: 74.78%\n",
      "Run: 01, Epoch: 68, Loss: 0.8089, Train: 76.83%, Valid: 76.41% Test: 75.00%\n",
      "Run: 01, Epoch: 69, Loss: 0.8075, Train: 76.65%, Valid: 75.31% Test: 74.78%\n",
      "Run: 01, Epoch: 70, Loss: 0.8154, Train: 77.20%, Valid: 76.95% Test: 74.12%\n",
      "Run: 01, Epoch: 71, Loss: 0.8036, Train: 75.73%, Valid: 76.27% Test: 74.78%\n",
      "Run: 01, Epoch: 72, Loss: 0.7898, Train: 76.47%, Valid: 76.82% Test: 75.22%\n",
      "Run: 01, Epoch: 73, Loss: 0.7873, Train: 77.38%, Valid: 77.50% Test: 76.54%\n",
      "Run: 01, Epoch: 74, Loss: 0.7902, Train: 76.65%, Valid: 76.68% Test: 76.10%\n",
      "Run: 01, Epoch: 75, Loss: 0.7960, Train: 76.19%, Valid: 77.09% Test: 75.22%\n",
      "Run: 01, Epoch: 76, Loss: 0.7627, Train: 74.54%, Valid: 75.03% Test: 73.46%\n",
      "Run: 01, Epoch: 77, Loss: 0.7749, Train: 72.80%, Valid: 73.80% Test: 71.71%\n",
      "Run: 01, Epoch: 78, Loss: 0.7819, Train: 74.82%, Valid: 75.58% Test: 73.46%\n",
      "Run: 01, Epoch: 79, Loss: 0.7870, Train: 76.65%, Valid: 77.23% Test: 75.00%\n",
      "Run: 01, Epoch: 80, Loss: 0.7954, Train: 77.93%, Valid: 76.95% Test: 75.00%\n",
      "Run: 01, Epoch: 81, Loss: 0.7641, Train: 77.66%, Valid: 76.54% Test: 76.32%\n",
      "Run: 01, Epoch: 82, Loss: 0.7921, Train: 78.39%, Valid: 78.46% Test: 78.07%\n",
      "Run: 01, Epoch: 83, Loss: 0.7822, Train: 78.21%, Valid: 78.19% Test: 76.75%\n",
      "Run: 01, Epoch: 84, Loss: 0.7647, Train: 77.20%, Valid: 76.13% Test: 76.10%\n",
      "Run: 01, Epoch: 85, Loss: 0.7707, Train: 77.29%, Valid: 76.54% Test: 76.10%\n",
      "Run: 01, Epoch: 86, Loss: 0.7436, Train: 77.38%, Valid: 78.19% Test: 76.97%\n",
      "Run: 01, Epoch: 87, Loss: 0.7413, Train: 77.47%, Valid: 78.46% Test: 75.88%\n",
      "Run: 01, Epoch: 88, Loss: 0.7393, Train: 77.93%, Valid: 78.74% Test: 76.97%\n",
      "Run: 01, Epoch: 89, Loss: 0.7422, Train: 78.30%, Valid: 78.74% Test: 75.88%\n",
      "Run: 01, Epoch: 90, Loss: 0.7411, Train: 77.84%, Valid: 79.42% Test: 75.66%\n",
      "Run: 01, Epoch: 91, Loss: 0.7347, Train: 77.56%, Valid: 79.01% Test: 74.78%\n",
      "Run: 01, Epoch: 92, Loss: 0.7475, Train: 77.84%, Valid: 78.33% Test: 75.22%\n",
      "Run: 01, Epoch: 93, Loss: 0.7241, Train: 77.93%, Valid: 79.01% Test: 75.88%\n",
      "Run: 01, Epoch: 94, Loss: 0.7075, Train: 78.30%, Valid: 78.46% Test: 75.00%\n",
      "Run: 01, Epoch: 95, Loss: 0.7217, Train: 78.02%, Valid: 78.05% Test: 75.66%\n",
      "Run: 01, Epoch: 96, Loss: 0.7234, Train: 76.65%, Valid: 77.91% Test: 75.44%\n",
      "Run: 01, Epoch: 97, Loss: 0.7063, Train: 77.66%, Valid: 78.74% Test: 76.75%\n",
      "Run: 01, Epoch: 98, Loss: 0.7186, Train: 79.95%, Valid: 78.88% Test: 78.07%\n",
      "Run: 01, Epoch: 99, Loss: 0.7318, Train: 80.22%, Valid: 78.74% Test: 77.63%\n",
      "Run: 01, Epoch: 100, Loss: 0.6962, Train: 79.12%, Valid: 78.74% Test: 77.85%\n",
      "Run 01:\n",
      "Highest Train: 80.22\n",
      "Highest Valid: 79.42\n",
      "  Final Train: 77.84\n",
      "   Final Test: 75.66\n",
      "Run: 02, Epoch: 01, Loss: 1.7450, Train: 20.51%, Valid: 23.05% Test: 21.27%\n",
      "Run: 02, Epoch: 02, Loss: 1.6032, Train: 23.63%, Valid: 24.69% Test: 22.81%\n",
      "Run: 02, Epoch: 03, Loss: 1.5010, Train: 26.10%, Valid: 26.61% Test: 23.90%\n",
      "Run: 02, Epoch: 04, Loss: 1.4554, Train: 30.95%, Valid: 29.90% Test: 28.07%\n",
      "Run: 02, Epoch: 05, Loss: 1.4182, Train: 34.71%, Valid: 33.06% Test: 33.11%\n",
      "Run: 02, Epoch: 06, Loss: 1.4194, Train: 37.91%, Valid: 36.76% Test: 36.84%\n",
      "Run: 02, Epoch: 07, Loss: 1.3958, Train: 39.29%, Valid: 38.68% Test: 38.16%\n",
      "Run: 02, Epoch: 08, Loss: 1.3499, Train: 41.21%, Valid: 43.76% Test: 44.30%\n",
      "Run: 02, Epoch: 09, Loss: 1.3194, Train: 46.70%, Valid: 46.50% Test: 51.54%\n",
      "Run: 02, Epoch: 10, Loss: 1.3277, Train: 48.35%, Valid: 48.70% Test: 49.56%\n",
      "Run: 02, Epoch: 11, Loss: 1.2868, Train: 44.69%, Valid: 46.50% Test: 50.66%\n",
      "Run: 02, Epoch: 12, Loss: 1.2859, Train: 42.31%, Valid: 43.07% Test: 46.93%\n",
      "Run: 02, Epoch: 13, Loss: 1.2666, Train: 40.75%, Valid: 42.39% Test: 47.15%\n",
      "Run: 02, Epoch: 14, Loss: 1.2448, Train: 39.65%, Valid: 42.80% Test: 44.74%\n",
      "Run: 02, Epoch: 15, Loss: 1.2600, Train: 41.03%, Valid: 43.62% Test: 44.96%\n",
      "Run: 02, Epoch: 16, Loss: 1.2239, Train: 44.23%, Valid: 45.54% Test: 48.46%\n",
      "Run: 02, Epoch: 17, Loss: 1.1938, Train: 47.62%, Valid: 49.11% Test: 52.85%\n",
      "Run: 02, Epoch: 18, Loss: 1.1548, Train: 51.19%, Valid: 52.13% Test: 54.39%\n",
      "Run: 02, Epoch: 19, Loss: 1.1707, Train: 53.39%, Valid: 54.18% Test: 54.61%\n",
      "Run: 02, Epoch: 20, Loss: 1.1726, Train: 54.30%, Valid: 53.09% Test: 53.73%\n",
      "Run: 02, Epoch: 21, Loss: 1.1243, Train: 54.30%, Valid: 52.26% Test: 53.73%\n",
      "Run: 02, Epoch: 22, Loss: 1.1208, Train: 54.58%, Valid: 53.09% Test: 53.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 02, Epoch: 23, Loss: 1.1189, Train: 55.13%, Valid: 53.50% Test: 55.92%\n",
      "Run: 02, Epoch: 24, Loss: 1.0965, Train: 56.32%, Valid: 54.73% Test: 58.11%\n",
      "Run: 02, Epoch: 25, Loss: 1.0642, Train: 57.97%, Valid: 56.10% Test: 60.75%\n",
      "Run: 02, Epoch: 26, Loss: 1.0769, Train: 57.88%, Valid: 55.42% Test: 59.65%\n",
      "Run: 02, Epoch: 27, Loss: 1.0390, Train: 58.42%, Valid: 55.01% Test: 59.21%\n",
      "Run: 02, Epoch: 28, Loss: 1.0468, Train: 57.05%, Valid: 57.75% Test: 59.21%\n",
      "Run: 02, Epoch: 29, Loss: 1.0255, Train: 58.15%, Valid: 58.30% Test: 59.43%\n",
      "Run: 02, Epoch: 30, Loss: 1.0240, Train: 60.16%, Valid: 58.30% Test: 61.18%\n",
      "Run: 02, Epoch: 31, Loss: 1.0120, Train: 61.54%, Valid: 59.12% Test: 62.06%\n",
      "Run: 02, Epoch: 32, Loss: 0.9934, Train: 62.36%, Valid: 61.59% Test: 62.72%\n",
      "Run: 02, Epoch: 33, Loss: 0.9768, Train: 61.17%, Valid: 62.00% Test: 63.38%\n",
      "Run: 02, Epoch: 34, Loss: 0.9624, Train: 60.71%, Valid: 62.00% Test: 62.72%\n",
      "Run: 02, Epoch: 35, Loss: 0.9813, Train: 63.10%, Valid: 62.14% Test: 63.60%\n",
      "Run: 02, Epoch: 36, Loss: 0.9500, Train: 62.73%, Valid: 61.59% Test: 63.16%\n",
      "Run: 02, Epoch: 37, Loss: 0.9530, Train: 63.37%, Valid: 63.10% Test: 62.50%\n",
      "Run: 02, Epoch: 38, Loss: 0.9335, Train: 64.65%, Valid: 63.51% Test: 64.91%\n",
      "Run: 02, Epoch: 39, Loss: 0.9514, Train: 66.21%, Valid: 64.47% Test: 67.11%\n",
      "Run: 02, Epoch: 40, Loss: 0.8927, Train: 68.86%, Valid: 66.39% Test: 68.20%\n",
      "Run: 02, Epoch: 41, Loss: 0.8887, Train: 69.23%, Valid: 65.16% Test: 66.89%\n",
      "Run: 02, Epoch: 42, Loss: 0.9239, Train: 69.05%, Valid: 65.71% Test: 66.89%\n",
      "Run: 02, Epoch: 43, Loss: 0.8810, Train: 68.04%, Valid: 65.16% Test: 66.67%\n",
      "Run: 02, Epoch: 44, Loss: 0.8867, Train: 68.13%, Valid: 64.47% Test: 67.32%\n",
      "Run: 02, Epoch: 45, Loss: 0.8686, Train: 68.32%, Valid: 64.06% Test: 65.35%\n",
      "Run: 02, Epoch: 46, Loss: 0.8606, Train: 68.04%, Valid: 63.65% Test: 64.91%\n",
      "Run: 02, Epoch: 47, Loss: 0.8336, Train: 69.32%, Valid: 63.24% Test: 65.79%\n",
      "Run: 02, Epoch: 48, Loss: 0.8700, Train: 69.51%, Valid: 64.88% Test: 65.79%\n",
      "Run: 02, Epoch: 49, Loss: 0.8396, Train: 69.78%, Valid: 65.71% Test: 68.64%\n",
      "Run: 02, Epoch: 50, Loss: 0.8303, Train: 69.14%, Valid: 65.16% Test: 68.42%\n",
      "Run: 02, Epoch: 51, Loss: 0.8532, Train: 70.60%, Valid: 66.80% Test: 68.86%\n",
      "Run: 02, Epoch: 52, Loss: 0.8185, Train: 70.51%, Valid: 67.22% Test: 69.30%\n",
      "Run: 02, Epoch: 53, Loss: 0.8366, Train: 72.62%, Valid: 68.31% Test: 71.27%\n",
      "Run: 02, Epoch: 54, Loss: 0.8214, Train: 74.08%, Valid: 70.10% Test: 70.83%\n",
      "Run: 02, Epoch: 55, Loss: 0.8077, Train: 73.17%, Valid: 68.45% Test: 69.30%\n",
      "Run: 02, Epoch: 56, Loss: 0.7945, Train: 73.44%, Valid: 69.14% Test: 69.52%\n",
      "Run: 02, Epoch: 57, Loss: 0.8107, Train: 74.54%, Valid: 70.23% Test: 71.05%\n",
      "Run: 02, Epoch: 58, Loss: 0.8211, Train: 75.09%, Valid: 70.37% Test: 72.37%\n",
      "Run: 02, Epoch: 59, Loss: 0.7850, Train: 75.09%, Valid: 70.23% Test: 73.03%\n",
      "Run: 02, Epoch: 60, Loss: 0.8020, Train: 78.75%, Valid: 73.80% Test: 76.10%\n",
      "Run: 02, Epoch: 61, Loss: 0.7860, Train: 77.93%, Valid: 74.21% Test: 75.88%\n",
      "Run: 02, Epoch: 62, Loss: 0.7994, Train: 77.38%, Valid: 73.94% Test: 76.10%\n",
      "Run: 02, Epoch: 63, Loss: 0.7877, Train: 77.29%, Valid: 73.39% Test: 75.22%\n",
      "Run: 02, Epoch: 64, Loss: 0.7840, Train: 76.19%, Valid: 71.33% Test: 73.90%\n",
      "Run: 02, Epoch: 65, Loss: 0.7702, Train: 75.73%, Valid: 72.15% Test: 72.37%\n",
      "Run: 02, Epoch: 66, Loss: 0.7706, Train: 75.27%, Valid: 71.74% Test: 72.59%\n",
      "Run: 02, Epoch: 67, Loss: 0.7765, Train: 75.82%, Valid: 73.25% Test: 71.93%\n",
      "Run: 02, Epoch: 68, Loss: 0.7608, Train: 77.29%, Valid: 74.21% Test: 73.46%\n",
      "Run: 02, Epoch: 69, Loss: 0.7939, Train: 77.66%, Valid: 74.35% Test: 74.12%\n",
      "Run: 02, Epoch: 70, Loss: 0.7533, Train: 77.56%, Valid: 73.53% Test: 74.12%\n",
      "Run: 02, Epoch: 71, Loss: 0.7430, Train: 77.11%, Valid: 73.80% Test: 74.12%\n",
      "Run: 02, Epoch: 72, Loss: 0.7502, Train: 79.40%, Valid: 75.99% Test: 76.75%\n",
      "Run: 02, Epoch: 73, Loss: 0.7354, Train: 78.66%, Valid: 76.27% Test: 73.68%\n",
      "Run: 02, Epoch: 74, Loss: 0.7821, Train: 78.11%, Valid: 73.39% Test: 72.37%\n",
      "Run: 02, Epoch: 75, Loss: 0.7279, Train: 78.30%, Valid: 74.21% Test: 73.46%\n",
      "Run: 02, Epoch: 76, Loss: 0.7320, Train: 78.39%, Valid: 74.35% Test: 75.00%\n",
      "Run: 02, Epoch: 77, Loss: 0.7508, Train: 77.20%, Valid: 73.25% Test: 72.59%\n",
      "Run: 02, Epoch: 78, Loss: 0.7415, Train: 77.11%, Valid: 73.66% Test: 72.81%\n",
      "Run: 02, Epoch: 79, Loss: 0.7609, Train: 77.56%, Valid: 74.90% Test: 74.12%\n",
      "Run: 02, Epoch: 80, Loss: 0.7533, Train: 79.03%, Valid: 75.58% Test: 75.66%\n",
      "Run: 02, Epoch: 81, Loss: 0.7191, Train: 79.76%, Valid: 75.99% Test: 75.00%\n",
      "Run: 02, Epoch: 82, Loss: 0.7653, Train: 79.21%, Valid: 75.03% Test: 73.90%\n",
      "Run: 02, Epoch: 83, Loss: 0.7166, Train: 78.30%, Valid: 76.27% Test: 75.00%\n",
      "Run: 02, Epoch: 84, Loss: 0.7099, Train: 77.84%, Valid: 75.17% Test: 74.78%\n",
      "Run: 02, Epoch: 85, Loss: 0.7072, Train: 77.75%, Valid: 74.21% Test: 74.34%\n",
      "Run: 02, Epoch: 86, Loss: 0.7253, Train: 78.94%, Valid: 74.90% Test: 75.88%\n",
      "Run: 02, Epoch: 87, Loss: 0.7192, Train: 78.94%, Valid: 75.45% Test: 75.22%\n",
      "Run: 02, Epoch: 88, Loss: 0.7191, Train: 80.59%, Valid: 76.54% Test: 77.41%\n",
      "Run: 02, Epoch: 89, Loss: 0.7237, Train: 81.14%, Valid: 75.72% Test: 77.41%\n",
      "Run: 02, Epoch: 90, Loss: 0.7008, Train: 81.41%, Valid: 75.45% Test: 76.75%\n",
      "Run: 02, Epoch: 91, Loss: 0.7002, Train: 80.59%, Valid: 75.03% Test: 77.63%\n",
      "Run: 02, Epoch: 92, Loss: 0.7222, Train: 81.68%, Valid: 76.68% Test: 77.41%\n",
      "Run: 02, Epoch: 93, Loss: 0.7003, Train: 81.59%, Valid: 76.82% Test: 78.73%\n",
      "Run: 02, Epoch: 94, Loss: 0.6782, Train: 81.41%, Valid: 75.45% Test: 77.41%\n",
      "Run: 02, Epoch: 95, Loss: 0.6886, Train: 81.14%, Valid: 74.35% Test: 77.19%\n",
      "Run: 02, Epoch: 96, Loss: 0.7114, Train: 81.68%, Valid: 77.23% Test: 78.07%\n",
      "Run: 02, Epoch: 97, Loss: 0.6871, Train: 82.78%, Valid: 78.19% Test: 78.29%\n",
      "Run: 02, Epoch: 98, Loss: 0.7045, Train: 82.05%, Valid: 78.05% Test: 79.39%\n",
      "Run: 02, Epoch: 99, Loss: 0.7028, Train: 81.87%, Valid: 78.33% Test: 78.95%\n",
      "Run: 02, Epoch: 100, Loss: 0.6943, Train: 81.32%, Valid: 76.95% Test: 78.51%\n",
      "Run 02:\n",
      "Highest Train: 82.78\n",
      "Highest Valid: 78.33\n",
      "  Final Train: 81.87\n",
      "   Final Test: 78.95\n",
      "Run: 03, Epoch: 01, Loss: 1.9553, Train: 30.59%, Valid: 29.08% Test: 28.95%\n",
      "Run: 03, Epoch: 02, Loss: 1.7233, Train: 28.21%, Valid: 28.53% Test: 27.63%\n",
      "Run: 03, Epoch: 03, Loss: 1.5866, Train: 31.23%, Valid: 31.55% Test: 30.48%\n",
      "Run: 03, Epoch: 04, Loss: 1.4903, Train: 34.80%, Valid: 33.33% Test: 33.11%\n",
      "Run: 03, Epoch: 05, Loss: 1.4389, Train: 36.72%, Valid: 38.96% Test: 32.02%\n",
      "Run: 03, Epoch: 06, Loss: 1.3893, Train: 40.11%, Valid: 42.52% Test: 39.25%\n",
      "Run: 03, Epoch: 07, Loss: 1.3616, Train: 41.21%, Valid: 42.11% Test: 39.69%\n",
      "Run: 03, Epoch: 08, Loss: 1.3037, Train: 44.23%, Valid: 46.78% Test: 44.74%\n",
      "Run: 03, Epoch: 09, Loss: 1.3170, Train: 46.61%, Valid: 46.91% Test: 44.96%\n",
      "Run: 03, Epoch: 10, Loss: 1.2884, Train: 46.52%, Valid: 46.64% Test: 44.96%\n",
      "Run: 03, Epoch: 11, Loss: 1.2676, Train: 46.25%, Valid: 46.36% Test: 45.83%\n",
      "Run: 03, Epoch: 12, Loss: 1.2634, Train: 45.42%, Valid: 46.50% Test: 46.05%\n",
      "Run: 03, Epoch: 13, Loss: 1.2139, Train: 47.89%, Valid: 47.74% Test: 45.39%\n",
      "Run: 03, Epoch: 14, Loss: 1.2174, Train: 51.47%, Valid: 48.15% Test: 46.93%\n",
      "Run: 03, Epoch: 15, Loss: 1.1965, Train: 53.94%, Valid: 49.66% Test: 48.46%\n",
      "Run: 03, Epoch: 16, Loss: 1.2216, Train: 53.21%, Valid: 48.70% Test: 48.46%\n",
      "Run: 03, Epoch: 17, Loss: 1.2027, Train: 52.29%, Valid: 48.15% Test: 47.81%\n",
      "Run: 03, Epoch: 18, Loss: 1.2015, Train: 52.47%, Valid: 47.87% Test: 47.37%\n",
      "Run: 03, Epoch: 19, Loss: 1.1730, Train: 52.20%, Valid: 47.46% Test: 47.81%\n",
      "Run: 03, Epoch: 20, Loss: 1.1755, Train: 52.20%, Valid: 47.74% Test: 47.37%\n",
      "Run: 03, Epoch: 21, Loss: 1.1611, Train: 53.02%, Valid: 47.33% Test: 48.03%\n",
      "Run: 03, Epoch: 22, Loss: 1.1493, Train: 53.66%, Valid: 47.33% Test: 47.81%\n",
      "Run: 03, Epoch: 23, Loss: 1.1341, Train: 53.57%, Valid: 47.19% Test: 48.90%\n",
      "Run: 03, Epoch: 24, Loss: 1.1146, Train: 53.11%, Valid: 47.05% Test: 49.12%\n",
      "Run: 03, Epoch: 25, Loss: 1.0832, Train: 53.94%, Valid: 48.83% Test: 48.90%\n",
      "Run: 03, Epoch: 26, Loss: 1.0954, Train: 55.22%, Valid: 49.66% Test: 50.44%\n",
      "Run: 03, Epoch: 27, Loss: 1.0941, Train: 56.04%, Valid: 50.34% Test: 50.00%\n",
      "Run: 03, Epoch: 28, Loss: 1.0807, Train: 56.68%, Valid: 50.07% Test: 50.22%\n",
      "Run: 03, Epoch: 29, Loss: 1.0633, Train: 55.77%, Valid: 49.79% Test: 50.44%\n",
      "Run: 03, Epoch: 30, Loss: 1.0643, Train: 55.68%, Valid: 48.70% Test: 50.00%\n",
      "Run: 03, Epoch: 31, Loss: 1.0484, Train: 56.14%, Valid: 49.11% Test: 50.88%\n",
      "Run: 03, Epoch: 32, Loss: 1.0342, Train: 57.42%, Valid: 50.34% Test: 50.66%\n",
      "Run: 03, Epoch: 33, Loss: 1.0378, Train: 58.88%, Valid: 52.26% Test: 51.10%\n",
      "Run: 03, Epoch: 34, Loss: 1.0430, Train: 59.71%, Valid: 51.71% Test: 51.10%\n",
      "Run: 03, Epoch: 35, Loss: 1.0284, Train: 60.81%, Valid: 54.32% Test: 53.51%\n",
      "Run: 03, Epoch: 36, Loss: 1.0074, Train: 60.62%, Valid: 55.42% Test: 53.73%\n",
      "Run: 03, Epoch: 37, Loss: 0.9751, Train: 60.81%, Valid: 55.01% Test: 55.92%\n",
      "Run: 03, Epoch: 38, Loss: 0.9689, Train: 61.54%, Valid: 56.24% Test: 56.36%\n",
      "Run: 03, Epoch: 39, Loss: 0.9648, Train: 63.10%, Valid: 56.38% Test: 58.11%\n",
      "Run: 03, Epoch: 40, Loss: 0.9850, Train: 62.91%, Valid: 54.87% Test: 58.33%\n",
      "Run: 03, Epoch: 41, Loss: 0.9523, Train: 63.00%, Valid: 55.28% Test: 58.77%\n",
      "Run: 03, Epoch: 42, Loss: 0.9351, Train: 64.29%, Valid: 56.93% Test: 56.58%\n",
      "Run: 03, Epoch: 43, Loss: 0.9076, Train: 63.10%, Valid: 55.83% Test: 55.04%\n",
      "Run: 03, Epoch: 44, Loss: 0.9142, Train: 61.81%, Valid: 55.42% Test: 55.48%\n",
      "Run: 03, Epoch: 45, Loss: 0.9274, Train: 64.10%, Valid: 56.38% Test: 57.24%\n",
      "Run: 03, Epoch: 46, Loss: 0.9204, Train: 67.40%, Valid: 58.98% Test: 59.87%\n",
      "Run: 03, Epoch: 47, Loss: 0.8715, Train: 68.59%, Valid: 60.91% Test: 61.84%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 03, Epoch: 48, Loss: 0.8945, Train: 69.41%, Valid: 61.45% Test: 63.16%\n",
      "Run: 03, Epoch: 49, Loss: 0.9205, Train: 71.89%, Valid: 63.92% Test: 64.91%\n",
      "Run: 03, Epoch: 50, Loss: 0.8772, Train: 72.71%, Valid: 64.88% Test: 64.69%\n",
      "Run: 03, Epoch: 51, Loss: 0.8871, Train: 71.79%, Valid: 66.39% Test: 65.79%\n",
      "Run: 03, Epoch: 52, Loss: 0.8674, Train: 71.52%, Valid: 66.53% Test: 66.23%\n",
      "Run: 03, Epoch: 53, Loss: 0.8825, Train: 72.99%, Valid: 65.57% Test: 66.89%\n",
      "Run: 03, Epoch: 54, Loss: 0.8628, Train: 71.61%, Valid: 65.43% Test: 66.89%\n",
      "Run: 03, Epoch: 55, Loss: 0.8530, Train: 73.90%, Valid: 66.53% Test: 66.67%\n",
      "Run: 03, Epoch: 56, Loss: 0.8419, Train: 73.08%, Valid: 67.76% Test: 66.45%\n",
      "Run: 03, Epoch: 57, Loss: 0.8363, Train: 73.44%, Valid: 65.98% Test: 64.91%\n",
      "Run: 03, Epoch: 58, Loss: 0.8626, Train: 74.82%, Valid: 68.18% Test: 65.79%\n",
      "Run: 03, Epoch: 59, Loss: 0.8127, Train: 77.29%, Valid: 69.27% Test: 69.08%\n",
      "Run: 03, Epoch: 60, Loss: 0.7900, Train: 78.21%, Valid: 70.23% Test: 70.18%\n",
      "Run: 03, Epoch: 61, Loss: 0.8014, Train: 79.30%, Valid: 70.92% Test: 70.61%\n",
      "Run: 03, Epoch: 62, Loss: 0.7925, Train: 76.65%, Valid: 69.55% Test: 69.52%\n",
      "Run: 03, Epoch: 63, Loss: 0.7916, Train: 74.91%, Valid: 68.04% Test: 67.11%\n",
      "Run: 03, Epoch: 64, Loss: 0.8094, Train: 72.71%, Valid: 65.16% Test: 66.45%\n",
      "Run: 03, Epoch: 65, Loss: 0.7842, Train: 71.43%, Valid: 63.10% Test: 65.35%\n",
      "Run: 03, Epoch: 66, Loss: 0.8061, Train: 72.71%, Valid: 64.20% Test: 65.79%\n",
      "Run: 03, Epoch: 67, Loss: 0.7692, Train: 74.45%, Valid: 67.63% Test: 70.18%\n",
      "Run: 03, Epoch: 68, Loss: 0.8261, Train: 74.08%, Valid: 67.22% Test: 70.18%\n",
      "Run: 03, Epoch: 69, Loss: 0.7682, Train: 74.82%, Valid: 67.76% Test: 71.05%\n",
      "Run: 03, Epoch: 70, Loss: 0.7571, Train: 78.11%, Valid: 71.33% Test: 71.49%\n",
      "Run: 03, Epoch: 71, Loss: 0.7792, Train: 79.03%, Valid: 71.60% Test: 71.49%\n",
      "Run: 03, Epoch: 72, Loss: 0.7373, Train: 78.75%, Valid: 70.51% Test: 71.49%\n",
      "Run: 03, Epoch: 73, Loss: 0.7537, Train: 80.13%, Valid: 72.02% Test: 70.83%\n",
      "Run: 03, Epoch: 74, Loss: 0.7428, Train: 79.76%, Valid: 73.66% Test: 72.81%\n",
      "Run: 03, Epoch: 75, Loss: 0.7500, Train: 77.93%, Valid: 72.70% Test: 71.71%\n",
      "Run: 03, Epoch: 76, Loss: 0.7488, Train: 76.65%, Valid: 71.60% Test: 70.39%\n",
      "Run: 03, Epoch: 77, Loss: 0.7532, Train: 78.66%, Valid: 72.02% Test: 70.61%\n",
      "Run: 03, Epoch: 78, Loss: 0.7670, Train: 79.49%, Valid: 72.29% Test: 70.83%\n",
      "Run: 03, Epoch: 79, Loss: 0.7257, Train: 77.93%, Valid: 70.10% Test: 70.39%\n",
      "Run: 03, Epoch: 80, Loss: 0.7582, Train: 79.03%, Valid: 70.92% Test: 71.49%\n",
      "Run: 03, Epoch: 81, Loss: 0.7268, Train: 80.04%, Valid: 73.25% Test: 74.34%\n",
      "Run: 03, Epoch: 82, Loss: 0.7028, Train: 81.68%, Valid: 74.21% Test: 75.00%\n",
      "Run: 03, Epoch: 83, Loss: 0.7040, Train: 81.59%, Valid: 73.80% Test: 74.78%\n",
      "Run: 03, Epoch: 84, Loss: 0.7167, Train: 82.05%, Valid: 74.62% Test: 75.66%\n",
      "Run: 03, Epoch: 85, Loss: 0.7133, Train: 82.60%, Valid: 75.58% Test: 75.00%\n",
      "Run: 03, Epoch: 86, Loss: 0.6975, Train: 82.33%, Valid: 74.62% Test: 73.25%\n",
      "Run: 03, Epoch: 87, Loss: 0.7131, Train: 81.59%, Valid: 73.80% Test: 72.81%\n",
      "Run: 03, Epoch: 88, Loss: 0.7262, Train: 81.78%, Valid: 73.94% Test: 75.00%\n",
      "Run: 03, Epoch: 89, Loss: 0.6962, Train: 80.68%, Valid: 73.11% Test: 73.90%\n",
      "Run: 03, Epoch: 90, Loss: 0.7279, Train: 81.41%, Valid: 72.70% Test: 75.22%\n",
      "Run: 03, Epoch: 91, Loss: 0.6945, Train: 82.42%, Valid: 74.62% Test: 74.34%\n",
      "Run: 03, Epoch: 92, Loss: 0.7141, Train: 80.04%, Valid: 72.29% Test: 73.68%\n",
      "Run: 03, Epoch: 93, Loss: 0.6967, Train: 78.39%, Valid: 70.51% Test: 71.27%\n",
      "Run: 03, Epoch: 94, Loss: 0.6866, Train: 79.03%, Valid: 72.02% Test: 71.93%\n",
      "Run: 03, Epoch: 95, Loss: 0.6732, Train: 80.13%, Valid: 74.21% Test: 72.15%\n",
      "Run: 03, Epoch: 96, Loss: 0.6958, Train: 80.40%, Valid: 73.94% Test: 74.34%\n",
      "Run: 03, Epoch: 97, Loss: 0.6682, Train: 80.95%, Valid: 72.84% Test: 75.44%\n",
      "Run: 03, Epoch: 98, Loss: 0.6978, Train: 82.88%, Valid: 74.76% Test: 75.88%\n",
      "Run: 03, Epoch: 99, Loss: 0.6471, Train: 82.23%, Valid: 74.35% Test: 74.56%\n",
      "Run: 03, Epoch: 100, Loss: 0.6925, Train: 81.68%, Valid: 74.35% Test: 73.25%\n",
      "Run 03:\n",
      "Highest Train: 82.88\n",
      "Highest Valid: 75.58\n",
      "  Final Train: 82.60\n",
      "   Final Test: 75.00\n",
      "Run: 04, Epoch: 01, Loss: 1.7953, Train: 26.47%, Valid: 24.97% Test: 25.44%\n",
      "Run: 04, Epoch: 02, Loss: 1.6023, Train: 25.92%, Valid: 25.24% Test: 23.25%\n",
      "Run: 04, Epoch: 03, Loss: 1.5167, Train: 24.82%, Valid: 24.42% Test: 23.25%\n",
      "Run: 04, Epoch: 04, Loss: 1.3991, Train: 27.11%, Valid: 27.16% Test: 26.54%\n",
      "Run: 04, Epoch: 05, Loss: 1.4024, Train: 31.41%, Valid: 32.65% Test: 30.70%\n",
      "Run: 04, Epoch: 06, Loss: 1.3662, Train: 34.52%, Valid: 35.12% Test: 36.18%\n",
      "Run: 04, Epoch: 07, Loss: 1.3213, Train: 36.90%, Valid: 38.13% Test: 39.25%\n",
      "Run: 04, Epoch: 08, Loss: 1.3087, Train: 38.55%, Valid: 39.78% Test: 41.67%\n",
      "Run: 04, Epoch: 09, Loss: 1.3020, Train: 41.30%, Valid: 42.39% Test: 42.76%\n",
      "Run: 04, Epoch: 10, Loss: 1.2808, Train: 45.60%, Valid: 44.99% Test: 47.37%\n",
      "Run: 04, Epoch: 11, Loss: 1.2493, Train: 49.08%, Valid: 48.56% Test: 47.15%\n",
      "Run: 04, Epoch: 12, Loss: 1.2492, Train: 53.21%, Valid: 49.25% Test: 51.54%\n",
      "Run: 04, Epoch: 13, Loss: 1.2149, Train: 51.83%, Valid: 47.87% Test: 50.66%\n",
      "Run: 04, Epoch: 14, Loss: 1.2141, Train: 51.74%, Valid: 48.01% Test: 51.10%\n",
      "Run: 04, Epoch: 15, Loss: 1.1999, Train: 51.56%, Valid: 48.29% Test: 53.51%\n",
      "Run: 04, Epoch: 16, Loss: 1.1964, Train: 51.65%, Valid: 47.74% Test: 51.97%\n",
      "Run: 04, Epoch: 17, Loss: 1.1628, Train: 51.74%, Valid: 47.60% Test: 51.10%\n",
      "Run: 04, Epoch: 18, Loss: 1.1867, Train: 52.20%, Valid: 48.42% Test: 51.10%\n",
      "Run: 04, Epoch: 19, Loss: 1.1218, Train: 52.56%, Valid: 49.79% Test: 51.75%\n",
      "Run: 04, Epoch: 20, Loss: 1.1201, Train: 52.29%, Valid: 49.38% Test: 52.19%\n",
      "Run: 04, Epoch: 21, Loss: 1.1115, Train: 51.83%, Valid: 48.70% Test: 51.75%\n",
      "Run: 04, Epoch: 22, Loss: 1.1037, Train: 53.11%, Valid: 49.93% Test: 53.51%\n",
      "Run: 04, Epoch: 23, Loss: 1.1085, Train: 54.95%, Valid: 50.21% Test: 54.39%\n",
      "Run: 04, Epoch: 24, Loss: 1.0951, Train: 54.49%, Valid: 49.52% Test: 55.70%\n",
      "Run: 04, Epoch: 25, Loss: 1.0860, Train: 54.40%, Valid: 49.79% Test: 55.48%\n",
      "Run: 04, Epoch: 26, Loss: 1.0603, Train: 53.85%, Valid: 51.03% Test: 54.61%\n",
      "Run: 04, Epoch: 27, Loss: 1.0725, Train: 54.21%, Valid: 53.50% Test: 56.36%\n",
      "Run: 04, Epoch: 28, Loss: 1.0591, Train: 57.33%, Valid: 54.32% Test: 56.80%\n",
      "Run: 04, Epoch: 29, Loss: 1.0244, Train: 60.26%, Valid: 56.93% Test: 60.09%\n",
      "Run: 04, Epoch: 30, Loss: 1.0019, Train: 60.99%, Valid: 57.48% Test: 60.31%\n",
      "Run: 04, Epoch: 31, Loss: 1.0313, Train: 60.99%, Valid: 57.20% Test: 61.18%\n",
      "Run: 04, Epoch: 32, Loss: 1.0071, Train: 60.62%, Valid: 57.61% Test: 60.75%\n",
      "Run: 04, Epoch: 33, Loss: 1.0084, Train: 60.26%, Valid: 56.79% Test: 60.75%\n",
      "Run: 04, Epoch: 34, Loss: 0.9945, Train: 60.71%, Valid: 56.79% Test: 59.87%\n",
      "Run: 04, Epoch: 35, Loss: 1.0054, Train: 62.45%, Valid: 58.85% Test: 60.75%\n",
      "Run: 04, Epoch: 36, Loss: 0.9770, Train: 64.10%, Valid: 60.49% Test: 63.38%\n",
      "Run: 04, Epoch: 37, Loss: 0.9577, Train: 65.11%, Valid: 62.96% Test: 64.91%\n",
      "Run: 04, Epoch: 38, Loss: 0.9575, Train: 66.30%, Valid: 63.79% Test: 65.57%\n",
      "Run: 04, Epoch: 39, Loss: 0.9565, Train: 67.31%, Valid: 63.51% Test: 64.91%\n",
      "Run: 04, Epoch: 40, Loss: 0.9347, Train: 66.76%, Valid: 62.55% Test: 62.94%\n",
      "Run: 04, Epoch: 41, Loss: 0.8923, Train: 66.03%, Valid: 61.32% Test: 64.04%\n",
      "Run: 04, Epoch: 42, Loss: 0.9352, Train: 66.30%, Valid: 61.73% Test: 63.16%\n",
      "Run: 04, Epoch: 43, Loss: 0.9408, Train: 67.22%, Valid: 63.10% Test: 63.38%\n",
      "Run: 04, Epoch: 44, Loss: 0.9470, Train: 67.77%, Valid: 64.88% Test: 65.35%\n",
      "Run: 04, Epoch: 45, Loss: 0.9214, Train: 69.69%, Valid: 66.12% Test: 65.57%\n",
      "Run: 04, Epoch: 46, Loss: 0.9176, Train: 70.60%, Valid: 67.22% Test: 66.23%\n",
      "Run: 04, Epoch: 47, Loss: 0.8998, Train: 71.06%, Valid: 67.63% Test: 67.54%\n",
      "Run: 04, Epoch: 48, Loss: 0.9067, Train: 70.97%, Valid: 68.04% Test: 67.32%\n",
      "Run: 04, Epoch: 49, Loss: 0.8975, Train: 71.98%, Valid: 67.63% Test: 67.98%\n",
      "Run: 04, Epoch: 50, Loss: 0.9054, Train: 71.43%, Valid: 67.22% Test: 67.76%\n",
      "Run: 04, Epoch: 51, Loss: 0.8655, Train: 70.42%, Valid: 67.35% Test: 67.76%\n",
      "Run: 04, Epoch: 52, Loss: 0.8647, Train: 70.79%, Valid: 68.18% Test: 68.64%\n",
      "Run: 04, Epoch: 53, Loss: 0.8603, Train: 72.25%, Valid: 69.14% Test: 69.30%\n",
      "Run: 04, Epoch: 54, Loss: 0.8700, Train: 73.08%, Valid: 70.37% Test: 70.61%\n",
      "Run: 04, Epoch: 55, Loss: 0.8356, Train: 72.99%, Valid: 69.82% Test: 71.05%\n",
      "Run: 04, Epoch: 56, Loss: 0.8398, Train: 72.89%, Valid: 69.00% Test: 70.83%\n",
      "Run: 04, Epoch: 57, Loss: 0.8434, Train: 72.44%, Valid: 68.59% Test: 70.39%\n",
      "Run: 04, Epoch: 58, Loss: 0.8473, Train: 72.89%, Valid: 69.00% Test: 70.61%\n",
      "Run: 04, Epoch: 59, Loss: 0.8463, Train: 74.54%, Valid: 69.55% Test: 70.39%\n",
      "Run: 04, Epoch: 60, Loss: 0.8460, Train: 73.26%, Valid: 69.82% Test: 69.52%\n",
      "Run: 04, Epoch: 61, Loss: 0.7975, Train: 72.89%, Valid: 69.27% Test: 70.39%\n",
      "Run: 04, Epoch: 62, Loss: 0.8172, Train: 73.44%, Valid: 70.10% Test: 71.71%\n",
      "Run: 04, Epoch: 63, Loss: 0.8439, Train: 74.27%, Valid: 70.51% Test: 72.59%\n",
      "Run: 04, Epoch: 64, Loss: 0.7924, Train: 75.55%, Valid: 71.33% Test: 73.46%\n",
      "Run: 04, Epoch: 65, Loss: 0.8125, Train: 75.55%, Valid: 70.51% Test: 73.25%\n",
      "Run: 04, Epoch: 66, Loss: 0.8134, Train: 75.18%, Valid: 70.51% Test: 71.93%\n",
      "Run: 04, Epoch: 67, Loss: 0.8047, Train: 75.09%, Valid: 71.33% Test: 71.49%\n",
      "Run: 04, Epoch: 68, Loss: 0.8119, Train: 76.37%, Valid: 72.98% Test: 72.81%\n",
      "Run: 04, Epoch: 69, Loss: 0.7856, Train: 75.92%, Valid: 74.35% Test: 75.22%\n",
      "Run: 04, Epoch: 70, Loss: 0.8020, Train: 75.73%, Valid: 71.74% Test: 75.22%\n",
      "Run: 04, Epoch: 71, Loss: 0.7867, Train: 76.10%, Valid: 71.47% Test: 75.66%\n",
      "Run: 04, Epoch: 72, Loss: 0.7841, Train: 76.47%, Valid: 71.88% Test: 75.22%\n",
      "Run: 04, Epoch: 73, Loss: 0.7736, Train: 76.47%, Valid: 72.43% Test: 75.00%\n",
      "Run: 04, Epoch: 74, Loss: 0.7633, Train: 75.92%, Valid: 72.98% Test: 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 04, Epoch: 75, Loss: 0.7546, Train: 75.64%, Valid: 73.11% Test: 75.44%\n",
      "Run: 04, Epoch: 76, Loss: 0.7684, Train: 75.92%, Valid: 73.66% Test: 75.66%\n",
      "Run: 04, Epoch: 77, Loss: 0.7957, Train: 76.28%, Valid: 72.70% Test: 76.32%\n",
      "Run: 04, Epoch: 78, Loss: 0.7673, Train: 76.10%, Valid: 70.92% Test: 75.44%\n",
      "Run: 04, Epoch: 79, Loss: 0.7739, Train: 75.64%, Valid: 71.74% Test: 73.90%\n",
      "Run: 04, Epoch: 80, Loss: 0.7790, Train: 76.83%, Valid: 71.60% Test: 73.25%\n",
      "Run: 04, Epoch: 81, Loss: 0.7418, Train: 76.37%, Valid: 71.06% Test: 73.46%\n",
      "Run: 04, Epoch: 82, Loss: 0.7541, Train: 76.47%, Valid: 74.21% Test: 75.22%\n",
      "Run: 04, Epoch: 83, Loss: 0.7639, Train: 76.56%, Valid: 73.39% Test: 76.75%\n",
      "Run: 04, Epoch: 84, Loss: 0.7512, Train: 77.56%, Valid: 75.03% Test: 77.19%\n",
      "Run: 04, Epoch: 85, Loss: 0.7633, Train: 78.11%, Valid: 75.58% Test: 78.95%\n",
      "Run: 04, Epoch: 86, Loss: 0.7437, Train: 78.11%, Valid: 74.35% Test: 78.51%\n",
      "Run: 04, Epoch: 87, Loss: 0.7190, Train: 77.20%, Valid: 72.15% Test: 77.41%\n",
      "Run: 04, Epoch: 88, Loss: 0.7434, Train: 77.75%, Valid: 73.39% Test: 77.19%\n",
      "Run: 04, Epoch: 89, Loss: 0.7226, Train: 77.84%, Valid: 74.62% Test: 77.19%\n",
      "Run: 04, Epoch: 90, Loss: 0.7348, Train: 77.93%, Valid: 73.94% Test: 76.32%\n",
      "Run: 04, Epoch: 91, Loss: 0.7105, Train: 78.02%, Valid: 73.11% Test: 75.66%\n",
      "Run: 04, Epoch: 92, Loss: 0.6990, Train: 78.48%, Valid: 74.07% Test: 76.54%\n",
      "Run: 04, Epoch: 93, Loss: 0.7091, Train: 78.21%, Valid: 73.66% Test: 76.75%\n",
      "Run: 04, Epoch: 94, Loss: 0.7043, Train: 77.56%, Valid: 73.80% Test: 76.10%\n",
      "Run: 04, Epoch: 95, Loss: 0.7094, Train: 78.30%, Valid: 73.11% Test: 77.63%\n",
      "Run: 04, Epoch: 96, Loss: 0.7135, Train: 78.94%, Valid: 73.94% Test: 78.51%\n",
      "Run: 04, Epoch: 97, Loss: 0.7080, Train: 78.94%, Valid: 73.94% Test: 77.19%\n",
      "Run: 04, Epoch: 98, Loss: 0.7045, Train: 77.84%, Valid: 74.21% Test: 76.32%\n",
      "Run: 04, Epoch: 99, Loss: 0.6976, Train: 76.92%, Valid: 74.07% Test: 75.00%\n",
      "Run: 04, Epoch: 100, Loss: 0.6801, Train: 76.83%, Valid: 72.84% Test: 74.34%\n",
      "Run 04:\n",
      "Highest Train: 78.94\n",
      "Highest Valid: 75.58\n",
      "  Final Train: 78.11\n",
      "   Final Test: 78.95\n",
      "Run: 05, Epoch: 01, Loss: 1.9447, Train: 31.78%, Valid: 32.65% Test: 39.25%\n",
      "Run: 05, Epoch: 02, Loss: 1.6681, Train: 25.73%, Valid: 27.98% Test: 26.75%\n",
      "Run: 05, Epoch: 03, Loss: 1.5190, Train: 26.65%, Valid: 29.22% Test: 30.26%\n",
      "Run: 05, Epoch: 04, Loss: 1.4416, Train: 30.49%, Valid: 29.08% Test: 29.17%\n",
      "Run: 05, Epoch: 05, Loss: 1.3863, Train: 33.33%, Valid: 31.00% Test: 31.80%\n",
      "Run: 05, Epoch: 06, Loss: 1.3660, Train: 35.16%, Valid: 31.41% Test: 34.21%\n",
      "Run: 05, Epoch: 07, Loss: 1.3518, Train: 35.99%, Valid: 33.06% Test: 35.09%\n",
      "Run: 05, Epoch: 08, Loss: 1.3128, Train: 37.36%, Valid: 35.67% Test: 35.96%\n",
      "Run: 05, Epoch: 09, Loss: 1.3090, Train: 41.85%, Valid: 39.78% Test: 37.72%\n",
      "Run: 05, Epoch: 10, Loss: 1.2984, Train: 44.05%, Valid: 43.62% Test: 39.91%\n",
      "Run: 05, Epoch: 11, Loss: 1.2706, Train: 44.51%, Valid: 44.31% Test: 41.45%\n",
      "Run: 05, Epoch: 12, Loss: 1.2686, Train: 46.06%, Valid: 44.58% Test: 39.91%\n",
      "Run: 05, Epoch: 13, Loss: 1.2204, Train: 47.89%, Valid: 43.48% Test: 42.11%\n",
      "Run: 05, Epoch: 14, Loss: 1.2025, Train: 49.63%, Valid: 47.74% Test: 47.15%\n",
      "Run: 05, Epoch: 15, Loss: 1.2050, Train: 51.56%, Valid: 48.56% Test: 49.78%\n",
      "Run: 05, Epoch: 16, Loss: 1.2130, Train: 52.75%, Valid: 50.89% Test: 52.85%\n",
      "Run: 05, Epoch: 17, Loss: 1.1886, Train: 52.29%, Valid: 50.89% Test: 51.10%\n",
      "Run: 05, Epoch: 18, Loss: 1.1920, Train: 52.29%, Valid: 51.17% Test: 51.97%\n",
      "Run: 05, Epoch: 19, Loss: 1.1523, Train: 53.21%, Valid: 51.99% Test: 51.97%\n",
      "Run: 05, Epoch: 20, Loss: 1.1582, Train: 53.11%, Valid: 53.64% Test: 54.17%\n",
      "Run: 05, Epoch: 21, Loss: 1.1605, Train: 53.21%, Valid: 52.95% Test: 54.82%\n",
      "Run: 05, Epoch: 22, Loss: 1.1200, Train: 53.21%, Valid: 52.54% Test: 53.29%\n",
      "Run: 05, Epoch: 23, Loss: 1.1271, Train: 51.47%, Valid: 47.87% Test: 49.34%\n",
      "Run: 05, Epoch: 24, Loss: 1.1015, Train: 48.26%, Valid: 46.23% Test: 48.03%\n",
      "Run: 05, Epoch: 25, Loss: 1.1086, Train: 49.82%, Valid: 48.01% Test: 48.90%\n",
      "Run: 05, Epoch: 26, Loss: 1.1128, Train: 53.02%, Valid: 52.13% Test: 50.66%\n",
      "Run: 05, Epoch: 27, Loss: 1.0802, Train: 56.04%, Valid: 55.28% Test: 54.39%\n",
      "Run: 05, Epoch: 28, Loss: 1.0767, Train: 56.59%, Valid: 55.42% Test: 56.14%\n",
      "Run: 05, Epoch: 29, Loss: 1.0721, Train: 56.68%, Valid: 55.14% Test: 54.82%\n",
      "Run: 05, Epoch: 30, Loss: 1.0430, Train: 55.68%, Valid: 55.01% Test: 55.04%\n",
      "Run: 05, Epoch: 31, Loss: 1.0547, Train: 57.60%, Valid: 55.83% Test: 53.73%\n",
      "Run: 05, Epoch: 32, Loss: 1.0129, Train: 58.61%, Valid: 57.89% Test: 55.48%\n",
      "Run: 05, Epoch: 33, Loss: 1.0077, Train: 62.27%, Valid: 58.71% Test: 58.33%\n",
      "Run: 05, Epoch: 34, Loss: 1.0061, Train: 62.55%, Valid: 59.67% Test: 61.18%\n",
      "Run: 05, Epoch: 35, Loss: 1.0064, Train: 63.00%, Valid: 59.95% Test: 61.62%\n",
      "Run: 05, Epoch: 36, Loss: 0.9992, Train: 62.27%, Valid: 59.40% Test: 60.31%\n",
      "Run: 05, Epoch: 37, Loss: 0.9996, Train: 63.10%, Valid: 59.40% Test: 60.53%\n",
      "Run: 05, Epoch: 38, Loss: 0.9805, Train: 61.45%, Valid: 57.48% Test: 55.70%\n",
      "Run: 05, Epoch: 39, Loss: 0.9561, Train: 62.36%, Valid: 59.40% Test: 55.26%\n",
      "Run: 05, Epoch: 40, Loss: 0.9274, Train: 64.65%, Valid: 60.08% Test: 56.80%\n",
      "Run: 05, Epoch: 41, Loss: 0.9383, Train: 66.94%, Valid: 62.41% Test: 62.72%\n",
      "Run: 05, Epoch: 42, Loss: 0.9344, Train: 66.48%, Valid: 63.37% Test: 63.82%\n",
      "Run: 05, Epoch: 43, Loss: 0.9227, Train: 66.12%, Valid: 62.83% Test: 65.57%\n",
      "Run: 05, Epoch: 44, Loss: 0.8937, Train: 66.48%, Valid: 62.69% Test: 65.57%\n",
      "Run: 05, Epoch: 45, Loss: 0.9016, Train: 66.39%, Valid: 61.45% Test: 65.35%\n",
      "Run: 05, Epoch: 46, Loss: 0.9290, Train: 67.03%, Valid: 62.96% Test: 66.67%\n",
      "Run: 05, Epoch: 47, Loss: 0.9179, Train: 66.48%, Valid: 64.47% Test: 67.11%\n",
      "Run: 05, Epoch: 48, Loss: 0.8787, Train: 67.31%, Valid: 65.29% Test: 67.76%\n",
      "Run: 05, Epoch: 49, Loss: 0.8689, Train: 68.13%, Valid: 66.26% Test: 69.30%\n",
      "Run: 05, Epoch: 50, Loss: 0.8749, Train: 67.22%, Valid: 65.02% Test: 69.52%\n",
      "Run: 05, Epoch: 51, Loss: 0.8744, Train: 66.21%, Valid: 64.06% Test: 67.54%\n",
      "Run: 05, Epoch: 52, Loss: 0.8574, Train: 64.84%, Valid: 63.10% Test: 66.01%\n",
      "Run: 05, Epoch: 53, Loss: 0.8817, Train: 66.85%, Valid: 65.02% Test: 67.54%\n",
      "Run: 05, Epoch: 54, Loss: 0.8690, Train: 70.33%, Valid: 66.67% Test: 69.08%\n",
      "Run: 05, Epoch: 55, Loss: 0.8599, Train: 72.34%, Valid: 67.76% Test: 69.08%\n",
      "Run: 05, Epoch: 56, Loss: 0.8416, Train: 70.79%, Valid: 67.22% Test: 68.20%\n",
      "Run: 05, Epoch: 57, Loss: 0.8385, Train: 71.25%, Valid: 66.39% Test: 67.32%\n",
      "Run: 05, Epoch: 58, Loss: 0.8291, Train: 73.26%, Valid: 69.00% Test: 71.05%\n",
      "Run: 05, Epoch: 59, Loss: 0.8432, Train: 72.62%, Valid: 69.14% Test: 73.03%\n",
      "Run: 05, Epoch: 60, Loss: 0.8312, Train: 72.07%, Valid: 69.96% Test: 72.81%\n",
      "Run: 05, Epoch: 61, Loss: 0.8133, Train: 73.81%, Valid: 71.06% Test: 72.81%\n",
      "Run: 05, Epoch: 62, Loss: 0.8562, Train: 73.63%, Valid: 70.10% Test: 72.37%\n",
      "Run: 05, Epoch: 63, Loss: 0.8059, Train: 73.44%, Valid: 69.96% Test: 71.27%\n",
      "Run: 05, Epoch: 64, Loss: 0.8462, Train: 73.81%, Valid: 71.19% Test: 71.93%\n",
      "Run: 05, Epoch: 65, Loss: 0.8336, Train: 75.46%, Valid: 73.80% Test: 74.12%\n",
      "Run: 05, Epoch: 66, Loss: 0.8396, Train: 74.73%, Valid: 72.57% Test: 75.44%\n",
      "Run: 05, Epoch: 67, Loss: 0.8234, Train: 74.73%, Valid: 72.15% Test: 75.00%\n",
      "Run: 05, Epoch: 68, Loss: 0.8072, Train: 74.36%, Valid: 71.47% Test: 74.12%\n",
      "Run: 05, Epoch: 69, Loss: 0.8120, Train: 75.82%, Valid: 72.57% Test: 75.00%\n",
      "Run: 05, Epoch: 70, Loss: 0.7945, Train: 75.09%, Valid: 72.02% Test: 73.03%\n",
      "Run: 05, Epoch: 71, Loss: 0.7898, Train: 76.10%, Valid: 71.47% Test: 74.56%\n",
      "Run: 05, Epoch: 72, Loss: 0.7661, Train: 77.01%, Valid: 73.53% Test: 74.34%\n",
      "Run: 05, Epoch: 73, Loss: 0.7741, Train: 76.92%, Valid: 74.07% Test: 76.10%\n",
      "Run: 05, Epoch: 74, Loss: 0.7833, Train: 76.83%, Valid: 74.35% Test: 74.12%\n",
      "Run: 05, Epoch: 75, Loss: 0.7786, Train: 76.10%, Valid: 73.39% Test: 73.90%\n",
      "Run: 05, Epoch: 76, Loss: 0.8087, Train: 75.27%, Valid: 73.66% Test: 74.34%\n",
      "Run: 05, Epoch: 77, Loss: 0.7876, Train: 75.00%, Valid: 72.43% Test: 74.78%\n",
      "Run: 05, Epoch: 78, Loss: 0.7934, Train: 74.27%, Valid: 70.37% Test: 71.71%\n",
      "Run: 05, Epoch: 79, Loss: 0.7731, Train: 74.82%, Valid: 71.60% Test: 72.37%\n",
      "Run: 05, Epoch: 80, Loss: 0.7707, Train: 76.28%, Valid: 73.80% Test: 74.56%\n",
      "Run: 05, Epoch: 81, Loss: 0.7892, Train: 74.54%, Valid: 72.57% Test: 73.68%\n",
      "Run: 05, Epoch: 82, Loss: 0.7434, Train: 75.64%, Valid: 73.25% Test: 73.46%\n",
      "Run: 05, Epoch: 83, Loss: 0.7392, Train: 77.38%, Valid: 75.72% Test: 73.46%\n",
      "Run: 05, Epoch: 84, Loss: 0.7655, Train: 77.38%, Valid: 73.94% Test: 73.68%\n",
      "Run: 05, Epoch: 85, Loss: 0.7544, Train: 78.39%, Valid: 73.39% Test: 74.34%\n",
      "Run: 05, Epoch: 86, Loss: 0.7661, Train: 77.84%, Valid: 73.94% Test: 74.56%\n",
      "Run: 05, Epoch: 87, Loss: 0.7862, Train: 77.84%, Valid: 73.66% Test: 76.54%\n",
      "Run: 05, Epoch: 88, Loss: 0.7482, Train: 77.93%, Valid: 73.80% Test: 76.32%\n",
      "Run: 05, Epoch: 89, Loss: 0.7414, Train: 76.92%, Valid: 72.98% Test: 75.44%\n",
      "Run: 05, Epoch: 90, Loss: 0.7517, Train: 76.74%, Valid: 73.66% Test: 75.66%\n",
      "Run: 05, Epoch: 91, Loss: 0.7197, Train: 77.38%, Valid: 74.76% Test: 76.32%\n",
      "Run: 05, Epoch: 92, Loss: 0.7653, Train: 77.38%, Valid: 74.76% Test: 76.75%\n",
      "Run: 05, Epoch: 93, Loss: 0.7222, Train: 77.01%, Valid: 74.07% Test: 75.88%\n",
      "Run: 05, Epoch: 94, Loss: 0.7615, Train: 78.21%, Valid: 74.90% Test: 75.66%\n",
      "Run: 05, Epoch: 95, Loss: 0.7627, Train: 77.93%, Valid: 74.49% Test: 75.00%\n",
      "Run: 05, Epoch: 96, Loss: 0.7526, Train: 77.38%, Valid: 75.45% Test: 76.54%\n",
      "Run: 05, Epoch: 97, Loss: 0.7265, Train: 77.84%, Valid: 75.03% Test: 74.78%\n",
      "Run: 05, Epoch: 98, Loss: 0.7157, Train: 78.11%, Valid: 75.58% Test: 74.12%\n",
      "Run: 05, Epoch: 99, Loss: 0.7510, Train: 78.75%, Valid: 75.45% Test: 75.22%\n",
      "Run: 05, Epoch: 100, Loss: 0.7112, Train: 78.11%, Valid: 75.31% Test: 74.78%\n",
      "Run 05:\n",
      "Highest Train: 78.75\n",
      "Highest Valid: 75.72\n",
      "  Final Train: 77.38\n",
      "   Final Test: 73.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 06, Epoch: 01, Loss: 1.7793, Train: 19.05%, Valid: 19.07% Test: 20.39%\n",
      "Run: 06, Epoch: 02, Loss: 1.6061, Train: 24.18%, Valid: 24.28% Test: 25.00%\n",
      "Run: 06, Epoch: 03, Loss: 1.4736, Train: 26.56%, Valid: 26.06% Test: 29.39%\n",
      "Run: 06, Epoch: 04, Loss: 1.4375, Train: 26.19%, Valid: 27.71% Test: 30.04%\n",
      "Run: 06, Epoch: 05, Loss: 1.3963, Train: 30.04%, Valid: 31.96% Test: 35.53%\n",
      "Run: 06, Epoch: 06, Loss: 1.3974, Train: 36.08%, Valid: 36.63% Test: 39.91%\n",
      "Run: 06, Epoch: 07, Loss: 1.3449, Train: 43.04%, Valid: 42.94% Test: 44.74%\n",
      "Run: 06, Epoch: 08, Loss: 1.3514, Train: 48.90%, Valid: 46.91% Test: 46.05%\n",
      "Run: 06, Epoch: 09, Loss: 1.3116, Train: 51.37%, Valid: 47.87% Test: 46.93%\n",
      "Run: 06, Epoch: 10, Loss: 1.2945, Train: 50.37%, Valid: 48.56% Test: 48.68%\n",
      "Run: 06, Epoch: 11, Loss: 1.2816, Train: 49.27%, Valid: 49.11% Test: 50.44%\n",
      "Run: 06, Epoch: 12, Loss: 1.2752, Train: 51.37%, Valid: 49.52% Test: 53.07%\n",
      "Run: 06, Epoch: 13, Loss: 1.2349, Train: 51.83%, Valid: 50.75% Test: 53.29%\n",
      "Run: 06, Epoch: 14, Loss: 1.2295, Train: 53.30%, Valid: 51.44% Test: 53.07%\n",
      "Run: 06, Epoch: 15, Loss: 1.1872, Train: 52.56%, Valid: 50.34% Test: 52.63%\n",
      "Run: 06, Epoch: 16, Loss: 1.2084, Train: 52.01%, Valid: 48.42% Test: 52.41%\n",
      "Run: 06, Epoch: 17, Loss: 1.1721, Train: 49.73%, Valid: 48.70% Test: 53.95%\n",
      "Run: 06, Epoch: 18, Loss: 1.1726, Train: 49.08%, Valid: 46.78% Test: 52.63%\n",
      "Run: 06, Epoch: 19, Loss: 1.1740, Train: 49.54%, Valid: 47.46% Test: 54.61%\n",
      "Run: 06, Epoch: 20, Loss: 1.1490, Train: 50.73%, Valid: 47.87% Test: 54.61%\n",
      "Run: 06, Epoch: 21, Loss: 1.1315, Train: 51.92%, Valid: 50.34% Test: 54.82%\n",
      "Run: 06, Epoch: 22, Loss: 1.1178, Train: 53.11%, Valid: 51.99% Test: 55.48%\n",
      "Run: 06, Epoch: 23, Loss: 1.1105, Train: 53.94%, Valid: 52.81% Test: 57.89%\n",
      "Run: 06, Epoch: 24, Loss: 1.1055, Train: 54.30%, Valid: 52.54% Test: 58.55%\n",
      "Run: 06, Epoch: 25, Loss: 1.0902, Train: 53.48%, Valid: 51.99% Test: 58.11%\n",
      "Run: 06, Epoch: 26, Loss: 1.0757, Train: 54.67%, Valid: 50.75% Test: 56.80%\n",
      "Run: 06, Epoch: 27, Loss: 1.0408, Train: 56.14%, Valid: 52.26% Test: 56.14%\n",
      "Run: 06, Epoch: 28, Loss: 1.0528, Train: 57.42%, Valid: 54.32% Test: 58.77%\n",
      "Run: 06, Epoch: 29, Loss: 1.0195, Train: 58.61%, Valid: 55.69% Test: 59.87%\n",
      "Run: 06, Epoch: 30, Loss: 1.0335, Train: 58.42%, Valid: 56.38% Test: 57.24%\n",
      "Run: 06, Epoch: 31, Loss: 1.0353, Train: 59.52%, Valid: 57.75% Test: 58.99%\n",
      "Run: 06, Epoch: 32, Loss: 1.0308, Train: 60.26%, Valid: 58.30% Test: 58.77%\n",
      "Run: 06, Epoch: 33, Loss: 0.9741, Train: 59.80%, Valid: 57.06% Test: 56.80%\n",
      "Run: 06, Epoch: 34, Loss: 0.9973, Train: 57.51%, Valid: 55.56% Test: 56.80%\n",
      "Run: 06, Epoch: 35, Loss: 0.9837, Train: 57.14%, Valid: 54.60% Test: 57.89%\n",
      "Run: 06, Epoch: 36, Loss: 0.9783, Train: 58.97%, Valid: 56.52% Test: 58.55%\n",
      "Run: 06, Epoch: 37, Loss: 0.9691, Train: 59.16%, Valid: 56.93% Test: 58.77%\n",
      "Run: 06, Epoch: 38, Loss: 0.9732, Train: 61.26%, Valid: 60.91% Test: 59.43%\n",
      "Run: 06, Epoch: 39, Loss: 0.9404, Train: 63.55%, Valid: 62.83% Test: 61.84%\n",
      "Run: 06, Epoch: 40, Loss: 0.9455, Train: 65.57%, Valid: 62.00% Test: 63.38%\n",
      "Run: 06, Epoch: 41, Loss: 0.9152, Train: 65.20%, Valid: 62.83% Test: 62.28%\n",
      "Run: 06, Epoch: 42, Loss: 0.9507, Train: 65.57%, Valid: 62.69% Test: 62.72%\n",
      "Run: 06, Epoch: 43, Loss: 0.9434, Train: 65.02%, Valid: 63.65% Test: 62.94%\n",
      "Run: 06, Epoch: 44, Loss: 0.9283, Train: 64.47%, Valid: 62.96% Test: 62.72%\n",
      "Run: 06, Epoch: 45, Loss: 0.9224, Train: 66.12%, Valid: 64.33% Test: 65.13%\n",
      "Run: 06, Epoch: 46, Loss: 0.9210, Train: 68.22%, Valid: 66.67% Test: 67.76%\n",
      "Run: 06, Epoch: 47, Loss: 0.9050, Train: 69.23%, Valid: 68.45% Test: 69.08%\n",
      "Run: 06, Epoch: 48, Loss: 0.8694, Train: 69.78%, Valid: 67.49% Test: 69.74%\n",
      "Run: 06, Epoch: 49, Loss: 0.9033, Train: 70.15%, Valid: 67.35% Test: 69.52%\n",
      "Run: 06, Epoch: 50, Loss: 0.8661, Train: 69.87%, Valid: 67.90% Test: 70.83%\n",
      "Run: 06, Epoch: 51, Loss: 0.9222, Train: 70.70%, Valid: 69.55% Test: 69.74%\n",
      "Run: 06, Epoch: 52, Loss: 0.8391, Train: 71.34%, Valid: 69.00% Test: 70.83%\n",
      "Run: 06, Epoch: 53, Loss: 0.8456, Train: 71.43%, Valid: 69.27% Test: 68.42%\n",
      "Run: 06, Epoch: 54, Loss: 0.8445, Train: 71.52%, Valid: 69.82% Test: 67.98%\n",
      "Run: 06, Epoch: 55, Loss: 0.8796, Train: 72.89%, Valid: 70.64% Test: 70.18%\n",
      "Run: 06, Epoch: 56, Loss: 0.8322, Train: 74.45%, Valid: 71.60% Test: 73.03%\n",
      "Run: 06, Epoch: 57, Loss: 0.8444, Train: 75.00%, Valid: 71.74% Test: 74.34%\n",
      "Run: 06, Epoch: 58, Loss: 0.8339, Train: 74.63%, Valid: 72.84% Test: 73.25%\n",
      "Run: 06, Epoch: 59, Loss: 0.8516, Train: 75.37%, Valid: 72.84% Test: 73.03%\n",
      "Run: 06, Epoch: 60, Loss: 0.8327, Train: 75.55%, Valid: 72.84% Test: 72.59%\n",
      "Run: 06, Epoch: 61, Loss: 0.8292, Train: 75.92%, Valid: 72.98% Test: 73.03%\n",
      "Run: 06, Epoch: 62, Loss: 0.8052, Train: 75.46%, Valid: 72.43% Test: 72.59%\n",
      "Run: 06, Epoch: 63, Loss: 0.8119, Train: 75.18%, Valid: 72.29% Test: 72.81%\n",
      "Run: 06, Epoch: 64, Loss: 0.8132, Train: 75.18%, Valid: 71.47% Test: 73.25%\n",
      "Run: 06, Epoch: 65, Loss: 0.8174, Train: 72.53%, Valid: 69.27% Test: 70.39%\n",
      "Run: 06, Epoch: 66, Loss: 0.8063, Train: 74.36%, Valid: 70.37% Test: 70.83%\n",
      "Run: 06, Epoch: 67, Loss: 0.7647, Train: 77.01%, Valid: 73.25% Test: 74.78%\n",
      "Run: 06, Epoch: 68, Loss: 0.7863, Train: 78.02%, Valid: 73.25% Test: 75.88%\n",
      "Run: 06, Epoch: 69, Loss: 0.7623, Train: 78.02%, Valid: 74.49% Test: 76.54%\n",
      "Run: 06, Epoch: 70, Loss: 0.7699, Train: 78.21%, Valid: 74.49% Test: 77.19%\n",
      "Run: 06, Epoch: 71, Loss: 0.7567, Train: 79.03%, Valid: 74.62% Test: 76.32%\n",
      "Run: 06, Epoch: 72, Loss: 0.7639, Train: 77.66%, Valid: 74.35% Test: 76.32%\n",
      "Run: 06, Epoch: 73, Loss: 0.7704, Train: 77.01%, Valid: 74.07% Test: 76.32%\n",
      "Run: 06, Epoch: 74, Loss: 0.7665, Train: 77.56%, Valid: 73.66% Test: 75.00%\n",
      "Run: 06, Epoch: 75, Loss: 0.7488, Train: 78.30%, Valid: 74.21% Test: 75.66%\n",
      "Run: 06, Epoch: 76, Loss: 0.7385, Train: 78.94%, Valid: 75.86% Test: 75.88%\n",
      "Run: 06, Epoch: 77, Loss: 0.7684, Train: 79.40%, Valid: 75.99% Test: 77.85%\n",
      "Run: 06, Epoch: 78, Loss: 0.7816, Train: 78.75%, Valid: 75.17% Test: 76.32%\n",
      "Run: 06, Epoch: 79, Loss: 0.7701, Train: 79.30%, Valid: 74.49% Test: 74.34%\n",
      "Run: 06, Epoch: 80, Loss: 0.7408, Train: 79.12%, Valid: 74.21% Test: 74.34%\n",
      "Run: 06, Epoch: 81, Loss: 0.7642, Train: 78.66%, Valid: 73.94% Test: 76.10%\n",
      "Run: 06, Epoch: 82, Loss: 0.7398, Train: 78.75%, Valid: 74.49% Test: 77.41%\n",
      "Run: 06, Epoch: 83, Loss: 0.7538, Train: 79.30%, Valid: 75.72% Test: 77.63%\n",
      "Run: 06, Epoch: 84, Loss: 0.7434, Train: 79.67%, Valid: 75.03% Test: 79.17%\n",
      "Run: 06, Epoch: 85, Loss: 0.7309, Train: 79.12%, Valid: 74.49% Test: 77.85%\n",
      "Run: 06, Epoch: 86, Loss: 0.7294, Train: 79.21%, Valid: 75.31% Test: 76.97%\n",
      "Run: 06, Epoch: 87, Loss: 0.7285, Train: 78.66%, Valid: 75.45% Test: 75.44%\n",
      "Run: 06, Epoch: 88, Loss: 0.7266, Train: 77.93%, Valid: 74.07% Test: 73.68%\n",
      "Run: 06, Epoch: 89, Loss: 0.7472, Train: 79.85%, Valid: 75.31% Test: 78.07%\n",
      "Run: 06, Epoch: 90, Loss: 0.7124, Train: 79.21%, Valid: 75.17% Test: 78.51%\n",
      "Run: 06, Epoch: 91, Loss: 0.6876, Train: 78.94%, Valid: 76.13% Test: 78.07%\n",
      "Run: 06, Epoch: 92, Loss: 0.7275, Train: 80.22%, Valid: 76.27% Test: 78.29%\n",
      "Run: 06, Epoch: 93, Loss: 0.6950, Train: 78.94%, Valid: 75.99% Test: 75.22%\n",
      "Run: 06, Epoch: 94, Loss: 0.7020, Train: 80.04%, Valid: 75.58% Test: 75.88%\n",
      "Run: 06, Epoch: 95, Loss: 0.7186, Train: 78.66%, Valid: 74.76% Test: 75.44%\n",
      "Run: 06, Epoch: 96, Loss: 0.6958, Train: 78.48%, Valid: 74.76% Test: 75.66%\n",
      "Run: 06, Epoch: 97, Loss: 0.7215, Train: 79.40%, Valid: 76.13% Test: 79.17%\n",
      "Run: 06, Epoch: 98, Loss: 0.6824, Train: 80.31%, Valid: 76.68% Test: 79.39%\n",
      "Run: 06, Epoch: 99, Loss: 0.6964, Train: 79.67%, Valid: 76.68% Test: 78.51%\n",
      "Run: 06, Epoch: 100, Loss: 0.7351, Train: 79.85%, Valid: 76.95% Test: 77.19%\n",
      "Run 06:\n",
      "Highest Train: 80.31\n",
      "Highest Valid: 76.95\n",
      "  Final Train: 79.85\n",
      "   Final Test: 77.19\n",
      "Run: 07, Epoch: 01, Loss: 1.9220, Train: 17.77%, Valid: 17.70% Test: 16.45%\n",
      "Run: 07, Epoch: 02, Loss: 1.6795, Train: 24.08%, Valid: 23.73% Test: 20.61%\n",
      "Run: 07, Epoch: 03, Loss: 1.5193, Train: 27.20%, Valid: 29.49% Test: 25.88%\n",
      "Run: 07, Epoch: 04, Loss: 1.4821, Train: 26.37%, Valid: 28.81% Test: 25.00%\n",
      "Run: 07, Epoch: 05, Loss: 1.3918, Train: 28.66%, Valid: 29.63% Test: 26.54%\n",
      "Run: 07, Epoch: 06, Loss: 1.3767, Train: 34.43%, Valid: 36.35% Test: 35.31%\n",
      "Run: 07, Epoch: 07, Loss: 1.3058, Train: 39.74%, Valid: 40.74% Test: 39.91%\n",
      "Run: 07, Epoch: 08, Loss: 1.3029, Train: 44.51%, Valid: 44.31% Test: 44.30%\n",
      "Run: 07, Epoch: 09, Loss: 1.2982, Train: 50.27%, Valid: 47.60% Test: 47.15%\n",
      "Run: 07, Epoch: 10, Loss: 1.2533, Train: 52.47%, Valid: 50.21% Test: 48.68%\n",
      "Run: 07, Epoch: 11, Loss: 1.2138, Train: 51.37%, Valid: 49.38% Test: 47.81%\n",
      "Run: 07, Epoch: 12, Loss: 1.2313, Train: 51.28%, Valid: 48.97% Test: 45.83%\n",
      "Run: 07, Epoch: 13, Loss: 1.2359, Train: 53.75%, Valid: 50.21% Test: 48.25%\n",
      "Run: 07, Epoch: 14, Loss: 1.2448, Train: 55.95%, Valid: 52.81% Test: 49.34%\n",
      "Run: 07, Epoch: 15, Loss: 1.2028, Train: 55.95%, Valid: 54.46% Test: 51.75%\n",
      "Run: 07, Epoch: 16, Loss: 1.1749, Train: 56.32%, Valid: 54.46% Test: 50.66%\n",
      "Run: 07, Epoch: 17, Loss: 1.1378, Train: 54.40%, Valid: 52.54% Test: 50.00%\n",
      "Run: 07, Epoch: 18, Loss: 1.1397, Train: 55.31%, Valid: 53.64% Test: 49.34%\n",
      "Run: 07, Epoch: 19, Loss: 1.1444, Train: 57.05%, Valid: 54.32% Test: 51.10%\n",
      "Run: 07, Epoch: 20, Loss: 1.1257, Train: 57.33%, Valid: 56.24% Test: 52.41%\n",
      "Run: 07, Epoch: 21, Loss: 1.1057, Train: 57.05%, Valid: 57.48% Test: 53.07%\n",
      "Run: 07, Epoch: 22, Loss: 1.0960, Train: 56.96%, Valid: 56.93% Test: 52.85%\n",
      "Run: 07, Epoch: 23, Loss: 1.0502, Train: 57.23%, Valid: 57.48% Test: 51.54%\n",
      "Run: 07, Epoch: 24, Loss: 1.0566, Train: 58.06%, Valid: 57.75% Test: 52.85%\n",
      "Run: 07, Epoch: 25, Loss: 1.0667, Train: 59.89%, Valid: 58.57% Test: 55.48%\n",
      "Run: 07, Epoch: 26, Loss: 1.0321, Train: 62.91%, Valid: 59.81% Test: 58.99%\n",
      "Run: 07, Epoch: 27, Loss: 1.0574, Train: 64.29%, Valid: 59.95% Test: 59.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 07, Epoch: 28, Loss: 1.0075, Train: 64.47%, Valid: 59.95% Test: 61.40%\n",
      "Run: 07, Epoch: 29, Loss: 1.0264, Train: 64.56%, Valid: 59.81% Test: 60.75%\n",
      "Run: 07, Epoch: 30, Loss: 1.0109, Train: 64.01%, Valid: 59.95% Test: 61.62%\n",
      "Run: 07, Epoch: 31, Loss: 0.9790, Train: 63.00%, Valid: 59.67% Test: 61.18%\n",
      "Run: 07, Epoch: 32, Loss: 0.9735, Train: 62.82%, Valid: 59.26% Test: 59.87%\n",
      "Run: 07, Epoch: 33, Loss: 0.9607, Train: 61.08%, Valid: 59.53% Test: 59.43%\n",
      "Run: 07, Epoch: 34, Loss: 0.9453, Train: 61.45%, Valid: 59.53% Test: 60.09%\n",
      "Run: 07, Epoch: 35, Loss: 0.9330, Train: 62.64%, Valid: 59.53% Test: 61.18%\n",
      "Run: 07, Epoch: 36, Loss: 0.9147, Train: 62.73%, Valid: 59.53% Test: 61.84%\n",
      "Run: 07, Epoch: 37, Loss: 0.9265, Train: 62.91%, Valid: 60.22% Test: 60.96%\n",
      "Run: 07, Epoch: 38, Loss: 0.9327, Train: 63.46%, Valid: 61.45% Test: 63.60%\n",
      "Run: 07, Epoch: 39, Loss: 0.8972, Train: 65.20%, Valid: 62.96% Test: 66.01%\n",
      "Run: 07, Epoch: 40, Loss: 0.8738, Train: 67.95%, Valid: 64.61% Test: 66.89%\n",
      "Run: 07, Epoch: 41, Loss: 0.8972, Train: 69.32%, Valid: 66.53% Test: 66.45%\n",
      "Run: 07, Epoch: 42, Loss: 0.8789, Train: 70.79%, Valid: 67.49% Test: 65.57%\n",
      "Run: 07, Epoch: 43, Loss: 0.8888, Train: 70.88%, Valid: 67.63% Test: 66.89%\n",
      "Run: 07, Epoch: 44, Loss: 0.8823, Train: 71.25%, Valid: 67.49% Test: 68.86%\n",
      "Run: 07, Epoch: 45, Loss: 0.8465, Train: 73.26%, Valid: 68.31% Test: 67.98%\n",
      "Run: 07, Epoch: 46, Loss: 0.8631, Train: 74.18%, Valid: 69.68% Test: 69.52%\n",
      "Run: 07, Epoch: 47, Loss: 0.8633, Train: 73.26%, Valid: 69.41% Test: 68.64%\n",
      "Run: 07, Epoch: 48, Loss: 0.8443, Train: 73.26%, Valid: 69.82% Test: 69.08%\n",
      "Run: 07, Epoch: 49, Loss: 0.8576, Train: 73.26%, Valid: 68.31% Test: 69.08%\n",
      "Run: 07, Epoch: 50, Loss: 0.8436, Train: 72.89%, Valid: 66.26% Test: 68.42%\n",
      "Run: 07, Epoch: 51, Loss: 0.8373, Train: 71.61%, Valid: 65.84% Test: 68.20%\n",
      "Run: 07, Epoch: 52, Loss: 0.8457, Train: 75.00%, Valid: 68.45% Test: 72.37%\n",
      "Run: 07, Epoch: 53, Loss: 0.7898, Train: 75.64%, Valid: 68.18% Test: 73.03%\n",
      "Run: 07, Epoch: 54, Loss: 0.8496, Train: 74.73%, Valid: 70.78% Test: 72.37%\n",
      "Run: 07, Epoch: 55, Loss: 0.8182, Train: 76.65%, Valid: 70.78% Test: 72.15%\n",
      "Run: 07, Epoch: 56, Loss: 0.8081, Train: 73.72%, Valid: 70.64% Test: 71.49%\n",
      "Run: 07, Epoch: 57, Loss: 0.8171, Train: 73.72%, Valid: 70.37% Test: 71.05%\n",
      "Run: 07, Epoch: 58, Loss: 0.7993, Train: 75.37%, Valid: 70.51% Test: 73.68%\n",
      "Run: 07, Epoch: 59, Loss: 0.8056, Train: 75.64%, Valid: 71.47% Test: 73.46%\n",
      "Run: 07, Epoch: 60, Loss: 0.8096, Train: 75.46%, Valid: 70.37% Test: 72.37%\n",
      "Run: 07, Epoch: 61, Loss: 0.7747, Train: 74.91%, Valid: 70.51% Test: 72.59%\n",
      "Run: 07, Epoch: 62, Loss: 0.7941, Train: 74.36%, Valid: 70.10% Test: 72.37%\n",
      "Run: 07, Epoch: 63, Loss: 0.8067, Train: 74.73%, Valid: 70.10% Test: 71.49%\n",
      "Run: 07, Epoch: 64, Loss: 0.7760, Train: 76.65%, Valid: 70.64% Test: 72.59%\n",
      "Run: 07, Epoch: 65, Loss: 0.7995, Train: 78.21%, Valid: 72.98% Test: 73.25%\n",
      "Run: 07, Epoch: 66, Loss: 0.7859, Train: 77.47%, Valid: 72.98% Test: 73.90%\n",
      "Run: 07, Epoch: 67, Loss: 0.7914, Train: 77.56%, Valid: 73.53% Test: 74.56%\n",
      "Run: 07, Epoch: 68, Loss: 0.7588, Train: 77.38%, Valid: 73.39% Test: 74.56%\n",
      "Run: 07, Epoch: 69, Loss: 0.7736, Train: 76.83%, Valid: 72.29% Test: 73.46%\n",
      "Run: 07, Epoch: 70, Loss: 0.7648, Train: 75.18%, Valid: 72.57% Test: 71.49%\n",
      "Run: 07, Epoch: 71, Loss: 0.7696, Train: 77.29%, Valid: 72.84% Test: 72.81%\n",
      "Run: 07, Epoch: 72, Loss: 0.7591, Train: 77.20%, Valid: 72.84% Test: 73.46%\n",
      "Run: 07, Epoch: 73, Loss: 0.7526, Train: 77.93%, Valid: 71.88% Test: 74.12%\n",
      "Run: 07, Epoch: 74, Loss: 0.7574, Train: 76.01%, Valid: 72.98% Test: 74.12%\n",
      "Run: 07, Epoch: 75, Loss: 0.7899, Train: 77.11%, Valid: 75.03% Test: 73.03%\n",
      "Run: 07, Epoch: 76, Loss: 0.7552, Train: 77.47%, Valid: 74.35% Test: 71.93%\n",
      "Run: 07, Epoch: 77, Loss: 0.7531, Train: 78.30%, Valid: 74.62% Test: 75.66%\n",
      "Run: 07, Epoch: 78, Loss: 0.7480, Train: 79.21%, Valid: 75.31% Test: 77.41%\n",
      "Run: 07, Epoch: 79, Loss: 0.7342, Train: 77.75%, Valid: 74.62% Test: 75.88%\n",
      "Run: 07, Epoch: 80, Loss: 0.7610, Train: 77.84%, Valid: 74.35% Test: 76.10%\n",
      "Run: 07, Epoch: 81, Loss: 0.7523, Train: 77.93%, Valid: 73.66% Test: 75.00%\n",
      "Run: 07, Epoch: 82, Loss: 0.7316, Train: 78.39%, Valid: 73.25% Test: 75.00%\n",
      "Run: 07, Epoch: 83, Loss: 0.7334, Train: 78.48%, Valid: 74.07% Test: 73.90%\n",
      "Run: 07, Epoch: 84, Loss: 0.7483, Train: 78.66%, Valid: 74.62% Test: 74.34%\n",
      "Run: 07, Epoch: 85, Loss: 0.7378, Train: 79.95%, Valid: 75.58% Test: 75.88%\n",
      "Run: 07, Epoch: 86, Loss: 0.7135, Train: 79.49%, Valid: 76.27% Test: 75.44%\n",
      "Run: 07, Epoch: 87, Loss: 0.7236, Train: 80.22%, Valid: 75.86% Test: 77.41%\n",
      "Run: 07, Epoch: 88, Loss: 0.7177, Train: 79.67%, Valid: 73.80% Test: 77.85%\n",
      "Run: 07, Epoch: 89, Loss: 0.7425, Train: 79.58%, Valid: 74.35% Test: 77.19%\n",
      "Run: 07, Epoch: 90, Loss: 0.7225, Train: 80.22%, Valid: 75.45% Test: 77.85%\n",
      "Run: 07, Epoch: 91, Loss: 0.7384, Train: 79.58%, Valid: 76.13% Test: 76.97%\n",
      "Run: 07, Epoch: 92, Loss: 0.7123, Train: 78.66%, Valid: 75.31% Test: 75.44%\n",
      "Run: 07, Epoch: 93, Loss: 0.6998, Train: 78.48%, Valid: 75.17% Test: 75.66%\n",
      "Run: 07, Epoch: 94, Loss: 0.7125, Train: 79.30%, Valid: 77.78% Test: 76.10%\n",
      "Run: 07, Epoch: 95, Loss: 0.7050, Train: 78.85%, Valid: 75.72% Test: 75.66%\n",
      "Run: 07, Epoch: 96, Loss: 0.7119, Train: 78.21%, Valid: 75.86% Test: 74.78%\n",
      "Run: 07, Epoch: 97, Loss: 0.6876, Train: 79.30%, Valid: 75.31% Test: 76.54%\n",
      "Run: 07, Epoch: 98, Loss: 0.6883, Train: 79.76%, Valid: 75.72% Test: 77.63%\n",
      "Run: 07, Epoch: 99, Loss: 0.6760, Train: 80.49%, Valid: 77.91% Test: 78.07%\n",
      "Run: 07, Epoch: 100, Loss: 0.7162, Train: 80.59%, Valid: 78.05% Test: 78.29%\n",
      "Run 07:\n",
      "Highest Train: 80.59\n",
      "Highest Valid: 78.05\n",
      "  Final Train: 80.59\n",
      "   Final Test: 78.29\n",
      "Run: 08, Epoch: 01, Loss: 1.8003, Train: 17.58%, Valid: 17.01% Test: 19.74%\n",
      "Run: 08, Epoch: 02, Loss: 1.6165, Train: 19.14%, Valid: 21.12% Test: 21.49%\n",
      "Run: 08, Epoch: 03, Loss: 1.5055, Train: 29.49%, Valid: 31.69% Test: 30.26%\n",
      "Run: 08, Epoch: 04, Loss: 1.4929, Train: 27.93%, Valid: 32.51% Test: 27.85%\n",
      "Run: 08, Epoch: 05, Loss: 1.4269, Train: 29.12%, Valid: 33.06% Test: 28.29%\n",
      "Run: 08, Epoch: 06, Loss: 1.3915, Train: 32.23%, Valid: 35.12% Test: 28.07%\n",
      "Run: 08, Epoch: 07, Loss: 1.3988, Train: 37.27%, Valid: 39.23% Test: 30.70%\n",
      "Run: 08, Epoch: 08, Loss: 1.3428, Train: 40.20%, Valid: 40.74% Test: 35.09%\n",
      "Run: 08, Epoch: 09, Loss: 1.3404, Train: 43.77%, Valid: 43.21% Test: 38.38%\n",
      "Run: 08, Epoch: 10, Loss: 1.2693, Train: 45.05%, Valid: 45.54% Test: 44.08%\n",
      "Run: 08, Epoch: 11, Loss: 1.2805, Train: 43.86%, Valid: 42.11% Test: 43.20%\n",
      "Run: 08, Epoch: 12, Loss: 1.2684, Train: 42.12%, Valid: 41.02% Test: 42.76%\n",
      "Run: 08, Epoch: 13, Loss: 1.2561, Train: 43.96%, Valid: 42.94% Test: 43.20%\n",
      "Run: 08, Epoch: 14, Loss: 1.2258, Train: 47.34%, Valid: 45.54% Test: 45.83%\n",
      "Run: 08, Epoch: 15, Loss: 1.2568, Train: 49.54%, Valid: 48.70% Test: 48.03%\n",
      "Run: 08, Epoch: 16, Loss: 1.1886, Train: 50.37%, Valid: 48.97% Test: 48.90%\n",
      "Run: 08, Epoch: 17, Loss: 1.2045, Train: 48.63%, Valid: 50.07% Test: 49.56%\n",
      "Run: 08, Epoch: 18, Loss: 1.1646, Train: 47.34%, Valid: 48.42% Test: 48.46%\n",
      "Run: 08, Epoch: 19, Loss: 1.1817, Train: 47.34%, Valid: 47.60% Test: 47.15%\n",
      "Run: 08, Epoch: 20, Loss: 1.1377, Train: 48.44%, Valid: 48.42% Test: 48.03%\n",
      "Run: 08, Epoch: 21, Loss: 1.1405, Train: 49.45%, Valid: 49.52% Test: 49.12%\n",
      "Run: 08, Epoch: 22, Loss: 1.1590, Train: 51.01%, Valid: 48.97% Test: 50.00%\n",
      "Run: 08, Epoch: 23, Loss: 1.1157, Train: 51.56%, Valid: 49.11% Test: 50.88%\n",
      "Run: 08, Epoch: 24, Loss: 1.1179, Train: 51.65%, Valid: 48.70% Test: 49.34%\n",
      "Run: 08, Epoch: 25, Loss: 1.0811, Train: 51.28%, Valid: 49.52% Test: 48.90%\n",
      "Run: 08, Epoch: 26, Loss: 1.0763, Train: 54.58%, Valid: 52.13% Test: 51.32%\n",
      "Run: 08, Epoch: 27, Loss: 1.0753, Train: 56.41%, Valid: 55.01% Test: 54.61%\n",
      "Run: 08, Epoch: 28, Loss: 1.0455, Train: 56.96%, Valid: 56.10% Test: 54.61%\n",
      "Run: 08, Epoch: 29, Loss: 1.0504, Train: 58.70%, Valid: 57.61% Test: 54.39%\n",
      "Run: 08, Epoch: 30, Loss: 1.0491, Train: 57.69%, Valid: 55.69% Test: 53.51%\n",
      "Run: 08, Epoch: 31, Loss: 1.1036, Train: 58.79%, Valid: 55.83% Test: 53.95%\n",
      "Run: 08, Epoch: 32, Loss: 1.0216, Train: 57.05%, Valid: 55.56% Test: 53.07%\n",
      "Run: 08, Epoch: 33, Loss: 0.9993, Train: 56.23%, Valid: 54.87% Test: 52.85%\n",
      "Run: 08, Epoch: 34, Loss: 1.0159, Train: 56.50%, Valid: 55.14% Test: 53.51%\n",
      "Run: 08, Epoch: 35, Loss: 0.9929, Train: 58.42%, Valid: 57.75% Test: 54.17%\n",
      "Run: 08, Epoch: 36, Loss: 1.0030, Train: 63.46%, Valid: 58.98% Test: 56.36%\n",
      "Run: 08, Epoch: 37, Loss: 0.9747, Train: 66.58%, Valid: 63.79% Test: 58.55%\n",
      "Run: 08, Epoch: 38, Loss: 1.0189, Train: 65.29%, Valid: 62.69% Test: 59.43%\n",
      "Run: 08, Epoch: 39, Loss: 0.9679, Train: 63.37%, Valid: 60.36% Test: 57.89%\n",
      "Run: 08, Epoch: 40, Loss: 0.9956, Train: 63.92%, Valid: 60.49% Test: 58.99%\n",
      "Run: 08, Epoch: 41, Loss: 0.9558, Train: 63.19%, Valid: 62.14% Test: 57.24%\n",
      "Run: 08, Epoch: 42, Loss: 0.9496, Train: 63.74%, Valid: 62.69% Test: 56.14%\n",
      "Run: 08, Epoch: 43, Loss: 0.9559, Train: 62.09%, Valid: 61.04% Test: 57.89%\n",
      "Run: 08, Epoch: 44, Loss: 0.9496, Train: 65.93%, Valid: 61.87% Test: 60.09%\n",
      "Run: 08, Epoch: 45, Loss: 0.9445, Train: 67.31%, Valid: 63.24% Test: 62.06%\n",
      "Run: 08, Epoch: 46, Loss: 0.9564, Train: 69.60%, Valid: 65.43% Test: 63.60%\n",
      "Run: 08, Epoch: 47, Loss: 0.9201, Train: 71.52%, Valid: 68.18% Test: 64.04%\n",
      "Run: 08, Epoch: 48, Loss: 0.9112, Train: 72.16%, Valid: 69.14% Test: 64.69%\n",
      "Run: 08, Epoch: 49, Loss: 0.9173, Train: 69.87%, Valid: 66.39% Test: 62.94%\n",
      "Run: 08, Epoch: 50, Loss: 0.8966, Train: 69.69%, Valid: 65.98% Test: 64.47%\n",
      "Run: 08, Epoch: 51, Loss: 0.8957, Train: 72.71%, Valid: 68.31% Test: 67.11%\n",
      "Run: 08, Epoch: 52, Loss: 0.9046, Train: 73.63%, Valid: 69.55% Test: 68.20%\n",
      "Run: 08, Epoch: 53, Loss: 0.8917, Train: 74.18%, Valid: 71.06% Test: 66.67%\n",
      "Run: 08, Epoch: 54, Loss: 0.8747, Train: 74.54%, Valid: 71.19% Test: 67.32%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 08, Epoch: 55, Loss: 0.8686, Train: 73.81%, Valid: 71.06% Test: 66.23%\n",
      "Run: 08, Epoch: 56, Loss: 0.8983, Train: 73.72%, Valid: 70.51% Test: 66.45%\n",
      "Run: 08, Epoch: 57, Loss: 0.8425, Train: 71.43%, Valid: 68.86% Test: 67.54%\n",
      "Run: 08, Epoch: 58, Loss: 0.8632, Train: 68.22%, Valid: 65.98% Test: 66.45%\n",
      "Run: 08, Epoch: 59, Loss: 0.8369, Train: 70.97%, Valid: 67.90% Test: 66.67%\n",
      "Run: 08, Epoch: 60, Loss: 0.8570, Train: 72.25%, Valid: 69.00% Test: 64.69%\n",
      "Run: 08, Epoch: 61, Loss: 0.8411, Train: 72.07%, Valid: 69.82% Test: 65.35%\n",
      "Run: 08, Epoch: 62, Loss: 0.8466, Train: 71.79%, Valid: 69.27% Test: 64.91%\n",
      "Run: 08, Epoch: 63, Loss: 0.8412, Train: 71.89%, Valid: 68.18% Test: 66.23%\n",
      "Run: 08, Epoch: 64, Loss: 0.8082, Train: 73.35%, Valid: 69.27% Test: 68.64%\n",
      "Run: 08, Epoch: 65, Loss: 0.8206, Train: 73.35%, Valid: 69.82% Test: 68.86%\n",
      "Run: 08, Epoch: 66, Loss: 0.8555, Train: 73.53%, Valid: 70.10% Test: 66.45%\n",
      "Run: 08, Epoch: 67, Loss: 0.7956, Train: 75.92%, Valid: 72.29% Test: 66.01%\n",
      "Run: 08, Epoch: 68, Loss: 0.8049, Train: 77.75%, Valid: 72.57% Test: 71.05%\n",
      "Run: 08, Epoch: 69, Loss: 0.8401, Train: 78.02%, Valid: 72.57% Test: 70.61%\n",
      "Run: 08, Epoch: 70, Loss: 0.7941, Train: 78.57%, Valid: 72.43% Test: 71.49%\n",
      "Run: 08, Epoch: 71, Loss: 0.8004, Train: 78.94%, Valid: 72.57% Test: 71.49%\n",
      "Run: 08, Epoch: 72, Loss: 0.8149, Train: 77.47%, Valid: 73.25% Test: 71.93%\n",
      "Run: 08, Epoch: 73, Loss: 0.7852, Train: 76.10%, Valid: 72.43% Test: 71.05%\n",
      "Run: 08, Epoch: 74, Loss: 0.7919, Train: 76.37%, Valid: 72.70% Test: 69.96%\n",
      "Run: 08, Epoch: 75, Loss: 0.7821, Train: 76.65%, Valid: 71.60% Test: 71.05%\n",
      "Run: 08, Epoch: 76, Loss: 0.7645, Train: 74.54%, Valid: 70.51% Test: 71.05%\n",
      "Run: 08, Epoch: 77, Loss: 0.8073, Train: 76.37%, Valid: 71.88% Test: 72.59%\n",
      "Run: 08, Epoch: 78, Loss: 0.7650, Train: 78.02%, Valid: 72.70% Test: 71.27%\n",
      "Run: 08, Epoch: 79, Loss: 0.7682, Train: 76.74%, Valid: 73.25% Test: 70.83%\n",
      "Run: 08, Epoch: 80, Loss: 0.7614, Train: 77.93%, Valid: 72.70% Test: 71.49%\n",
      "Run: 08, Epoch: 81, Loss: 0.7315, Train: 78.85%, Valid: 73.53% Test: 71.71%\n",
      "Run: 08, Epoch: 82, Loss: 0.7498, Train: 79.30%, Valid: 74.76% Test: 73.03%\n",
      "Run: 08, Epoch: 83, Loss: 0.7683, Train: 79.40%, Valid: 75.03% Test: 72.81%\n",
      "Run: 08, Epoch: 84, Loss: 0.7505, Train: 78.39%, Valid: 74.76% Test: 72.15%\n",
      "Run: 08, Epoch: 85, Loss: 0.7490, Train: 78.66%, Valid: 75.17% Test: 73.03%\n",
      "Run: 08, Epoch: 86, Loss: 0.7421, Train: 78.94%, Valid: 75.45% Test: 73.03%\n",
      "Run: 08, Epoch: 87, Loss: 0.7530, Train: 80.49%, Valid: 76.41% Test: 73.25%\n",
      "Run: 08, Epoch: 88, Loss: 0.7442, Train: 81.14%, Valid: 75.72% Test: 71.71%\n",
      "Run: 08, Epoch: 89, Loss: 0.7582, Train: 80.77%, Valid: 74.49% Test: 71.49%\n",
      "Run: 08, Epoch: 90, Loss: 0.7220, Train: 80.04%, Valid: 74.35% Test: 72.37%\n",
      "Run: 08, Epoch: 91, Loss: 0.7531, Train: 79.58%, Valid: 75.45% Test: 74.56%\n",
      "Run: 08, Epoch: 92, Loss: 0.7179, Train: 80.13%, Valid: 75.72% Test: 75.22%\n",
      "Run: 08, Epoch: 93, Loss: 0.6954, Train: 78.66%, Valid: 74.90% Test: 73.03%\n",
      "Run: 08, Epoch: 94, Loss: 0.7418, Train: 78.66%, Valid: 74.49% Test: 71.71%\n",
      "Run: 08, Epoch: 95, Loss: 0.7214, Train: 79.76%, Valid: 75.31% Test: 73.46%\n",
      "Run: 08, Epoch: 96, Loss: 0.7025, Train: 79.49%, Valid: 74.76% Test: 74.34%\n",
      "Run: 08, Epoch: 97, Loss: 0.6899, Train: 79.40%, Valid: 73.53% Test: 74.12%\n",
      "Run: 08, Epoch: 98, Loss: 0.6988, Train: 79.12%, Valid: 74.62% Test: 75.22%\n",
      "Run: 08, Epoch: 99, Loss: 0.6924, Train: 81.59%, Valid: 76.54% Test: 74.12%\n",
      "Run: 08, Epoch: 100, Loss: 0.6947, Train: 79.40%, Valid: 75.72% Test: 71.71%\n",
      "Run 08:\n",
      "Highest Train: 81.59\n",
      "Highest Valid: 76.54\n",
      "  Final Train: 81.59\n",
      "   Final Test: 74.12\n",
      "Run: 09, Epoch: 01, Loss: 1.8303, Train: 27.93%, Valid: 29.08% Test: 28.73%\n",
      "Run: 09, Epoch: 02, Loss: 1.6242, Train: 35.81%, Valid: 38.41% Test: 35.31%\n",
      "Run: 09, Epoch: 03, Loss: 1.5111, Train: 36.63%, Valid: 40.47% Test: 36.62%\n",
      "Run: 09, Epoch: 04, Loss: 1.4038, Train: 44.41%, Valid: 46.09% Test: 45.39%\n",
      "Run: 09, Epoch: 05, Loss: 1.3980, Train: 42.86%, Valid: 40.19% Test: 40.35%\n",
      "Run: 09, Epoch: 06, Loss: 1.3425, Train: 36.81%, Valid: 35.39% Test: 33.99%\n",
      "Run: 09, Epoch: 07, Loss: 1.3251, Train: 36.17%, Valid: 35.94% Test: 35.53%\n",
      "Run: 09, Epoch: 08, Loss: 1.2920, Train: 38.10%, Valid: 37.59% Test: 35.96%\n",
      "Run: 09, Epoch: 09, Loss: 1.2841, Train: 40.66%, Valid: 40.05% Test: 37.28%\n",
      "Run: 09, Epoch: 10, Loss: 1.2735, Train: 45.24%, Valid: 44.17% Test: 40.13%\n",
      "Run: 09, Epoch: 11, Loss: 1.2294, Train: 46.34%, Valid: 43.48% Test: 41.23%\n",
      "Run: 09, Epoch: 12, Loss: 1.2465, Train: 47.89%, Valid: 44.86% Test: 43.42%\n",
      "Run: 09, Epoch: 13, Loss: 1.1969, Train: 51.19%, Valid: 49.11% Test: 48.46%\n",
      "Run: 09, Epoch: 14, Loss: 1.1862, Train: 53.85%, Valid: 51.85% Test: 49.78%\n",
      "Run: 09, Epoch: 15, Loss: 1.1926, Train: 52.20%, Valid: 50.62% Test: 50.66%\n",
      "Run: 09, Epoch: 16, Loss: 1.1728, Train: 51.74%, Valid: 49.66% Test: 50.00%\n",
      "Run: 09, Epoch: 17, Loss: 1.1478, Train: 53.02%, Valid: 50.21% Test: 52.19%\n",
      "Run: 09, Epoch: 18, Loss: 1.1464, Train: 54.21%, Valid: 52.40% Test: 53.07%\n",
      "Run: 09, Epoch: 19, Loss: 1.1313, Train: 57.05%, Valid: 54.46% Test: 56.80%\n",
      "Run: 09, Epoch: 20, Loss: 1.1205, Train: 58.88%, Valid: 55.01% Test: 57.68%\n",
      "Run: 09, Epoch: 21, Loss: 1.1137, Train: 58.88%, Valid: 55.14% Test: 55.92%\n",
      "Run: 09, Epoch: 22, Loss: 1.1197, Train: 57.60%, Valid: 54.73% Test: 54.82%\n",
      "Run: 09, Epoch: 23, Loss: 1.1222, Train: 58.70%, Valid: 57.75% Test: 57.68%\n",
      "Run: 09, Epoch: 24, Loss: 1.1183, Train: 60.16%, Valid: 58.02% Test: 60.09%\n",
      "Run: 09, Epoch: 25, Loss: 1.0937, Train: 60.71%, Valid: 59.12% Test: 61.40%\n",
      "Run: 09, Epoch: 26, Loss: 1.0836, Train: 61.36%, Valid: 59.67% Test: 61.40%\n",
      "Run: 09, Epoch: 27, Loss: 1.0812, Train: 61.90%, Valid: 58.98% Test: 61.62%\n",
      "Run: 09, Epoch: 28, Loss: 1.0371, Train: 62.00%, Valid: 58.30% Test: 59.43%\n",
      "Run: 09, Epoch: 29, Loss: 1.0550, Train: 61.81%, Valid: 57.75% Test: 59.21%\n",
      "Run: 09, Epoch: 30, Loss: 1.0518, Train: 62.91%, Valid: 61.04% Test: 61.40%\n",
      "Run: 09, Epoch: 31, Loss: 1.0410, Train: 62.91%, Valid: 61.04% Test: 62.94%\n",
      "Run: 09, Epoch: 32, Loss: 1.0171, Train: 62.45%, Valid: 60.63% Test: 62.72%\n",
      "Run: 09, Epoch: 33, Loss: 1.0331, Train: 63.83%, Valid: 62.55% Test: 63.60%\n",
      "Run: 09, Epoch: 34, Loss: 0.9836, Train: 66.03%, Valid: 64.47% Test: 64.25%\n",
      "Run: 09, Epoch: 35, Loss: 0.9890, Train: 67.22%, Valid: 65.71% Test: 66.45%\n",
      "Run: 09, Epoch: 36, Loss: 0.9875, Train: 65.11%, Valid: 64.20% Test: 67.11%\n",
      "Run: 09, Epoch: 37, Loss: 0.9901, Train: 65.29%, Valid: 64.20% Test: 67.98%\n",
      "Run: 09, Epoch: 38, Loss: 0.9972, Train: 66.58%, Valid: 64.47% Test: 65.35%\n",
      "Run: 09, Epoch: 39, Loss: 1.0023, Train: 67.49%, Valid: 65.57% Test: 67.76%\n",
      "Run: 09, Epoch: 40, Loss: 0.9561, Train: 67.67%, Valid: 66.80% Test: 67.98%\n",
      "Run: 09, Epoch: 41, Loss: 0.9347, Train: 65.75%, Valid: 67.90% Test: 64.69%\n",
      "Run: 09, Epoch: 42, Loss: 0.9457, Train: 65.20%, Valid: 66.67% Test: 65.13%\n",
      "Run: 09, Epoch: 43, Loss: 0.9200, Train: 65.38%, Valid: 64.61% Test: 66.01%\n",
      "Run: 09, Epoch: 44, Loss: 0.9032, Train: 65.48%, Valid: 63.92% Test: 65.79%\n",
      "Run: 09, Epoch: 45, Loss: 0.9271, Train: 66.12%, Valid: 63.92% Test: 65.13%\n",
      "Run: 09, Epoch: 46, Loss: 0.9298, Train: 66.76%, Valid: 66.67% Test: 66.45%\n",
      "Run: 09, Epoch: 47, Loss: 0.8995, Train: 70.51%, Valid: 69.55% Test: 69.74%\n",
      "Run: 09, Epoch: 48, Loss: 0.8843, Train: 69.41%, Valid: 69.14% Test: 68.86%\n",
      "Run: 09, Epoch: 49, Loss: 0.8727, Train: 69.41%, Valid: 70.64% Test: 67.32%\n",
      "Run: 09, Epoch: 50, Loss: 0.8980, Train: 70.51%, Valid: 71.33% Test: 67.76%\n",
      "Run: 09, Epoch: 51, Loss: 0.8974, Train: 71.70%, Valid: 71.74% Test: 70.39%\n",
      "Run: 09, Epoch: 52, Loss: 0.8761, Train: 72.80%, Valid: 72.15% Test: 69.96%\n",
      "Run: 09, Epoch: 53, Loss: 0.8601, Train: 71.52%, Valid: 71.33% Test: 70.18%\n",
      "Run: 09, Epoch: 54, Loss: 0.8600, Train: 70.79%, Valid: 70.64% Test: 70.39%\n",
      "Run: 09, Epoch: 55, Loss: 0.8525, Train: 71.15%, Valid: 70.92% Test: 71.49%\n",
      "Run: 09, Epoch: 56, Loss: 0.8733, Train: 73.35%, Valid: 71.74% Test: 74.78%\n",
      "Run: 09, Epoch: 57, Loss: 0.8435, Train: 75.00%, Valid: 74.07% Test: 74.78%\n",
      "Run: 09, Epoch: 58, Loss: 0.8463, Train: 75.82%, Valid: 74.35% Test: 72.81%\n",
      "Run: 09, Epoch: 59, Loss: 0.8273, Train: 74.54%, Valid: 72.43% Test: 73.03%\n",
      "Run: 09, Epoch: 60, Loss: 0.8253, Train: 73.08%, Valid: 71.60% Test: 72.37%\n",
      "Run: 09, Epoch: 61, Loss: 0.8032, Train: 73.81%, Valid: 71.60% Test: 72.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 09, Epoch: 62, Loss: 0.8287, Train: 74.36%, Valid: 71.60% Test: 73.46%\n",
      "Run: 09, Epoch: 63, Loss: 0.7769, Train: 74.54%, Valid: 71.74% Test: 73.03%\n",
      "Run: 09, Epoch: 64, Loss: 0.8363, Train: 72.25%, Valid: 70.51% Test: 70.18%\n",
      "Run: 09, Epoch: 65, Loss: 0.8044, Train: 71.79%, Valid: 70.92% Test: 68.20%\n",
      "Run: 09, Epoch: 66, Loss: 0.7831, Train: 74.73%, Valid: 73.80% Test: 71.93%\n",
      "Run: 09, Epoch: 67, Loss: 0.8016, Train: 77.29%, Valid: 76.13% Test: 73.90%\n",
      "Run: 09, Epoch: 68, Loss: 0.7961, Train: 76.83%, Valid: 75.72% Test: 74.34%\n",
      "Run: 09, Epoch: 69, Loss: 0.7755, Train: 77.47%, Valid: 75.31% Test: 74.34%\n",
      "Run: 09, Epoch: 70, Loss: 0.7983, Train: 74.27%, Valid: 72.29% Test: 73.25%\n",
      "Run: 09, Epoch: 71, Loss: 0.8024, Train: 72.71%, Valid: 71.06% Test: 70.18%\n",
      "Run: 09, Epoch: 72, Loss: 0.8099, Train: 77.29%, Valid: 73.94% Test: 74.12%\n",
      "Run: 09, Epoch: 73, Loss: 0.7783, Train: 77.20%, Valid: 74.49% Test: 73.90%\n",
      "Run: 09, Epoch: 74, Loss: 0.7924, Train: 74.45%, Valid: 73.11% Test: 73.68%\n",
      "Run: 09, Epoch: 75, Loss: 0.7933, Train: 75.73%, Valid: 72.15% Test: 72.59%\n",
      "Run: 09, Epoch: 76, Loss: 0.7822, Train: 74.91%, Valid: 72.57% Test: 73.25%\n",
      "Run: 09, Epoch: 77, Loss: 0.7884, Train: 75.00%, Valid: 73.25% Test: 71.49%\n",
      "Run: 09, Epoch: 78, Loss: 0.7797, Train: 75.27%, Valid: 73.39% Test: 71.71%\n",
      "Run: 09, Epoch: 79, Loss: 0.7603, Train: 76.74%, Valid: 75.03% Test: 72.81%\n",
      "Run: 09, Epoch: 80, Loss: 0.7749, Train: 75.82%, Valid: 75.99% Test: 73.46%\n",
      "Run: 09, Epoch: 81, Loss: 0.7508, Train: 75.27%, Valid: 74.49% Test: 72.81%\n",
      "Run: 09, Epoch: 82, Loss: 0.7602, Train: 75.92%, Valid: 75.31% Test: 73.25%\n",
      "Run: 09, Epoch: 83, Loss: 0.7475, Train: 79.12%, Valid: 77.23% Test: 76.32%\n",
      "Run: 09, Epoch: 84, Loss: 0.7283, Train: 78.75%, Valid: 74.21% Test: 72.37%\n",
      "Run: 09, Epoch: 85, Loss: 0.7465, Train: 79.03%, Valid: 76.95% Test: 74.12%\n",
      "Run: 09, Epoch: 86, Loss: 0.7434, Train: 79.12%, Valid: 77.78% Test: 75.44%\n",
      "Run: 09, Epoch: 87, Loss: 0.7256, Train: 77.93%, Valid: 75.45% Test: 74.78%\n",
      "Run: 09, Epoch: 88, Loss: 0.7129, Train: 78.39%, Valid: 77.91% Test: 75.44%\n",
      "Run: 09, Epoch: 89, Loss: 0.7260, Train: 79.12%, Valid: 78.74% Test: 75.88%\n",
      "Run: 09, Epoch: 90, Loss: 0.7464, Train: 79.85%, Valid: 77.78% Test: 75.66%\n",
      "Run: 09, Epoch: 91, Loss: 0.7303, Train: 80.22%, Valid: 77.91% Test: 75.88%\n",
      "Run: 09, Epoch: 92, Loss: 0.7108, Train: 79.85%, Valid: 77.91% Test: 75.88%\n",
      "Run: 09, Epoch: 93, Loss: 0.6884, Train: 78.75%, Valid: 76.13% Test: 74.78%\n",
      "Run: 09, Epoch: 94, Loss: 0.7116, Train: 78.48%, Valid: 76.27% Test: 74.56%\n",
      "Run: 09, Epoch: 95, Loss: 0.7138, Train: 80.31%, Valid: 78.88% Test: 75.44%\n",
      "Run: 09, Epoch: 96, Loss: 0.7187, Train: 80.22%, Valid: 78.60% Test: 76.32%\n",
      "Run: 09, Epoch: 97, Loss: 0.7024, Train: 79.58%, Valid: 77.37% Test: 77.41%\n",
      "Run: 09, Epoch: 98, Loss: 0.7428, Train: 79.95%, Valid: 78.88% Test: 76.97%\n",
      "Run: 09, Epoch: 99, Loss: 0.7213, Train: 79.67%, Valid: 77.78% Test: 76.75%\n",
      "Run: 09, Epoch: 100, Loss: 0.7205, Train: 79.03%, Valid: 75.72% Test: 76.10%\n",
      "Run 09:\n",
      "Highest Train: 80.31\n",
      "Highest Valid: 78.88\n",
      "  Final Train: 80.31\n",
      "   Final Test: 75.44\n",
      "Run: 10, Epoch: 01, Loss: 1.7743, Train: 20.51%, Valid: 18.66% Test: 17.76%\n",
      "Run: 10, Epoch: 02, Loss: 1.5723, Train: 23.90%, Valid: 24.28% Test: 28.29%\n",
      "Run: 10, Epoch: 03, Loss: 1.5030, Train: 27.11%, Valid: 28.12% Test: 28.73%\n",
      "Run: 10, Epoch: 04, Loss: 1.4343, Train: 30.40%, Valid: 30.86% Test: 29.61%\n",
      "Run: 10, Epoch: 05, Loss: 1.3924, Train: 34.16%, Valid: 34.71% Test: 35.53%\n",
      "Run: 10, Epoch: 06, Loss: 1.3711, Train: 39.65%, Valid: 39.09% Test: 40.13%\n",
      "Run: 10, Epoch: 07, Loss: 1.3209, Train: 44.51%, Valid: 43.21% Test: 44.96%\n",
      "Run: 10, Epoch: 08, Loss: 1.3163, Train: 46.06%, Valid: 45.40% Test: 47.37%\n",
      "Run: 10, Epoch: 09, Loss: 1.2730, Train: 46.61%, Valid: 46.23% Test: 48.90%\n",
      "Run: 10, Epoch: 10, Loss: 1.2632, Train: 48.81%, Valid: 48.83% Test: 48.90%\n",
      "Run: 10, Epoch: 11, Loss: 1.2367, Train: 51.10%, Valid: 51.03% Test: 51.32%\n",
      "Run: 10, Epoch: 12, Loss: 1.2289, Train: 51.01%, Valid: 51.44% Test: 50.22%\n",
      "Run: 10, Epoch: 13, Loss: 1.2282, Train: 51.56%, Valid: 50.21% Test: 50.88%\n",
      "Run: 10, Epoch: 14, Loss: 1.2049, Train: 54.12%, Valid: 51.17% Test: 52.85%\n",
      "Run: 10, Epoch: 15, Loss: 1.2030, Train: 53.94%, Valid: 51.58% Test: 53.95%\n",
      "Run: 10, Epoch: 16, Loss: 1.1776, Train: 54.03%, Valid: 51.03% Test: 53.95%\n",
      "Run: 10, Epoch: 17, Loss: 1.1395, Train: 52.29%, Valid: 50.21% Test: 53.29%\n",
      "Run: 10, Epoch: 18, Loss: 1.1283, Train: 51.92%, Valid: 49.79% Test: 53.07%\n",
      "Run: 10, Epoch: 19, Loss: 1.1331, Train: 52.01%, Valid: 49.38% Test: 51.10%\n",
      "Run: 10, Epoch: 20, Loss: 1.1254, Train: 53.57%, Valid: 50.07% Test: 50.66%\n",
      "Run: 10, Epoch: 21, Loss: 1.1275, Train: 53.75%, Valid: 50.21% Test: 51.54%\n",
      "Run: 10, Epoch: 22, Loss: 1.0864, Train: 53.02%, Valid: 48.83% Test: 50.44%\n",
      "Run: 10, Epoch: 23, Loss: 1.0908, Train: 53.94%, Valid: 50.34% Test: 49.78%\n",
      "Run: 10, Epoch: 24, Loss: 1.0617, Train: 55.59%, Valid: 51.03% Test: 50.00%\n",
      "Run: 10, Epoch: 25, Loss: 1.0404, Train: 51.65%, Valid: 49.38% Test: 49.78%\n",
      "Run: 10, Epoch: 26, Loss: 1.0347, Train: 50.18%, Valid: 46.50% Test: 47.15%\n",
      "Run: 10, Epoch: 27, Loss: 1.0157, Train: 52.93%, Valid: 48.15% Test: 47.59%\n",
      "Run: 10, Epoch: 28, Loss: 1.0338, Train: 56.87%, Valid: 52.54% Test: 52.63%\n",
      "Run: 10, Epoch: 29, Loss: 1.0006, Train: 60.26%, Valid: 59.12% Test: 58.33%\n",
      "Run: 10, Epoch: 30, Loss: 0.9889, Train: 62.91%, Valid: 61.59% Test: 61.18%\n",
      "Run: 10, Epoch: 31, Loss: 1.0049, Train: 62.64%, Valid: 60.36% Test: 59.87%\n",
      "Run: 10, Epoch: 32, Loss: 0.9603, Train: 62.45%, Valid: 59.26% Test: 58.77%\n",
      "Run: 10, Epoch: 33, Loss: 0.9975, Train: 65.38%, Valid: 59.95% Test: 60.31%\n",
      "Run: 10, Epoch: 34, Loss: 0.9576, Train: 67.58%, Valid: 62.41% Test: 63.60%\n",
      "Run: 10, Epoch: 35, Loss: 0.9475, Train: 68.04%, Valid: 63.65% Test: 64.04%\n",
      "Run: 10, Epoch: 36, Loss: 0.9438, Train: 66.58%, Valid: 62.55% Test: 61.62%\n",
      "Run: 10, Epoch: 37, Loss: 0.9200, Train: 64.47%, Valid: 61.45% Test: 58.77%\n",
      "Run: 10, Epoch: 38, Loss: 0.9158, Train: 66.85%, Valid: 64.47% Test: 61.62%\n",
      "Run: 10, Epoch: 39, Loss: 0.9080, Train: 69.69%, Valid: 67.90% Test: 67.54%\n",
      "Run: 10, Epoch: 40, Loss: 0.9213, Train: 70.70%, Valid: 69.27% Test: 69.96%\n",
      "Run: 10, Epoch: 41, Loss: 0.9108, Train: 71.43%, Valid: 68.72% Test: 71.49%\n",
      "Run: 10, Epoch: 42, Loss: 0.8893, Train: 71.89%, Valid: 69.68% Test: 71.49%\n",
      "Run: 10, Epoch: 43, Loss: 0.8924, Train: 72.16%, Valid: 70.37% Test: 71.27%\n",
      "Run: 10, Epoch: 44, Loss: 0.8700, Train: 71.52%, Valid: 71.88% Test: 69.96%\n",
      "Run: 10, Epoch: 45, Loss: 0.8694, Train: 70.88%, Valid: 71.60% Test: 69.74%\n",
      "Run: 10, Epoch: 46, Loss: 0.8725, Train: 72.07%, Valid: 69.96% Test: 70.83%\n",
      "Run: 10, Epoch: 47, Loss: 0.8532, Train: 72.44%, Valid: 71.33% Test: 70.18%\n",
      "Run: 10, Epoch: 48, Loss: 0.8407, Train: 72.44%, Valid: 72.02% Test: 71.05%\n",
      "Run: 10, Epoch: 49, Loss: 0.8598, Train: 71.79%, Valid: 71.19% Test: 70.83%\n",
      "Run: 10, Epoch: 50, Loss: 0.8525, Train: 71.70%, Valid: 71.06% Test: 70.39%\n",
      "Run: 10, Epoch: 51, Loss: 0.8805, Train: 70.79%, Valid: 69.82% Test: 70.83%\n",
      "Run: 10, Epoch: 52, Loss: 0.8192, Train: 71.79%, Valid: 71.88% Test: 71.05%\n",
      "Run: 10, Epoch: 53, Loss: 0.8432, Train: 72.53%, Valid: 72.15% Test: 73.03%\n",
      "Run: 10, Epoch: 54, Loss: 0.8154, Train: 73.35%, Valid: 71.47% Test: 71.71%\n",
      "Run: 10, Epoch: 55, Loss: 0.8181, Train: 72.62%, Valid: 71.74% Test: 72.59%\n",
      "Run: 10, Epoch: 56, Loss: 0.8246, Train: 73.26%, Valid: 72.98% Test: 73.46%\n",
      "Run: 10, Epoch: 57, Loss: 0.8191, Train: 73.63%, Valid: 73.66% Test: 74.34%\n",
      "Run: 10, Epoch: 58, Loss: 0.8373, Train: 73.99%, Valid: 74.07% Test: 74.12%\n",
      "Run: 10, Epoch: 59, Loss: 0.7991, Train: 74.27%, Valid: 73.53% Test: 75.44%\n",
      "Run: 10, Epoch: 60, Loss: 0.7813, Train: 74.36%, Valid: 73.80% Test: 74.78%\n",
      "Run: 10, Epoch: 61, Loss: 0.8176, Train: 75.46%, Valid: 74.49% Test: 75.88%\n",
      "Run: 10, Epoch: 62, Loss: 0.7866, Train: 76.37%, Valid: 74.21% Test: 75.88%\n",
      "Run: 10, Epoch: 63, Loss: 0.7605, Train: 77.20%, Valid: 74.35% Test: 76.32%\n",
      "Run: 10, Epoch: 64, Loss: 0.7958, Train: 76.01%, Valid: 74.90% Test: 75.00%\n",
      "Run: 10, Epoch: 65, Loss: 0.7715, Train: 74.08%, Valid: 73.11% Test: 73.03%\n",
      "Run: 10, Epoch: 66, Loss: 0.8003, Train: 73.08%, Valid: 70.51% Test: 71.05%\n",
      "Run: 10, Epoch: 67, Loss: 0.7800, Train: 75.82%, Valid: 72.02% Test: 72.15%\n",
      "Run: 10, Epoch: 68, Loss: 0.7762, Train: 76.83%, Valid: 73.53% Test: 72.81%\n",
      "Run: 10, Epoch: 69, Loss: 0.7906, Train: 77.38%, Valid: 74.90% Test: 74.78%\n",
      "Run: 10, Epoch: 70, Loss: 0.7521, Train: 77.38%, Valid: 75.17% Test: 75.88%\n",
      "Run: 10, Epoch: 71, Loss: 0.7749, Train: 76.83%, Valid: 76.13% Test: 76.10%\n",
      "Run: 10, Epoch: 72, Loss: 0.7805, Train: 76.92%, Valid: 76.13% Test: 76.54%\n",
      "Run: 10, Epoch: 73, Loss: 0.7607, Train: 75.82%, Valid: 75.45% Test: 74.12%\n",
      "Run: 10, Epoch: 74, Loss: 0.7443, Train: 76.56%, Valid: 75.03% Test: 73.68%\n",
      "Run: 10, Epoch: 75, Loss: 0.7426, Train: 76.47%, Valid: 75.45% Test: 75.88%\n",
      "Run: 10, Epoch: 76, Loss: 0.7406, Train: 76.47%, Valid: 76.27% Test: 75.88%\n",
      "Run: 10, Epoch: 77, Loss: 0.7494, Train: 77.29%, Valid: 75.58% Test: 76.32%\n",
      "Run: 10, Epoch: 78, Loss: 0.7494, Train: 77.01%, Valid: 75.99% Test: 76.75%\n",
      "Run: 10, Epoch: 79, Loss: 0.7540, Train: 77.47%, Valid: 76.27% Test: 76.97%\n",
      "Run: 10, Epoch: 80, Loss: 0.7155, Train: 77.11%, Valid: 75.99% Test: 76.75%\n",
      "Run: 10, Epoch: 81, Loss: 0.7538, Train: 76.83%, Valid: 76.54% Test: 76.75%\n",
      "Run: 10, Epoch: 82, Loss: 0.7707, Train: 76.74%, Valid: 77.09% Test: 76.97%\n",
      "Run: 10, Epoch: 83, Loss: 0.7588, Train: 76.65%, Valid: 75.58% Test: 74.34%\n",
      "Run: 10, Epoch: 84, Loss: 0.7194, Train: 76.56%, Valid: 75.99% Test: 75.88%\n",
      "Run: 10, Epoch: 85, Loss: 0.7202, Train: 77.66%, Valid: 75.31% Test: 75.22%\n",
      "Run: 10, Epoch: 86, Loss: 0.7169, Train: 78.02%, Valid: 75.45% Test: 75.66%\n",
      "Run: 10, Epoch: 87, Loss: 0.7399, Train: 78.21%, Valid: 77.09% Test: 75.66%\n",
      "Run: 10, Epoch: 88, Loss: 0.7271, Train: 77.56%, Valid: 76.13% Test: 76.32%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 10, Epoch: 89, Loss: 0.7117, Train: 78.75%, Valid: 76.82% Test: 75.66%\n",
      "Run: 10, Epoch: 90, Loss: 0.7388, Train: 78.66%, Valid: 76.13% Test: 75.00%\n",
      "Run: 10, Epoch: 91, Loss: 0.7111, Train: 78.85%, Valid: 77.23% Test: 75.00%\n",
      "Run: 10, Epoch: 92, Loss: 0.6955, Train: 79.30%, Valid: 77.50% Test: 75.44%\n",
      "Run: 10, Epoch: 93, Loss: 0.6925, Train: 79.58%, Valid: 77.91% Test: 75.88%\n",
      "Run: 10, Epoch: 94, Loss: 0.7191, Train: 79.40%, Valid: 78.19% Test: 76.54%\n",
      "Run: 10, Epoch: 95, Loss: 0.6913, Train: 78.48%, Valid: 77.50% Test: 76.10%\n",
      "Run: 10, Epoch: 96, Loss: 0.7213, Train: 78.30%, Valid: 76.68% Test: 76.32%\n",
      "Run: 10, Epoch: 97, Loss: 0.6968, Train: 77.84%, Valid: 76.54% Test: 76.32%\n",
      "Run: 10, Epoch: 98, Loss: 0.6764, Train: 78.48%, Valid: 78.19% Test: 75.66%\n",
      "Run: 10, Epoch: 99, Loss: 0.6610, Train: 78.11%, Valid: 78.46% Test: 76.10%\n",
      "Run: 10, Epoch: 100, Loss: 0.6785, Train: 78.30%, Valid: 78.74% Test: 76.10%\n",
      "Run 10:\n",
      "Highest Train: 79.58\n",
      "Highest Valid: 78.74\n",
      "  Final Train: 78.30\n",
      "   Final Test: 76.10\n",
      "All runs:\n",
      "Highest Train: 80.60 ± 1.43\n",
      "Highest Valid: 77.38 ± 1.48\n",
      "  Final Train: 79.84 ± 1.86\n",
      "   Final Test: 76.32 ± 1.96\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
    "         'batch_size': 32, 'hidden_channels': 32, 'dropout': 0.5, 'epochs': 100, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-6, 'lr': 0.01}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = WebKB(root='/tmp/Texas', name='Texas',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    #idx_train=[data.train_mask[i][0] for i in range(len(data.y))]\n",
    "    #train_idx = np.where(idx_train)[0]\n",
    "    #idx_val=[data.val_mask[i][0] for i in range(len(data.y))]\n",
    "    #valid_idx = np.where(idx_val)[0]\n",
    "    #idx_test=[data.test_mask[i][0] for i in range(len(data.y))]\n",
    "    #test_idx = np.where(idx_test)[0]\n",
    "    \n",
    "    model = SAGE(data.num_features, args.hidden_channels,\n",
    "                    dataset.num_classes, args.num_layers,\n",
    "                    args.dropout)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        idx_train=[data.train_mask[i][run] for i in range(len(data.y))]\n",
    "        train_idx = np.where(idx_train)[0]\n",
    "        idx_val=[data.val_mask[i][run] for i in range(len(data.y))]\n",
    "        valid_idx = np.where(idx_val)[0]\n",
    "        idx_test=[data.test_mask[i][run] for i in range(len(data.y))]\n",
    "        test_idx = np.where(idx_test)[0]\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085c7fd",
   "metadata": {},
   "source": [
    "# Topological Encodding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e47b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2277, 2325], edge_index=[2, 36101], y=[2277], train_mask=[2277, 10], val_mask=[2277, 10], test_mask=[2277, 10])\n"
     ]
    }
   ],
   "source": [
    "dataset = WikipediaNetwork(root='/tmp/chameleon', name='chameleon')\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "607be4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 2276 2276 2276]\n",
      " [1161 1667 1991 ... 2212 2246 2276]]\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_index.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52514bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Edge_idx=data.edge_index.numpy()\n",
    "Node=range(Number_nodes)\n",
    "Edgelist=[]\n",
    "for i in range(len(Edge_idx[1])):\n",
    "    Edgelist.append((Edge_idx[0][i],Edge_idx[1][i]))\n",
    "#print(Edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f9d236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a \"plain\" graph is undirected\n",
    "#G = nx.DiGraph()\n",
    "G = nx.Graph()\n",
    "\n",
    "# give each a node a 'name', which is a letter in this case.\n",
    "#G.add_node('a')\n",
    "\n",
    "# the add_nodes_from method allows adding nodes from a sequence, in this case a list\n",
    "#nodes_to_add = ['b', 'c', 'd']\n",
    "G.add_nodes_from(Node)\n",
    "\n",
    "# add edge from 'a' to 'b'\n",
    "# since this graph is undirected, the order doesn't matter here\n",
    "#G.add_edge('a', 'b')\n",
    "\n",
    "# just like add_nodes_from, we can add edges from a sequence\n",
    "# edges should be specified as 2-tuples\n",
    "#edges_to_add = [('a', 'c'), ('b', 'c'), ('c', 'd')]\n",
    "G.add_edges_from(Edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "781abc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31421\n"
     ]
    }
   ],
   "source": [
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77abd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Topological_Feature_subLevel(adj,filtration_fun, Filtration):\n",
    "        betti_0=[]\n",
    "        betti_1=[]\n",
    "        for p in range(len(Filtration)):\n",
    "            n_active = np.where(np.array(filtration_fun) <= Filtration[p])[0].tolist()\n",
    "            Active_node=np.unique(n_active)\n",
    "            if (len(Active_node)==0):\n",
    "                betti_0.append(0)\n",
    "                betti_1.append(0)\n",
    "            else:\n",
    "                b=adj[Active_node,:][:,Active_node]\n",
    "                my_flag=pyflagser.flagser_unweighted(b, min_dimension=0, max_dimension=2, directed=False, coeff=2, approximation=None)\n",
    "                x = my_flag[\"betti\"]\n",
    "                betti_0.append(x[0])\n",
    "                betti_1.append(x[1])\n",
    "            n_active.clear()\n",
    "        return betti_0,betti_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e40cacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Degree_list(Graph):\n",
    "    degree_list = [Graph.degree(node) for node in Graph.nodes]\n",
    "    return np.array(degree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "118b65fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  |  109 \n",
      "\n",
      "2  |  89 \n",
      "\n",
      "3  |  94 \n",
      "\n",
      "4  |  167 \n",
      "\n",
      "5  |  100 \n",
      "\n",
      "6  |  96 \n",
      "\n",
      "7  |  121 \n",
      "\n",
      "8  |  81 \n",
      "\n",
      "9  |  66 \n",
      "\n",
      "10  |  99 \n",
      "\n",
      "11  |  69 \n",
      "\n",
      "12  |  54 \n",
      "\n",
      "13  |  62 \n",
      "\n",
      "14  |  38 \n",
      "\n",
      "15  |  19 \n",
      "\n",
      "16  |  39 \n",
      "\n",
      "17  |  25 \n",
      "\n",
      "18  |  131 \n",
      "\n",
      "19  |  17 \n",
      "\n",
      "20  |  34 \n",
      "\n",
      "21  |  5 \n",
      "\n",
      "22  |  8 \n",
      "\n",
      "23  |  33 \n",
      "\n",
      "24  |  28 \n",
      "\n",
      "25  |  54 \n",
      "\n",
      "26  |  3 \n",
      "\n",
      "27  |  6 \n",
      "\n",
      "28  |  28 \n",
      "\n",
      "29  |  36 \n",
      "\n",
      "30  |  23 \n",
      "\n",
      "31  |  30 \n",
      "\n",
      "32  |  9 \n",
      "\n",
      "33  |  60 \n",
      "\n",
      "34  |  3 \n",
      "\n",
      "35  |  6 \n",
      "\n",
      "37  |  6 \n",
      "\n",
      "38  |  12 \n",
      "\n",
      "39  |  4 \n",
      "\n",
      "40  |  9 \n",
      "\n",
      "41  |  20 \n",
      "\n",
      "42  |  2 \n",
      "\n",
      "43  |  4 \n",
      "\n",
      "44  |  6 \n",
      "\n",
      "45  |  1 \n",
      "\n",
      "46  |  6 \n",
      "\n",
      "47  |  2 \n",
      "\n",
      "48  |  8 \n",
      "\n",
      "49  |  3 \n",
      "\n",
      "50  |  4 \n",
      "\n",
      "51  |  19 \n",
      "\n",
      "52  |  17 \n",
      "\n",
      "53  |  6 \n",
      "\n",
      "54  |  3 \n",
      "\n",
      "55  |  2 \n",
      "\n",
      "56  |  9 \n",
      "\n",
      "57  |  14 \n",
      "\n",
      "59  |  2 \n",
      "\n",
      "60  |  6 \n",
      "\n",
      "61  |  7 \n",
      "\n",
      "62  |  6 \n",
      "\n",
      "63  |  6 \n",
      "\n",
      "64  |  2 \n",
      "\n",
      "65  |  5 \n",
      "\n",
      "66  |  4 \n",
      "\n",
      "67  |  4 \n",
      "\n",
      "68  |  5 \n",
      "\n",
      "69  |  2 \n",
      "\n",
      "70  |  2 \n",
      "\n",
      "71  |  12 \n",
      "\n",
      "72  |  10 \n",
      "\n",
      "73  |  3 \n",
      "\n",
      "74  |  6 \n",
      "\n",
      "75  |  2 \n",
      "\n",
      "76  |  5 \n",
      "\n",
      "77  |  2 \n",
      "\n",
      "78  |  8 \n",
      "\n",
      "79  |  3 \n",
      "\n",
      "80  |  5 \n",
      "\n",
      "81  |  2 \n",
      "\n",
      "82  |  1 \n",
      "\n",
      "83  |  1 \n",
      "\n",
      "85  |  1 \n",
      "\n",
      "86  |  1 \n",
      "\n",
      "87  |  2 \n",
      "\n",
      "89  |  1 \n",
      "\n",
      "91  |  1 \n",
      "\n",
      "92  |  2 \n",
      "\n",
      "93  |  1 \n",
      "\n",
      "94  |  1 \n",
      "\n",
      "95  |  1 \n",
      "\n",
      "96  |  2 \n",
      "\n",
      "97  |  2 \n",
      "\n",
      "98  |  2 \n",
      "\n",
      "99  |  1 \n",
      "\n",
      "100  |  2 \n",
      "\n",
      "102  |  1 \n",
      "\n",
      "104  |  5 \n",
      "\n",
      "105  |  13 \n",
      "\n",
      "106  |  5 \n",
      "\n",
      "107  |  2 \n",
      "\n",
      "108  |  5 \n",
      "\n",
      "109  |  1 \n",
      "\n",
      "110  |  3 \n",
      "\n",
      "111  |  3 \n",
      "\n",
      "112  |  2 \n",
      "\n",
      "115  |  1 \n",
      "\n",
      "116  |  1 \n",
      "\n",
      "117  |  5 \n",
      "\n",
      "118  |  3 \n",
      "\n",
      "119  |  2 \n",
      "\n",
      "120  |  1 \n",
      "\n",
      "121  |  1 \n",
      "\n",
      "124  |  1 \n",
      "\n",
      "125  |  1 \n",
      "\n",
      "126  |  1 \n",
      "\n",
      "128  |  1 \n",
      "\n",
      "129  |  3 \n",
      "\n",
      "130  |  1 \n",
      "\n",
      "131  |  3 \n",
      "\n",
      "133  |  3 \n",
      "\n",
      "134  |  2 \n",
      "\n",
      "135  |  1 \n",
      "\n",
      "136  |  1 \n",
      "\n",
      "137  |  2 \n",
      "\n",
      "139  |  3 \n",
      "\n",
      "140  |  3 \n",
      "\n",
      "142  |  4 \n",
      "\n",
      "143  |  2 \n",
      "\n",
      "144  |  1 \n",
      "\n",
      "145  |  2 \n",
      "\n",
      "146  |  4 \n",
      "\n",
      "147  |  1 \n",
      "\n",
      "148  |  3 \n",
      "\n",
      "149  |  2 \n",
      "\n",
      "152  |  1 \n",
      "\n",
      "153  |  2 \n",
      "\n",
      "154  |  4 \n",
      "\n",
      "155  |  3 \n",
      "\n",
      "156  |  2 \n",
      "\n",
      "157  |  2 \n",
      "\n",
      "159  |  1 \n",
      "\n",
      "168  |  1 \n",
      "\n",
      "169  |  1 \n",
      "\n",
      "174  |  1 \n",
      "\n",
      "175  |  1 \n",
      "\n",
      "181  |  2 \n",
      "\n",
      "187  |  3 \n",
      "\n",
      "195  |  1 \n",
      "\n",
      "200  |  1 \n",
      "\n",
      "207  |  1 \n",
      "\n",
      "210  |  1 \n",
      "\n",
      "211  |  1 \n",
      "\n",
      "213  |  2 \n",
      "\n",
      "214  |  1 \n",
      "\n",
      "220  |  1 \n",
      "\n",
      "233  |  1 \n",
      "\n",
      "237  |  2 \n",
      "\n",
      "256  |  1 \n",
      "\n",
      "260  |  1 \n",
      "\n",
      "269  |  1 \n",
      "\n",
      "272  |  1 \n",
      "\n",
      "273  |  1 \n",
      "\n",
      "308  |  1 \n",
      "\n",
      "322  |  1 \n",
      "\n",
      "402  |  1 \n",
      "\n",
      "531  |  1 \n",
      "\n",
      "654  |  1 \n",
      "\n",
      "677  |  1 \n",
      "\n",
      "732  |  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "degree_list=Degree_list(G)\n",
    "unique_list=np.unique(degree_list)\n",
    "for d in unique_list:\n",
    "    count=0\n",
    "    for i in range(len(degree_list)):\n",
    "        if degree_list[i]==d:\n",
    "            count=count+1\n",
    "    print(int(d),\" | \",count,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7080881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 45 (1%)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m filt\u001b[38;5;241m=\u001b[39mDegree_list(subgraph)\n\u001b[1;32m     10\u001b[0m A_sub \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mto_numpy_array(subgraph)\u001b[38;5;66;03m# adjacency matrix of subgraph\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m fe\u001b[38;5;241m=\u001b[39m\u001b[43mTopological_Feature_subLevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfilt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNode_fil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m topo_betti_0\u001b[38;5;241m.\u001b[39mappend(fe[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m topo_betti_1\u001b[38;5;241m.\u001b[39mappend(fe[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m, in \u001b[0;36mTopological_Feature_subLevel\u001b[0;34m(adj, filtration_fun, Filtration)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     b\u001b[38;5;241m=\u001b[39madj[Active_node,:][:,Active_node]\n\u001b[0;32m---> 12\u001b[0m     my_flag\u001b[38;5;241m=\u001b[39m\u001b[43mpyflagser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflagser_unweighted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m my_flag[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetti\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m     betti_0\u001b[38;5;241m.\u001b[39mappend(x[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/pyflagser/flagser.py:102\u001b[0m, in \u001b[0;36mflagser_unweighted\u001b[0;34m(adjacency_matrix, min_dimension, max_dimension, directed, coeff, approximation)\u001b[0m\n\u001b[1;32m     99\u001b[0m     _compute_homology \u001b[38;5;241m=\u001b[39m compute_homology_coeff\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Call flagser binding\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m homology \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_homology\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_dimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m_max_dimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m_approximation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_filtration\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Creating dictionary of return values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetti\u001b[39m\u001b[38;5;124m'\u001b[39m: homology\u001b[38;5;241m.\u001b[39mget_betti_numbers(),\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_count\u001b[39m\u001b[38;5;124m'\u001b[39m: homology\u001b[38;5;241m.\u001b[39mget_cell_count(),\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuler\u001b[39m\u001b[38;5;124m'\u001b[39m: homology\u001b[38;5;241m.\u001b[39mget_euler_characteristic()\n\u001b[1;32m    111\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyflagser\n",
    "Node_fil=[1,2,3,4,5,6,7,8,9,10,12,15,20,25,30,50,100,200,400]\n",
    "topo_betti_0=[]\n",
    "topo_betti_1=[]\n",
    "Node_Edge=[]\n",
    "for i in range(Number_nodes):\n",
    "    print(\"\\rProcessing file {} ({}%)\".format(i, 100*i//(Number_nodes-1)), end='', flush=True)\n",
    "    subgraph=ego_graph(G, i, radius=2, center=True, undirected=True, distance=None)\n",
    "    filt=Degree_list(subgraph)\n",
    "    A_sub = nx.to_numpy_array(subgraph)# adjacency matrix of subgraph\n",
    "    fe=Topological_Feature_subLevel(A_sub,filt,Node_fil)\n",
    "    topo_betti_0.append(fe[0])\n",
    "    topo_betti_1.append(fe[1])\n",
    "    Node_Edge.append([subgraph.number_of_nodes(),subgraph.number_of_edges()])\n",
    "    #topo_with_NE.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a5892bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>9.1</th>\n",
       "      <th>10.1</th>\n",
       "      <th>11.1</th>\n",
       "      <th>12.1</th>\n",
       "      <th>13.1</th>\n",
       "      <th>14.1</th>\n",
       "      <th>15.1</th>\n",
       "      <th>16.1</th>\n",
       "      <th>17.1</th>\n",
       "      <th>18.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>170</td>\n",
       "      <td>308</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>89</td>\n",
       "      <td>109</td>\n",
       "      <td>134</td>\n",
       "      <td>201</td>\n",
       "      <td>266</td>\n",
       "      <td>309</td>\n",
       "      <td>324</td>\n",
       "      <td>328</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "      <td>105</td>\n",
       "      <td>185</td>\n",
       "      <td>1191</td>\n",
       "      <td>1169</td>\n",
       "      <td>617</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   0   1    2    3    4    5    6    7    8  ...  9.1  10.1  \\\n",
       "0           0   0   5    5   13   14   14   14    8    8  ...    3     5   \n",
       "1           1   4  13   20   25   27   24   25   27   26  ...    0     0   \n",
       "2           2   3   4   11   12   17   18   26   26   30  ...    0     4   \n",
       "3           3   7   7    7    7    7    7    1    1    1  ...    0     0   \n",
       "4           4  33  89  109  134  201  266  309  324  328  ...   14    15   \n",
       "\n",
       "   11.1  12.1  13.1  14.1  15.1  16.1  17.1  18.1  \n",
       "0    13    13    10    10    10    10    10    10  \n",
       "1     3    13    40    56    25     3     3     3  \n",
       "2     6    13    23    38   170   308    44    13  \n",
       "3     0     0     0     0     0     0     0     0  \n",
       "4    27    53   105   185  1191  1169   617   617  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Feature_Cham.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a01c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9.1</th>\n",
       "      <th>10.1</th>\n",
       "      <th>11.1</th>\n",
       "      <th>12.1</th>\n",
       "      <th>13.1</th>\n",
       "      <th>14.1</th>\n",
       "      <th>15.1</th>\n",
       "      <th>16.1</th>\n",
       "      <th>17.1</th>\n",
       "      <th>18.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>170</td>\n",
       "      <td>308</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>89</td>\n",
       "      <td>109</td>\n",
       "      <td>134</td>\n",
       "      <td>201</td>\n",
       "      <td>266</td>\n",
       "      <td>309</td>\n",
       "      <td>324</td>\n",
       "      <td>328</td>\n",
       "      <td>321</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "      <td>105</td>\n",
       "      <td>185</td>\n",
       "      <td>1191</td>\n",
       "      <td>1169</td>\n",
       "      <td>617</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1    2    3    4    5    6    7    8    9  ...  9.1  10.1  11.1  12.1  \\\n",
       "0   0   5    5   13   14   14   14    8    8    8  ...    3     5    13    13   \n",
       "1   4  13   20   25   27   24   25   27   26   27  ...    0     0     3    13   \n",
       "2   3   4   11   12   17   18   26   26   30   31  ...    0     4     6    13   \n",
       "3   7   7    7    7    7    7    1    1    1    1  ...    0     0     0     0   \n",
       "4  33  89  109  134  201  266  309  324  328  321  ...   14    15    27    53   \n",
       "\n",
       "   13.1  14.1  15.1  16.1  17.1  18.1  \n",
       "0    10    10    10    10    10    10  \n",
       "1    40    56    25     3     3     3  \n",
       "2    23    38   170   308    44    13  \n",
       "3     0     0     0     0     0     0  \n",
       "4   105   185  1191  1169   617   617  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data1=data.drop(['Unnamed: 0'], axis=1)\n",
    "Data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0845911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topo_fe=torch.tensor(Data1.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64cd0780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2277, 2325], y=[2277], train_mask=[2277, 10], val_mask=[2277, 10], test_mask=[2277, 10], adj_t=[2277, 2277, nnz=36101])\n"
     ]
    }
   ],
   "source": [
    "dataset = WikipediaNetwork(root='/tmp/chameleon', name='chameleon',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbeea52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2277, 10], y=[2277], train_mask=[2277, 10], val_mask=[2277, 10], test_mask=[2277, 10], adj_t=[2277, 2277, nnz=36101], topo=[2277, 38])\n"
     ]
    }
   ],
   "source": [
    "data.x=CC_domain\n",
    "data.topo=Topo_fe\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec13457",
   "metadata": {},
   "source": [
    "# TOPO-W-GSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd4668e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x\n",
    "        #return x.log_softmax(dim=-1)\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters_mlp(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            #x = F.relu(x)\n",
    "            x=F.sigmoid(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        #return torch.log_softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "class MLP2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters_mlp2(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            #x = F.relu(x)\n",
    "            x=F.sigmoid(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n",
    "def train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2):\n",
    "    model.train()\n",
    "    mlp_model.train()\n",
    "    mlp_2.train()\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_mlp.zero_grad()\n",
    "    optimizer_mlp2.zero_grad()\n",
    "    gcn_embedding = model(data.x, data.adj_t)[train_idx]\n",
    "    #print(gcn_embedding)\n",
    "    mlp_embedding = mlp_model(data.topo[train_idx])\n",
    "    #print(mlp_embedding)\n",
    "    combined_embedding = torch.cat((gcn_embedding, mlp_embedding), dim=1)\n",
    "    #print(combined_embedding)\n",
    "    mlp_emb = mlp_2(combined_embedding)\n",
    "    #print(mlp_emb)\n",
    "    loss = F.nll_loss(mlp_emb, data.y.squeeze()[train_idx])\n",
    "    #loss = F.nll_loss(combined_embedding, data.y.squeeze()[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer_mlp2.step()\n",
    "    optimizer.step()\n",
    "    optimizer_mlp.step()\n",
    "    \n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def ACC(Prediction, Label):\n",
    "    correct = Prediction.view(-1).eq(Label).sum().item()\n",
    "    total=len(Label)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx):\n",
    "    model.eval()\n",
    "    mlp_model.eval()\n",
    "    mlp_2.eval()\n",
    "\n",
    "    gcn_out = model(data.x, data.adj_t)\n",
    "    #print(gcn_out[0])\n",
    "    mlp_out=mlp_model(data.topo)\n",
    "    #print(mlp_out)\n",
    "    #out=torch.cat((gcn_out,mlp_out),dim=1)\n",
    "    Com=torch.cat((gcn_out,mlp_out),dim=1)\n",
    "    out=mlp_2(Com)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    #print(y_pred[0])\n",
    "    y_pred=y_pred.view(-1)\n",
    "    train_acc=ACC(data.y[train_idx],y_pred[train_idx])\n",
    "    valid_acc=ACC(data.y[valid_idx],y_pred[valid_idx])\n",
    "    test_acc =ACC(data.y[test_idx],y_pred[test_idx])\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef21f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x162648730>\n",
      "Run: 01, Epoch: 01, Loss: 1.6141, Train: 20.05%, Valid: 20.30% Test: 19.08%\n",
      "Run: 01, Epoch: 02, Loss: 1.5395, Train: 22.07%, Valid: 24.42% Test: 22.37%\n",
      "Run: 01, Epoch: 03, Loss: 1.3991, Train: 22.07%, Valid: 24.42% Test: 22.37%\n",
      "Run: 01, Epoch: 04, Loss: 1.3348, Train: 22.07%, Valid: 24.42% Test: 22.37%\n",
      "Run: 01, Epoch: 05, Loss: 1.3077, Train: 22.07%, Valid: 24.42% Test: 22.37%\n",
      "Run: 01, Epoch: 06, Loss: 1.2708, Train: 25.27%, Valid: 26.89% Test: 25.66%\n",
      "Run: 01, Epoch: 07, Loss: 1.2461, Train: 29.49%, Valid: 30.32% Test: 29.61%\n",
      "Run: 01, Epoch: 08, Loss: 1.2377, Train: 31.23%, Valid: 32.10% Test: 32.24%\n",
      "Run: 01, Epoch: 09, Loss: 1.1869, Train: 30.13%, Valid: 31.82% Test: 30.92%\n",
      "Run: 01, Epoch: 10, Loss: 1.1607, Train: 33.42%, Valid: 35.53% Test: 34.65%\n",
      "Run: 01, Epoch: 11, Loss: 1.1692, Train: 35.16%, Valid: 36.90% Test: 38.16%\n",
      "Run: 01, Epoch: 12, Loss: 1.1222, Train: 39.19%, Valid: 41.70% Test: 39.69%\n",
      "Run: 01, Epoch: 13, Loss: 1.1180, Train: 42.77%, Valid: 44.99% Test: 41.89%\n",
      "Run: 01, Epoch: 14, Loss: 1.1168, Train: 43.32%, Valid: 45.68% Test: 42.76%\n",
      "Run: 01, Epoch: 15, Loss: 1.0841, Train: 46.61%, Valid: 48.01% Test: 45.39%\n",
      "Run: 01, Epoch: 16, Loss: 1.0646, Train: 51.56%, Valid: 52.95% Test: 51.75%\n",
      "Run: 01, Epoch: 17, Loss: 1.0438, Train: 55.04%, Valid: 56.79% Test: 54.82%\n",
      "Run: 01, Epoch: 18, Loss: 1.0422, Train: 59.52%, Valid: 58.85% Test: 57.24%\n",
      "Run: 01, Epoch: 19, Loss: 1.0301, Train: 61.08%, Valid: 58.98% Test: 60.09%\n",
      "Run: 01, Epoch: 20, Loss: 1.0162, Train: 61.36%, Valid: 57.06% Test: 60.09%\n",
      "Run: 01, Epoch: 21, Loss: 1.0128, Train: 58.42%, Valid: 55.42% Test: 58.99%\n",
      "Run: 01, Epoch: 22, Loss: 1.0088, Train: 56.68%, Valid: 55.56% Test: 55.92%\n",
      "Run: 01, Epoch: 23, Loss: 0.9974, Train: 57.97%, Valid: 56.52% Test: 57.46%\n",
      "Run: 01, Epoch: 24, Loss: 0.9895, Train: 59.71%, Valid: 57.34% Test: 58.55%\n",
      "Run: 01, Epoch: 25, Loss: 0.9939, Train: 61.45%, Valid: 59.95% Test: 58.99%\n",
      "Run: 01, Epoch: 26, Loss: 0.9694, Train: 65.38%, Valid: 62.83% Test: 62.50%\n",
      "Run: 01, Epoch: 27, Loss: 0.9647, Train: 65.38%, Valid: 62.28% Test: 61.84%\n",
      "Run: 01, Epoch: 28, Loss: 0.9596, Train: 64.65%, Valid: 61.73% Test: 63.16%\n",
      "Run: 01, Epoch: 29, Loss: 0.9196, Train: 67.95%, Valid: 64.61% Test: 66.45%\n",
      "Run: 01, Epoch: 30, Loss: 0.9356, Train: 67.86%, Valid: 66.26% Test: 67.76%\n",
      "Run: 01, Epoch: 31, Loss: 0.8935, Train: 67.03%, Valid: 66.39% Test: 67.54%\n",
      "Run: 01, Epoch: 32, Loss: 0.9648, Train: 66.21%, Valid: 65.16% Test: 67.32%\n",
      "Run: 01, Epoch: 33, Loss: 0.8989, Train: 63.92%, Valid: 63.79% Test: 64.04%\n",
      "Run: 01, Epoch: 34, Loss: 0.8784, Train: 63.10%, Valid: 60.49% Test: 62.94%\n",
      "Run: 01, Epoch: 35, Loss: 0.8960, Train: 63.92%, Valid: 61.73% Test: 63.82%\n",
      "Run: 01, Epoch: 36, Loss: 0.8703, Train: 64.65%, Valid: 61.73% Test: 64.69%\n",
      "Run: 01, Epoch: 37, Loss: 0.8889, Train: 65.20%, Valid: 62.28% Test: 66.89%\n",
      "Run: 01, Epoch: 38, Loss: 0.8818, Train: 68.04%, Valid: 64.75% Test: 69.08%\n",
      "Run: 01, Epoch: 39, Loss: 0.8492, Train: 69.87%, Valid: 65.57% Test: 69.52%\n",
      "Run: 01, Epoch: 40, Loss: 0.8691, Train: 69.87%, Valid: 66.39% Test: 69.30%\n",
      "Run: 01, Epoch: 41, Loss: 0.8793, Train: 72.53%, Valid: 70.51% Test: 71.71%\n",
      "Run: 01, Epoch: 42, Loss: 0.8165, Train: 73.81%, Valid: 73.53% Test: 73.03%\n",
      "Run: 01, Epoch: 43, Loss: 0.8252, Train: 72.62%, Valid: 72.57% Test: 73.46%\n",
      "Run: 01, Epoch: 44, Loss: 0.8388, Train: 73.81%, Valid: 74.35% Test: 74.34%\n",
      "Run: 01, Epoch: 45, Loss: 0.8330, Train: 72.44%, Valid: 71.74% Test: 72.15%\n",
      "Run: 01, Epoch: 46, Loss: 0.8004, Train: 72.16%, Valid: 71.19% Test: 70.61%\n",
      "Run: 01, Epoch: 47, Loss: 0.8635, Train: 73.90%, Valid: 72.57% Test: 72.81%\n",
      "Run: 01, Epoch: 48, Loss: 0.7898, Train: 74.63%, Valid: 72.84% Test: 73.25%\n",
      "Run: 01, Epoch: 49, Loss: 0.8084, Train: 74.82%, Valid: 73.39% Test: 74.56%\n",
      "Run: 01, Epoch: 50, Loss: 0.7664, Train: 75.27%, Valid: 75.03% Test: 75.66%\n",
      "Run: 01, Epoch: 51, Loss: 0.7874, Train: 74.36%, Valid: 71.88% Test: 74.12%\n",
      "Run: 01, Epoch: 52, Loss: 0.7842, Train: 74.18%, Valid: 71.47% Test: 75.00%\n",
      "Run: 01, Epoch: 53, Loss: 0.7566, Train: 74.54%, Valid: 71.33% Test: 73.46%\n",
      "Run: 01, Epoch: 54, Loss: 0.7905, Train: 73.63%, Valid: 71.06% Test: 72.59%\n",
      "Run: 01, Epoch: 55, Loss: 0.7773, Train: 72.71%, Valid: 70.92% Test: 71.49%\n",
      "Run: 01, Epoch: 56, Loss: 0.7409, Train: 72.53%, Valid: 69.41% Test: 71.71%\n",
      "Run: 01, Epoch: 57, Loss: 0.7323, Train: 72.16%, Valid: 71.74% Test: 71.27%\n",
      "Run: 01, Epoch: 58, Loss: 0.7879, Train: 74.82%, Valid: 74.07% Test: 75.00%\n",
      "Run: 01, Epoch: 59, Loss: 0.7722, Train: 75.09%, Valid: 72.84% Test: 72.37%\n",
      "Run: 01, Epoch: 60, Loss: 0.7317, Train: 74.18%, Valid: 70.92% Test: 71.93%\n",
      "Run: 01, Epoch: 61, Loss: 0.7661, Train: 73.26%, Valid: 70.92% Test: 71.93%\n",
      "Run: 01, Epoch: 62, Loss: 0.7140, Train: 76.10%, Valid: 72.70% Test: 73.68%\n",
      "Run: 01, Epoch: 63, Loss: 0.7203, Train: 76.01%, Valid: 73.11% Test: 75.22%\n",
      "Run: 01, Epoch: 64, Loss: 0.7184, Train: 73.17%, Valid: 69.68% Test: 72.59%\n",
      "Run: 01, Epoch: 65, Loss: 0.6964, Train: 71.70%, Valid: 68.86% Test: 71.27%\n",
      "Run: 01, Epoch: 66, Loss: 0.7328, Train: 74.73%, Valid: 71.88% Test: 71.71%\n",
      "Run: 01, Epoch: 67, Loss: 0.6922, Train: 75.55%, Valid: 73.53% Test: 73.90%\n",
      "Run: 01, Epoch: 68, Loss: 0.7461, Train: 76.74%, Valid: 74.35% Test: 75.22%\n",
      "Run: 01, Epoch: 69, Loss: 0.7074, Train: 77.01%, Valid: 74.35% Test: 74.56%\n",
      "Run: 01, Epoch: 70, Loss: 0.7048, Train: 75.46%, Valid: 73.39% Test: 73.90%\n",
      "Run: 01, Epoch: 71, Loss: 0.7163, Train: 76.10%, Valid: 74.90% Test: 74.12%\n",
      "Run: 01, Epoch: 72, Loss: 0.7098, Train: 77.66%, Valid: 76.54% Test: 75.66%\n",
      "Run: 01, Epoch: 73, Loss: 0.7099, Train: 78.30%, Valid: 75.45% Test: 74.12%\n",
      "Run: 01, Epoch: 74, Loss: 0.6954, Train: 77.84%, Valid: 74.49% Test: 74.56%\n",
      "Run: 01, Epoch: 75, Loss: 0.6759, Train: 78.66%, Valid: 73.80% Test: 73.90%\n",
      "Run: 01, Epoch: 76, Loss: 0.6763, Train: 80.04%, Valid: 75.31% Test: 76.32%\n",
      "Run: 01, Epoch: 77, Loss: 0.6882, Train: 79.30%, Valid: 76.54% Test: 76.97%\n",
      "Run: 01, Epoch: 78, Loss: 0.6726, Train: 78.94%, Valid: 77.23% Test: 76.32%\n",
      "Run: 01, Epoch: 79, Loss: 0.6773, Train: 79.67%, Valid: 78.05% Test: 76.54%\n",
      "Run: 01, Epoch: 80, Loss: 0.6845, Train: 78.39%, Valid: 76.82% Test: 76.32%\n",
      "Run: 01, Epoch: 81, Loss: 0.6899, Train: 79.12%, Valid: 76.27% Test: 76.54%\n",
      "Run: 01, Epoch: 82, Loss: 0.6762, Train: 79.21%, Valid: 77.37% Test: 78.07%\n",
      "Run: 01, Epoch: 83, Loss: 0.6739, Train: 81.50%, Valid: 79.84% Test: 79.82%\n",
      "Run: 01, Epoch: 84, Loss: 0.6556, Train: 81.04%, Valid: 79.01% Test: 79.17%\n",
      "Run: 01, Epoch: 85, Loss: 0.6602, Train: 79.95%, Valid: 77.91% Test: 79.61%\n",
      "Run: 01, Epoch: 86, Loss: 0.6552, Train: 80.22%, Valid: 78.46% Test: 76.75%\n",
      "Run: 01, Epoch: 87, Loss: 0.6827, Train: 79.76%, Valid: 77.64% Test: 76.54%\n",
      "Run: 01, Epoch: 88, Loss: 0.6728, Train: 79.30%, Valid: 77.23% Test: 77.41%\n",
      "Run: 01, Epoch: 89, Loss: 0.6879, Train: 78.48%, Valid: 77.09% Test: 77.85%\n",
      "Run: 01, Epoch: 90, Loss: 0.6034, Train: 79.03%, Valid: 77.37% Test: 77.41%\n",
      "Run: 01, Epoch: 91, Loss: 0.6585, Train: 78.48%, Valid: 77.09% Test: 75.22%\n",
      "Run: 01, Epoch: 92, Loss: 0.6193, Train: 77.47%, Valid: 75.99% Test: 72.37%\n",
      "Run: 01, Epoch: 93, Loss: 0.6181, Train: 79.49%, Valid: 77.91% Test: 75.66%\n",
      "Run: 01, Epoch: 94, Loss: 0.6248, Train: 80.49%, Valid: 77.50% Test: 76.32%\n",
      "Run: 01, Epoch: 95, Loss: 0.6435, Train: 81.50%, Valid: 79.29% Test: 76.10%\n",
      "Run: 01, Epoch: 96, Loss: 0.6304, Train: 80.49%, Valid: 78.74% Test: 77.41%\n",
      "Run: 01, Epoch: 97, Loss: 0.6445, Train: 80.40%, Valid: 79.42% Test: 80.04%\n",
      "Run: 01, Epoch: 98, Loss: 0.6153, Train: 81.50%, Valid: 78.33% Test: 79.17%\n",
      "Run: 01, Epoch: 99, Loss: 0.6333, Train: 79.49%, Valid: 78.05% Test: 76.54%\n",
      "Run: 01, Epoch: 100, Loss: 0.6304, Train: 79.76%, Valid: 77.09% Test: 77.41%\n",
      "Run 01:\n",
      "Highest Train: 81.50\n",
      "Highest Valid: 79.84\n",
      "  Final Train: 81.50\n",
      "   Final Test: 79.82\n",
      "Run: 02, Epoch: 01, Loss: 1.6216, Train: 28.21%, Valid: 28.40% Test: 27.85%\n",
      "Run: 02, Epoch: 02, Loss: 1.5128, Train: 22.16%, Valid: 24.28% Test: 22.37%\n",
      "Run: 02, Epoch: 03, Loss: 1.4385, Train: 31.50%, Valid: 33.74% Test: 34.21%\n",
      "Run: 02, Epoch: 04, Loss: 1.4028, Train: 37.45%, Valid: 36.76% Test: 38.38%\n",
      "Run: 02, Epoch: 05, Loss: 1.3568, Train: 33.97%, Valid: 33.06% Test: 36.40%\n",
      "Run: 02, Epoch: 06, Loss: 1.3184, Train: 40.11%, Valid: 37.45% Test: 37.50%\n",
      "Run: 02, Epoch: 07, Loss: 1.3130, Train: 44.87%, Valid: 41.02% Test: 39.04%\n",
      "Run: 02, Epoch: 08, Loss: 1.2686, Train: 51.47%, Valid: 47.74% Test: 47.15%\n",
      "Run: 02, Epoch: 09, Loss: 1.2289, Train: 54.49%, Valid: 50.34% Test: 51.75%\n",
      "Run: 02, Epoch: 10, Loss: 1.1856, Train: 49.54%, Valid: 48.56% Test: 50.66%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 02, Epoch: 11, Loss: 1.1470, Train: 46.79%, Valid: 45.54% Test: 47.37%\n",
      "Run: 02, Epoch: 12, Loss: 1.1332, Train: 45.79%, Valid: 46.78% Test: 45.61%\n",
      "Run: 02, Epoch: 13, Loss: 1.1032, Train: 44.87%, Valid: 44.86% Test: 41.89%\n",
      "Run: 02, Epoch: 14, Loss: 1.0867, Train: 37.55%, Valid: 38.27% Test: 36.40%\n",
      "Run: 02, Epoch: 15, Loss: 1.0839, Train: 49.36%, Valid: 47.33% Test: 46.49%\n",
      "Run: 02, Epoch: 16, Loss: 1.0772, Train: 55.77%, Valid: 53.36% Test: 54.39%\n",
      "Run: 02, Epoch: 17, Loss: 1.0348, Train: 57.88%, Valid: 53.64% Test: 54.61%\n",
      "Run: 02, Epoch: 18, Loss: 1.0184, Train: 58.24%, Valid: 54.46% Test: 53.29%\n",
      "Run: 02, Epoch: 19, Loss: 1.0107, Train: 59.71%, Valid: 58.02% Test: 57.89%\n",
      "Run: 02, Epoch: 20, Loss: 0.9894, Train: 57.14%, Valid: 57.48% Test: 55.26%\n",
      "Run: 02, Epoch: 21, Loss: 0.9619, Train: 58.70%, Valid: 57.06% Test: 56.36%\n",
      "Run: 02, Epoch: 22, Loss: 0.9687, Train: 60.99%, Valid: 60.49% Test: 59.65%\n",
      "Run: 02, Epoch: 23, Loss: 0.9608, Train: 59.52%, Valid: 58.44% Test: 59.43%\n",
      "Run: 02, Epoch: 24, Loss: 0.9522, Train: 62.27%, Valid: 60.49% Test: 59.21%\n",
      "Run: 02, Epoch: 25, Loss: 0.9385, Train: 63.10%, Valid: 62.00% Test: 60.75%\n",
      "Run: 02, Epoch: 26, Loss: 0.8918, Train: 64.01%, Valid: 62.83% Test: 62.94%\n",
      "Run: 02, Epoch: 27, Loss: 0.9353, Train: 65.75%, Valid: 65.02% Test: 64.69%\n",
      "Run: 02, Epoch: 28, Loss: 0.9200, Train: 67.31%, Valid: 65.98% Test: 67.98%\n",
      "Run: 02, Epoch: 29, Loss: 0.8990, Train: 70.70%, Valid: 67.08% Test: 68.20%\n",
      "Run: 02, Epoch: 30, Loss: 0.8889, Train: 70.51%, Valid: 65.16% Test: 69.30%\n",
      "Run: 02, Epoch: 31, Loss: 0.8793, Train: 68.13%, Valid: 65.02% Test: 66.23%\n",
      "Run: 02, Epoch: 32, Loss: 0.9024, Train: 67.40%, Valid: 63.65% Test: 66.23%\n",
      "Run: 02, Epoch: 33, Loss: 0.8924, Train: 68.13%, Valid: 64.75% Test: 66.89%\n",
      "Run: 02, Epoch: 34, Loss: 0.8614, Train: 69.14%, Valid: 66.80% Test: 67.11%\n",
      "Run: 02, Epoch: 35, Loss: 0.8625, Train: 69.23%, Valid: 64.88% Test: 67.54%\n",
      "Run: 02, Epoch: 36, Loss: 0.8668, Train: 69.51%, Valid: 64.61% Test: 67.11%\n",
      "Run: 02, Epoch: 37, Loss: 0.8469, Train: 68.96%, Valid: 64.33% Test: 64.91%\n",
      "Run: 02, Epoch: 38, Loss: 0.8242, Train: 68.96%, Valid: 65.29% Test: 65.13%\n",
      "Run: 02, Epoch: 39, Loss: 0.8612, Train: 71.43%, Valid: 69.27% Test: 66.89%\n",
      "Run: 02, Epoch: 40, Loss: 0.8179, Train: 73.08%, Valid: 69.41% Test: 67.98%\n",
      "Run: 02, Epoch: 41, Loss: 0.8193, Train: 73.90%, Valid: 70.64% Test: 70.83%\n",
      "Run: 02, Epoch: 42, Loss: 0.8134, Train: 75.27%, Valid: 70.92% Test: 72.59%\n",
      "Run: 02, Epoch: 43, Loss: 0.8264, Train: 76.19%, Valid: 73.39% Test: 72.81%\n",
      "Run: 02, Epoch: 44, Loss: 0.8250, Train: 76.28%, Valid: 72.98% Test: 74.78%\n",
      "Run: 02, Epoch: 45, Loss: 0.8321, Train: 76.65%, Valid: 73.80% Test: 76.54%\n",
      "Run: 02, Epoch: 46, Loss: 0.7847, Train: 77.29%, Valid: 73.66% Test: 75.44%\n",
      "Run: 02, Epoch: 47, Loss: 0.7995, Train: 77.47%, Valid: 74.21% Test: 74.12%\n",
      "Run: 02, Epoch: 48, Loss: 0.7817, Train: 76.47%, Valid: 72.84% Test: 73.90%\n",
      "Run: 02, Epoch: 49, Loss: 0.7983, Train: 76.65%, Valid: 72.15% Test: 73.46%\n",
      "Run: 02, Epoch: 50, Loss: 0.7811, Train: 77.11%, Valid: 72.84% Test: 75.00%\n",
      "Run: 02, Epoch: 51, Loss: 0.7792, Train: 77.56%, Valid: 73.53% Test: 76.32%\n",
      "Run: 02, Epoch: 52, Loss: 0.7606, Train: 76.47%, Valid: 73.94% Test: 76.32%\n",
      "Run: 02, Epoch: 53, Loss: 0.7573, Train: 76.65%, Valid: 73.66% Test: 75.44%\n",
      "Run: 02, Epoch: 54, Loss: 0.7819, Train: 78.11%, Valid: 74.62% Test: 73.90%\n",
      "Run: 02, Epoch: 55, Loss: 0.7628, Train: 77.75%, Valid: 73.11% Test: 75.00%\n",
      "Run: 02, Epoch: 56, Loss: 0.7511, Train: 78.85%, Valid: 75.31% Test: 75.88%\n",
      "Run: 02, Epoch: 57, Loss: 0.7474, Train: 79.12%, Valid: 75.03% Test: 77.41%\n",
      "Run: 02, Epoch: 58, Loss: 0.7507, Train: 78.21%, Valid: 73.80% Test: 77.41%\n",
      "Run: 02, Epoch: 59, Loss: 0.7299, Train: 78.48%, Valid: 74.76% Test: 77.19%\n",
      "Run: 02, Epoch: 60, Loss: 0.7583, Train: 78.48%, Valid: 75.03% Test: 76.97%\n",
      "Run: 02, Epoch: 61, Loss: 0.7564, Train: 79.58%, Valid: 75.17% Test: 76.54%\n",
      "Run: 02, Epoch: 62, Loss: 0.7215, Train: 78.11%, Valid: 73.80% Test: 76.97%\n",
      "Run: 02, Epoch: 63, Loss: 0.7271, Train: 78.30%, Valid: 74.62% Test: 77.19%\n",
      "Run: 02, Epoch: 64, Loss: 0.7486, Train: 80.40%, Valid: 75.58% Test: 77.19%\n",
      "Run: 02, Epoch: 65, Loss: 0.7321, Train: 79.49%, Valid: 74.62% Test: 78.07%\n",
      "Run: 02, Epoch: 66, Loss: 0.7214, Train: 79.40%, Valid: 74.21% Test: 77.85%\n",
      "Run: 02, Epoch: 67, Loss: 0.7116, Train: 79.12%, Valid: 73.94% Test: 76.54%\n",
      "Run: 02, Epoch: 68, Loss: 0.6914, Train: 78.94%, Valid: 72.29% Test: 75.22%\n",
      "Run: 02, Epoch: 69, Loss: 0.6946, Train: 78.11%, Valid: 72.57% Test: 74.34%\n",
      "Run: 02, Epoch: 70, Loss: 0.7008, Train: 78.02%, Valid: 73.66% Test: 76.10%\n",
      "Run: 02, Epoch: 71, Loss: 0.7000, Train: 80.49%, Valid: 75.31% Test: 77.41%\n",
      "Run: 02, Epoch: 72, Loss: 0.6637, Train: 80.13%, Valid: 75.86% Test: 78.73%\n",
      "Run: 02, Epoch: 73, Loss: 0.6917, Train: 80.13%, Valid: 72.84% Test: 76.10%\n",
      "Run: 02, Epoch: 74, Loss: 0.7224, Train: 78.75%, Valid: 71.33% Test: 76.10%\n",
      "Run: 02, Epoch: 75, Loss: 0.7017, Train: 78.48%, Valid: 72.15% Test: 75.44%\n",
      "Run: 02, Epoch: 76, Loss: 0.6763, Train: 78.48%, Valid: 71.74% Test: 73.68%\n",
      "Run: 02, Epoch: 77, Loss: 0.6991, Train: 79.03%, Valid: 72.70% Test: 74.12%\n",
      "Run: 02, Epoch: 78, Loss: 0.6829, Train: 79.30%, Valid: 73.66% Test: 75.22%\n",
      "Run: 02, Epoch: 79, Loss: 0.6720, Train: 79.12%, Valid: 73.39% Test: 75.44%\n",
      "Run: 02, Epoch: 80, Loss: 0.6897, Train: 77.11%, Valid: 72.70% Test: 77.19%\n",
      "Run: 02, Epoch: 81, Loss: 0.6558, Train: 76.28%, Valid: 71.47% Test: 76.32%\n",
      "Run: 02, Epoch: 82, Loss: 0.6702, Train: 77.66%, Valid: 74.90% Test: 76.97%\n",
      "Run: 02, Epoch: 83, Loss: 0.6743, Train: 80.77%, Valid: 75.31% Test: 76.54%\n",
      "Run: 02, Epoch: 84, Loss: 0.6694, Train: 81.96%, Valid: 76.41% Test: 76.97%\n",
      "Run: 02, Epoch: 85, Loss: 0.6441, Train: 81.87%, Valid: 76.54% Test: 77.19%\n",
      "Run: 02, Epoch: 86, Loss: 0.6325, Train: 81.23%, Valid: 75.86% Test: 76.97%\n",
      "Run: 02, Epoch: 87, Loss: 0.6539, Train: 80.31%, Valid: 73.53% Test: 76.54%\n",
      "Run: 02, Epoch: 88, Loss: 0.6158, Train: 79.40%, Valid: 74.07% Test: 77.85%\n",
      "Run: 02, Epoch: 89, Loss: 0.6684, Train: 81.32%, Valid: 76.54% Test: 79.39%\n",
      "Run: 02, Epoch: 90, Loss: 0.6453, Train: 79.40%, Valid: 76.68% Test: 78.29%\n",
      "Run: 02, Epoch: 91, Loss: 0.6420, Train: 78.75%, Valid: 75.31% Test: 75.44%\n",
      "Run: 02, Epoch: 92, Loss: 0.6388, Train: 78.02%, Valid: 74.76% Test: 74.34%\n",
      "Run: 02, Epoch: 93, Loss: 0.6450, Train: 79.76%, Valid: 75.31% Test: 76.32%\n",
      "Run: 02, Epoch: 94, Loss: 0.6586, Train: 80.04%, Valid: 76.54% Test: 77.19%\n",
      "Run: 02, Epoch: 95, Loss: 0.6295, Train: 81.59%, Valid: 77.78% Test: 77.63%\n",
      "Run: 02, Epoch: 96, Loss: 0.6516, Train: 81.23%, Valid: 76.82% Test: 77.63%\n",
      "Run: 02, Epoch: 97, Loss: 0.6817, Train: 82.78%, Valid: 76.82% Test: 77.41%\n",
      "Run: 02, Epoch: 98, Loss: 0.6262, Train: 82.78%, Valid: 76.68% Test: 75.88%\n",
      "Run: 02, Epoch: 99, Loss: 0.6630, Train: 81.23%, Valid: 74.90% Test: 76.75%\n",
      "Run: 02, Epoch: 100, Loss: 0.6179, Train: 79.67%, Valid: 74.07% Test: 77.41%\n",
      "Run 02:\n",
      "Highest Train: 82.78\n",
      "Highest Valid: 77.78\n",
      "  Final Train: 81.59\n",
      "   Final Test: 77.63\n",
      "Run: 03, Epoch: 01, Loss: 1.6430, Train: 22.99%, Valid: 21.67% Test: 24.56%\n",
      "Run: 03, Epoch: 02, Loss: 1.5031, Train: 22.99%, Valid: 21.67% Test: 24.56%\n",
      "Run: 03, Epoch: 03, Loss: 1.4000, Train: 29.85%, Valid: 30.04% Test: 29.82%\n",
      "Run: 03, Epoch: 04, Loss: 1.3260, Train: 29.40%, Valid: 30.73% Test: 29.82%\n",
      "Run: 03, Epoch: 05, Loss: 1.2878, Train: 35.62%, Valid: 34.02% Test: 35.31%\n",
      "Run: 03, Epoch: 06, Loss: 1.2632, Train: 47.34%, Valid: 44.31% Test: 44.52%\n",
      "Run: 03, Epoch: 07, Loss: 1.2208, Train: 53.02%, Valid: 47.19% Test: 48.25%\n",
      "Run: 03, Epoch: 08, Loss: 1.1934, Train: 55.95%, Valid: 49.79% Test: 52.41%\n",
      "Run: 03, Epoch: 09, Loss: 1.1727, Train: 55.95%, Valid: 51.30% Test: 50.44%\n",
      "Run: 03, Epoch: 10, Loss: 1.1140, Train: 52.47%, Valid: 47.87% Test: 48.03%\n",
      "Run: 03, Epoch: 11, Loss: 1.1016, Train: 46.79%, Valid: 44.03% Test: 42.76%\n",
      "Run: 03, Epoch: 12, Loss: 1.0990, Train: 47.34%, Valid: 42.25% Test: 41.89%\n",
      "Run: 03, Epoch: 13, Loss: 1.0410, Train: 48.08%, Valid: 44.03% Test: 44.08%\n",
      "Run: 03, Epoch: 14, Loss: 1.0473, Train: 46.34%, Valid: 42.66% Test: 44.08%\n",
      "Run: 03, Epoch: 15, Loss: 1.0166, Train: 47.44%, Valid: 43.76% Test: 46.05%\n",
      "Run: 03, Epoch: 16, Loss: 1.0160, Train: 49.63%, Valid: 45.13% Test: 48.03%\n",
      "Run: 03, Epoch: 17, Loss: 0.9951, Train: 50.46%, Valid: 45.40% Test: 49.12%\n",
      "Run: 03, Epoch: 18, Loss: 0.9979, Train: 57.60%, Valid: 51.58% Test: 53.73%\n",
      "Run: 03, Epoch: 19, Loss: 0.9425, Train: 61.81%, Valid: 55.01% Test: 55.48%\n",
      "Run: 03, Epoch: 20, Loss: 0.9507, Train: 62.36%, Valid: 55.28% Test: 55.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 03, Epoch: 21, Loss: 0.9451, Train: 62.27%, Valid: 56.93% Test: 54.61%\n",
      "Run: 03, Epoch: 22, Loss: 0.9195, Train: 63.00%, Valid: 57.06% Test: 57.24%\n",
      "Run: 03, Epoch: 23, Loss: 0.9256, Train: 63.92%, Valid: 57.61% Test: 58.33%\n",
      "Run: 03, Epoch: 24, Loss: 0.8980, Train: 65.75%, Valid: 58.02% Test: 60.31%\n",
      "Run: 03, Epoch: 25, Loss: 0.8384, Train: 65.75%, Valid: 58.98% Test: 59.65%\n",
      "Run: 03, Epoch: 26, Loss: 0.8840, Train: 66.94%, Valid: 60.91% Test: 59.43%\n",
      "Run: 03, Epoch: 27, Loss: 0.8641, Train: 70.42%, Valid: 62.69% Test: 62.94%\n",
      "Run: 03, Epoch: 28, Loss: 0.8630, Train: 69.51%, Valid: 63.65% Test: 62.50%\n",
      "Run: 03, Epoch: 29, Loss: 0.8275, Train: 71.34%, Valid: 66.67% Test: 64.69%\n",
      "Run: 03, Epoch: 30, Loss: 0.8470, Train: 72.89%, Valid: 67.90% Test: 66.23%\n",
      "Run: 03, Epoch: 31, Loss: 0.8218, Train: 73.81%, Valid: 67.90% Test: 65.57%\n",
      "Run: 03, Epoch: 32, Loss: 0.8210, Train: 72.80%, Valid: 66.12% Test: 64.25%\n",
      "Run: 03, Epoch: 33, Loss: 0.7990, Train: 70.97%, Valid: 64.33% Test: 64.47%\n",
      "Run: 03, Epoch: 34, Loss: 0.7844, Train: 72.99%, Valid: 66.12% Test: 65.79%\n",
      "Run: 03, Epoch: 35, Loss: 0.8118, Train: 76.01%, Valid: 68.45% Test: 68.42%\n",
      "Run: 03, Epoch: 36, Loss: 0.8131, Train: 76.83%, Valid: 69.55% Test: 69.08%\n",
      "Run: 03, Epoch: 37, Loss: 0.7662, Train: 76.74%, Valid: 70.51% Test: 69.74%\n",
      "Run: 03, Epoch: 38, Loss: 0.7439, Train: 77.01%, Valid: 71.33% Test: 71.27%\n",
      "Run: 03, Epoch: 39, Loss: 0.7415, Train: 75.73%, Valid: 68.45% Test: 68.20%\n",
      "Run: 03, Epoch: 40, Loss: 0.7893, Train: 76.56%, Valid: 68.31% Test: 67.98%\n",
      "Run: 03, Epoch: 41, Loss: 0.7367, Train: 77.01%, Valid: 70.51% Test: 70.39%\n",
      "Run: 03, Epoch: 42, Loss: 0.7447, Train: 78.66%, Valid: 70.23% Test: 72.59%\n",
      "Run: 03, Epoch: 43, Loss: 0.7420, Train: 78.48%, Valid: 70.37% Test: 70.39%\n",
      "Run: 03, Epoch: 44, Loss: 0.7675, Train: 78.30%, Valid: 71.06% Test: 70.18%\n",
      "Run: 03, Epoch: 45, Loss: 0.7462, Train: 77.11%, Valid: 68.45% Test: 70.39%\n",
      "Run: 03, Epoch: 46, Loss: 0.7087, Train: 76.65%, Valid: 71.88% Test: 71.93%\n",
      "Run: 03, Epoch: 47, Loss: 0.7034, Train: 78.48%, Valid: 72.15% Test: 71.93%\n",
      "Run: 03, Epoch: 48, Loss: 0.7459, Train: 79.30%, Valid: 72.70% Test: 72.59%\n",
      "Run: 03, Epoch: 49, Loss: 0.6984, Train: 79.21%, Valid: 71.33% Test: 69.08%\n",
      "Run: 03, Epoch: 50, Loss: 0.7035, Train: 79.40%, Valid: 70.78% Test: 69.74%\n",
      "Run: 03, Epoch: 51, Loss: 0.7138, Train: 80.86%, Valid: 71.47% Test: 71.27%\n",
      "Run: 03, Epoch: 52, Loss: 0.6922, Train: 81.32%, Valid: 72.84% Test: 72.59%\n",
      "Run: 03, Epoch: 53, Loss: 0.6714, Train: 80.95%, Valid: 73.66% Test: 73.03%\n",
      "Run: 03, Epoch: 54, Loss: 0.6725, Train: 80.86%, Valid: 73.80% Test: 72.59%\n",
      "Run: 03, Epoch: 55, Loss: 0.6687, Train: 78.75%, Valid: 70.37% Test: 70.61%\n",
      "Run: 03, Epoch: 56, Loss: 0.6817, Train: 76.83%, Valid: 70.37% Test: 70.39%\n",
      "Run: 03, Epoch: 57, Loss: 0.6761, Train: 79.95%, Valid: 72.70% Test: 74.56%\n",
      "Run: 03, Epoch: 58, Loss: 0.6842, Train: 81.04%, Valid: 74.07% Test: 73.46%\n",
      "Run: 03, Epoch: 59, Loss: 0.6865, Train: 80.95%, Valid: 73.11% Test: 71.93%\n",
      "Run: 03, Epoch: 60, Loss: 0.6554, Train: 80.68%, Valid: 72.98% Test: 71.93%\n",
      "Run: 03, Epoch: 61, Loss: 0.6552, Train: 77.66%, Valid: 70.10% Test: 68.20%\n",
      "Run: 03, Epoch: 62, Loss: 0.6447, Train: 74.63%, Valid: 68.45% Test: 66.23%\n",
      "Run: 03, Epoch: 63, Loss: 0.6659, Train: 77.93%, Valid: 70.78% Test: 69.96%\n",
      "Run: 03, Epoch: 64, Loss: 0.6489, Train: 82.05%, Valid: 74.35% Test: 75.66%\n",
      "Run: 03, Epoch: 65, Loss: 0.6280, Train: 82.97%, Valid: 74.76% Test: 80.04%\n",
      "Run: 03, Epoch: 66, Loss: 0.6527, Train: 80.68%, Valid: 73.39% Test: 75.22%\n",
      "Run: 03, Epoch: 67, Loss: 0.6451, Train: 80.49%, Valid: 72.29% Test: 73.90%\n",
      "Run: 03, Epoch: 68, Loss: 0.6395, Train: 79.85%, Valid: 72.70% Test: 73.25%\n",
      "Run: 03, Epoch: 69, Loss: 0.6540, Train: 79.40%, Valid: 72.43% Test: 72.37%\n",
      "Run: 03, Epoch: 70, Loss: 0.6441, Train: 79.03%, Valid: 70.10% Test: 71.49%\n",
      "Run: 03, Epoch: 71, Loss: 0.6238, Train: 79.03%, Valid: 70.78% Test: 70.83%\n",
      "Run: 03, Epoch: 72, Loss: 0.6426, Train: 78.11%, Valid: 71.47% Test: 70.61%\n",
      "Run: 03, Epoch: 73, Loss: 0.6374, Train: 79.03%, Valid: 70.51% Test: 71.27%\n",
      "Run: 03, Epoch: 74, Loss: 0.6562, Train: 81.87%, Valid: 73.53% Test: 73.25%\n",
      "Run: 03, Epoch: 75, Loss: 0.6058, Train: 82.60%, Valid: 74.90% Test: 76.10%\n",
      "Run: 03, Epoch: 76, Loss: 0.6187, Train: 82.78%, Valid: 75.86% Test: 76.54%\n",
      "Run: 03, Epoch: 77, Loss: 0.6059, Train: 81.23%, Valid: 75.31% Test: 76.75%\n",
      "Run: 03, Epoch: 78, Loss: 0.5876, Train: 81.50%, Valid: 73.39% Test: 75.66%\n",
      "Run: 03, Epoch: 79, Loss: 0.6194, Train: 82.51%, Valid: 73.39% Test: 76.75%\n",
      "Run: 03, Epoch: 80, Loss: 0.5819, Train: 82.78%, Valid: 73.66% Test: 74.12%\n",
      "Run: 03, Epoch: 81, Loss: 0.5648, Train: 78.66%, Valid: 72.29% Test: 69.30%\n",
      "Run: 03, Epoch: 82, Loss: 0.6138, Train: 79.40%, Valid: 71.19% Test: 71.49%\n",
      "Run: 03, Epoch: 83, Loss: 0.6007, Train: 82.97%, Valid: 73.25% Test: 75.66%\n",
      "Run: 03, Epoch: 84, Loss: 0.5943, Train: 83.97%, Valid: 74.76% Test: 75.88%\n",
      "Run: 03, Epoch: 85, Loss: 0.5592, Train: 83.24%, Valid: 74.07% Test: 74.56%\n",
      "Run: 03, Epoch: 86, Loss: 0.5637, Train: 83.61%, Valid: 74.90% Test: 75.66%\n",
      "Run: 03, Epoch: 87, Loss: 0.5407, Train: 84.98%, Valid: 76.82% Test: 75.88%\n",
      "Run: 03, Epoch: 88, Loss: 0.5564, Train: 85.07%, Valid: 76.82% Test: 76.97%\n",
      "Run: 03, Epoch: 89, Loss: 0.5842, Train: 84.62%, Valid: 75.99% Test: 78.51%\n",
      "Run: 03, Epoch: 90, Loss: 0.5861, Train: 84.98%, Valid: 75.58% Test: 76.97%\n",
      "Run: 03, Epoch: 91, Loss: 0.5661, Train: 83.15%, Valid: 73.25% Test: 75.88%\n",
      "Run: 03, Epoch: 92, Loss: 0.5521, Train: 81.59%, Valid: 70.92% Test: 73.03%\n",
      "Run: 03, Epoch: 93, Loss: 0.5663, Train: 82.88%, Valid: 72.43% Test: 74.78%\n",
      "Run: 03, Epoch: 94, Loss: 0.5740, Train: 85.26%, Valid: 75.72% Test: 76.75%\n",
      "Run: 03, Epoch: 95, Loss: 0.5777, Train: 84.80%, Valid: 75.99% Test: 76.32%\n",
      "Run: 03, Epoch: 96, Loss: 0.5690, Train: 82.05%, Valid: 76.41% Test: 75.00%\n",
      "Run: 03, Epoch: 97, Loss: 0.5760, Train: 83.52%, Valid: 75.72% Test: 76.10%\n",
      "Run: 03, Epoch: 98, Loss: 0.5522, Train: 85.26%, Valid: 76.68% Test: 77.41%\n",
      "Run: 03, Epoch: 99, Loss: 0.5661, Train: 85.35%, Valid: 76.54% Test: 77.41%\n",
      "Run: 03, Epoch: 100, Loss: 0.5389, Train: 85.44%, Valid: 75.86% Test: 76.54%\n",
      "Run 03:\n",
      "Highest Train: 85.44\n",
      "Highest Valid: 76.82\n",
      "  Final Train: 84.98\n",
      "   Final Test: 75.88\n",
      "Run: 04, Epoch: 01, Loss: 1.6253, Train: 20.33%, Valid: 21.12% Test: 21.05%\n",
      "Run: 04, Epoch: 02, Loss: 1.4870, Train: 33.42%, Valid: 28.94% Test: 31.36%\n",
      "Run: 04, Epoch: 03, Loss: 1.4087, Train: 28.94%, Valid: 28.67% Test: 25.88%\n",
      "Run: 04, Epoch: 04, Loss: 1.3482, Train: 23.44%, Valid: 23.46% Test: 21.71%\n",
      "Run: 04, Epoch: 05, Loss: 1.2912, Train: 28.30%, Valid: 28.40% Test: 26.10%\n",
      "Run: 04, Epoch: 06, Loss: 1.2746, Train: 31.32%, Valid: 32.37% Test: 31.36%\n",
      "Run: 04, Epoch: 07, Loss: 1.2425, Train: 39.65%, Valid: 39.09% Test: 41.89%\n",
      "Run: 04, Epoch: 08, Loss: 1.2152, Train: 43.86%, Valid: 42.80% Test: 44.52%\n",
      "Run: 04, Epoch: 09, Loss: 1.1753, Train: 41.67%, Valid: 40.47% Test: 40.57%\n",
      "Run: 04, Epoch: 10, Loss: 1.1765, Train: 40.93%, Valid: 38.96% Test: 40.13%\n",
      "Run: 04, Epoch: 11, Loss: 1.1493, Train: 40.57%, Valid: 38.68% Test: 40.13%\n",
      "Run: 04, Epoch: 12, Loss: 1.1140, Train: 47.07%, Valid: 45.68% Test: 46.71%\n",
      "Run: 04, Epoch: 13, Loss: 1.1251, Train: 50.18%, Valid: 50.07% Test: 52.41%\n",
      "Run: 04, Epoch: 14, Loss: 1.0980, Train: 52.66%, Valid: 52.54% Test: 54.61%\n",
      "Run: 04, Epoch: 15, Loss: 1.0730, Train: 55.04%, Valid: 52.81% Test: 55.92%\n",
      "Run: 04, Epoch: 16, Loss: 1.0581, Train: 55.04%, Valid: 54.05% Test: 55.70%\n",
      "Run: 04, Epoch: 17, Loss: 1.0497, Train: 54.40%, Valid: 52.54% Test: 55.04%\n",
      "Run: 04, Epoch: 18, Loss: 1.0233, Train: 53.57%, Valid: 51.44% Test: 53.29%\n",
      "Run: 04, Epoch: 19, Loss: 1.0169, Train: 52.66%, Valid: 50.75% Test: 54.61%\n",
      "Run: 04, Epoch: 20, Loss: 1.0149, Train: 55.68%, Valid: 52.95% Test: 56.80%\n",
      "Run: 04, Epoch: 21, Loss: 0.9948, Train: 58.24%, Valid: 55.97% Test: 62.72%\n",
      "Run: 04, Epoch: 22, Loss: 0.9763, Train: 59.16%, Valid: 57.34% Test: 61.18%\n",
      "Run: 04, Epoch: 23, Loss: 0.9451, Train: 57.69%, Valid: 55.01% Test: 60.75%\n",
      "Run: 04, Epoch: 24, Loss: 0.9931, Train: 56.23%, Valid: 54.73% Test: 59.87%\n",
      "Run: 04, Epoch: 25, Loss: 0.9407, Train: 60.07%, Valid: 56.52% Test: 64.04%\n",
      "Run: 04, Epoch: 26, Loss: 0.9509, Train: 63.92%, Valid: 60.08% Test: 64.04%\n",
      "Run: 04, Epoch: 27, Loss: 0.9412, Train: 67.31%, Valid: 66.53% Test: 69.52%\n",
      "Run: 04, Epoch: 28, Loss: 0.9512, Train: 64.74%, Valid: 65.02% Test: 66.45%\n",
      "Run: 04, Epoch: 29, Loss: 0.9131, Train: 60.90%, Valid: 59.67% Test: 62.28%\n",
      "Run: 04, Epoch: 30, Loss: 0.8874, Train: 61.81%, Valid: 60.36% Test: 65.13%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 04, Epoch: 31, Loss: 0.8913, Train: 67.95%, Valid: 64.88% Test: 69.96%\n",
      "Run: 04, Epoch: 32, Loss: 0.9166, Train: 71.34%, Valid: 66.94% Test: 74.78%\n",
      "Run: 04, Epoch: 33, Loss: 0.8760, Train: 71.98%, Valid: 65.71% Test: 72.81%\n",
      "Run: 04, Epoch: 34, Loss: 0.8636, Train: 68.41%, Valid: 63.51% Test: 73.25%\n",
      "Run: 04, Epoch: 35, Loss: 0.8649, Train: 66.94%, Valid: 63.65% Test: 70.83%\n",
      "Run: 04, Epoch: 36, Loss: 0.8933, Train: 68.32%, Valid: 65.98% Test: 70.83%\n",
      "Run: 04, Epoch: 37, Loss: 0.8543, Train: 69.87%, Valid: 67.35% Test: 71.27%\n",
      "Run: 04, Epoch: 38, Loss: 0.8331, Train: 72.89%, Valid: 68.86% Test: 77.63%\n",
      "Run: 04, Epoch: 39, Loss: 0.8170, Train: 73.99%, Valid: 69.41% Test: 75.22%\n",
      "Run: 04, Epoch: 40, Loss: 0.8266, Train: 71.52%, Valid: 66.12% Test: 71.71%\n",
      "Run: 04, Epoch: 41, Loss: 0.8096, Train: 71.25%, Valid: 63.65% Test: 70.39%\n",
      "Run: 04, Epoch: 42, Loss: 0.7938, Train: 72.89%, Valid: 66.53% Test: 71.49%\n",
      "Run: 04, Epoch: 43, Loss: 0.7934, Train: 72.62%, Valid: 65.84% Test: 72.59%\n",
      "Run: 04, Epoch: 44, Loss: 0.7676, Train: 74.73%, Valid: 68.59% Test: 75.44%\n",
      "Run: 04, Epoch: 45, Loss: 0.7873, Train: 74.36%, Valid: 67.90% Test: 74.34%\n",
      "Run: 04, Epoch: 46, Loss: 0.8063, Train: 73.81%, Valid: 68.86% Test: 75.22%\n",
      "Run: 04, Epoch: 47, Loss: 0.7608, Train: 75.37%, Valid: 70.78% Test: 75.44%\n",
      "Run: 04, Epoch: 48, Loss: 0.7740, Train: 75.09%, Valid: 71.47% Test: 76.75%\n",
      "Run: 04, Epoch: 49, Loss: 0.7348, Train: 75.18%, Valid: 72.29% Test: 77.19%\n",
      "Run: 04, Epoch: 50, Loss: 0.7700, Train: 75.46%, Valid: 72.70% Test: 78.29%\n",
      "Run: 04, Epoch: 51, Loss: 0.7566, Train: 75.82%, Valid: 70.78% Test: 76.10%\n",
      "Run: 04, Epoch: 52, Loss: 0.7374, Train: 76.28%, Valid: 71.06% Test: 77.19%\n",
      "Run: 04, Epoch: 53, Loss: 0.7310, Train: 77.38%, Valid: 70.92% Test: 79.39%\n",
      "Run: 04, Epoch: 54, Loss: 0.7491, Train: 76.37%, Valid: 72.57% Test: 78.29%\n",
      "Run: 04, Epoch: 55, Loss: 0.7324, Train: 77.11%, Valid: 73.11% Test: 78.73%\n",
      "Run: 04, Epoch: 56, Loss: 0.7481, Train: 76.10%, Valid: 73.80% Test: 77.41%\n",
      "Run: 04, Epoch: 57, Loss: 0.7659, Train: 77.01%, Valid: 74.21% Test: 76.97%\n",
      "Run: 04, Epoch: 58, Loss: 0.7425, Train: 75.46%, Valid: 71.47% Test: 76.97%\n",
      "Run: 04, Epoch: 59, Loss: 0.6980, Train: 73.72%, Valid: 69.68% Test: 75.00%\n",
      "Run: 04, Epoch: 60, Loss: 0.7195, Train: 77.01%, Valid: 71.88% Test: 76.32%\n",
      "Run: 04, Epoch: 61, Loss: 0.7188, Train: 79.12%, Valid: 74.49% Test: 78.29%\n",
      "Run: 04, Epoch: 62, Loss: 0.7181, Train: 78.75%, Valid: 73.94% Test: 76.97%\n",
      "Run: 04, Epoch: 63, Loss: 0.7228, Train: 79.76%, Valid: 75.58% Test: 79.17%\n",
      "Run: 04, Epoch: 64, Loss: 0.7053, Train: 75.92%, Valid: 70.37% Test: 76.54%\n",
      "Run: 04, Epoch: 65, Loss: 0.6999, Train: 75.82%, Valid: 70.64% Test: 75.88%\n",
      "Run: 04, Epoch: 66, Loss: 0.7423, Train: 76.92%, Valid: 71.88% Test: 75.66%\n",
      "Run: 04, Epoch: 67, Loss: 0.7190, Train: 77.01%, Valid: 74.90% Test: 77.85%\n",
      "Run: 04, Epoch: 68, Loss: 0.6813, Train: 77.29%, Valid: 75.17% Test: 76.97%\n",
      "Run: 04, Epoch: 69, Loss: 0.6822, Train: 78.02%, Valid: 72.98% Test: 76.32%\n",
      "Run: 04, Epoch: 70, Loss: 0.6876, Train: 80.31%, Valid: 75.17% Test: 78.07%\n",
      "Run: 04, Epoch: 71, Loss: 0.6875, Train: 80.40%, Valid: 75.31% Test: 80.04%\n",
      "Run: 04, Epoch: 72, Loss: 0.6989, Train: 79.58%, Valid: 74.49% Test: 79.17%\n",
      "Run: 04, Epoch: 73, Loss: 0.6661, Train: 80.22%, Valid: 74.07% Test: 78.29%\n",
      "Run: 04, Epoch: 74, Loss: 0.6564, Train: 80.59%, Valid: 75.17% Test: 80.04%\n",
      "Run: 04, Epoch: 75, Loss: 0.6568, Train: 81.23%, Valid: 75.58% Test: 79.17%\n",
      "Run: 04, Epoch: 76, Loss: 0.6488, Train: 79.58%, Valid: 75.72% Test: 78.29%\n",
      "Run: 04, Epoch: 77, Loss: 0.6647, Train: 79.67%, Valid: 75.58% Test: 78.07%\n",
      "Run: 04, Epoch: 78, Loss: 0.6760, Train: 81.50%, Valid: 76.68% Test: 78.95%\n",
      "Run: 04, Epoch: 79, Loss: 0.6449, Train: 82.42%, Valid: 76.54% Test: 80.04%\n",
      "Run: 04, Epoch: 80, Loss: 0.6547, Train: 82.23%, Valid: 75.17% Test: 80.26%\n",
      "Run: 04, Epoch: 81, Loss: 0.6370, Train: 81.59%, Valid: 74.07% Test: 80.04%\n",
      "Run: 04, Epoch: 82, Loss: 0.6407, Train: 81.68%, Valid: 75.17% Test: 78.73%\n",
      "Run: 04, Epoch: 83, Loss: 0.6278, Train: 81.41%, Valid: 75.86% Test: 78.07%\n",
      "Run: 04, Epoch: 84, Loss: 0.6286, Train: 81.50%, Valid: 77.23% Test: 80.04%\n",
      "Run: 04, Epoch: 85, Loss: 0.6637, Train: 81.87%, Valid: 75.99% Test: 79.39%\n",
      "Run: 04, Epoch: 86, Loss: 0.6534, Train: 81.32%, Valid: 76.41% Test: 78.51%\n",
      "Run: 04, Epoch: 87, Loss: 0.6079, Train: 81.96%, Valid: 76.27% Test: 78.07%\n",
      "Run: 04, Epoch: 88, Loss: 0.6576, Train: 80.86%, Valid: 76.13% Test: 77.85%\n",
      "Run: 04, Epoch: 89, Loss: 0.6208, Train: 81.14%, Valid: 75.72% Test: 79.39%\n",
      "Run: 04, Epoch: 90, Loss: 0.6177, Train: 82.14%, Valid: 77.09% Test: 81.36%\n",
      "Run: 04, Epoch: 91, Loss: 0.6110, Train: 83.24%, Valid: 78.19% Test: 83.33%\n",
      "Run: 04, Epoch: 92, Loss: 0.6229, Train: 83.70%, Valid: 77.91% Test: 83.99%\n",
      "Run: 04, Epoch: 93, Loss: 0.6318, Train: 81.32%, Valid: 75.99% Test: 79.17%\n",
      "Run: 04, Epoch: 94, Loss: 0.5982, Train: 78.39%, Valid: 74.21% Test: 78.73%\n",
      "Run: 04, Epoch: 95, Loss: 0.6185, Train: 79.95%, Valid: 73.94% Test: 80.04%\n",
      "Run: 04, Epoch: 96, Loss: 0.6445, Train: 83.52%, Valid: 75.99% Test: 81.14%\n",
      "Run: 04, Epoch: 97, Loss: 0.6388, Train: 84.34%, Valid: 76.54% Test: 82.89%\n",
      "Run: 04, Epoch: 98, Loss: 0.6182, Train: 84.43%, Valid: 76.95% Test: 82.89%\n",
      "Run: 04, Epoch: 99, Loss: 0.5745, Train: 84.34%, Valid: 78.19% Test: 82.89%\n",
      "Run: 04, Epoch: 100, Loss: 0.6184, Train: 82.88%, Valid: 78.60% Test: 81.80%\n",
      "Run 04:\n",
      "Highest Train: 84.43\n",
      "Highest Valid: 78.60\n",
      "  Final Train: 82.88\n",
      "   Final Test: 81.80\n",
      "Run: 05, Epoch: 01, Loss: 1.6492, Train: 21.61%, Valid: 24.55% Test: 23.25%\n",
      "Run: 05, Epoch: 02, Loss: 1.5402, Train: 25.73%, Valid: 28.12% Test: 25.88%\n",
      "Run: 05, Epoch: 03, Loss: 1.4580, Train: 32.05%, Valid: 32.10% Test: 32.02%\n",
      "Run: 05, Epoch: 04, Loss: 1.3811, Train: 23.53%, Valid: 26.06% Test: 24.12%\n",
      "Run: 05, Epoch: 05, Loss: 1.3560, Train: 29.03%, Valid: 30.86% Test: 29.82%\n",
      "Run: 05, Epoch: 06, Loss: 1.3221, Train: 32.60%, Valid: 32.10% Test: 32.46%\n",
      "Run: 05, Epoch: 07, Loss: 1.2757, Train: 40.93%, Valid: 38.96% Test: 35.96%\n",
      "Run: 05, Epoch: 08, Loss: 1.2581, Train: 37.82%, Valid: 38.96% Test: 36.62%\n",
      "Run: 05, Epoch: 09, Loss: 1.2284, Train: 37.64%, Valid: 38.13% Test: 35.75%\n",
      "Run: 05, Epoch: 10, Loss: 1.2039, Train: 41.94%, Valid: 40.19% Test: 37.50%\n",
      "Run: 05, Epoch: 11, Loss: 1.1772, Train: 45.05%, Valid: 43.07% Test: 40.35%\n",
      "Run: 05, Epoch: 12, Loss: 1.1498, Train: 50.64%, Valid: 49.38% Test: 46.27%\n",
      "Run: 05, Epoch: 13, Loss: 1.1112, Train: 52.93%, Valid: 49.93% Test: 47.15%\n",
      "Run: 05, Epoch: 14, Loss: 1.1320, Train: 57.42%, Valid: 57.61% Test: 52.85%\n",
      "Run: 05, Epoch: 15, Loss: 1.0931, Train: 58.06%, Valid: 56.38% Test: 54.82%\n",
      "Run: 05, Epoch: 16, Loss: 1.0779, Train: 52.84%, Valid: 49.11% Test: 48.90%\n",
      "Run: 05, Epoch: 17, Loss: 1.0787, Train: 50.55%, Valid: 47.87% Test: 46.05%\n",
      "Run: 05, Epoch: 18, Loss: 1.0122, Train: 51.74%, Valid: 48.01% Test: 44.96%\n",
      "Run: 05, Epoch: 19, Loss: 1.0419, Train: 54.12%, Valid: 50.34% Test: 50.66%\n",
      "Run: 05, Epoch: 20, Loss: 1.0149, Train: 56.68%, Valid: 52.54% Test: 53.95%\n",
      "Run: 05, Epoch: 21, Loss: 1.0162, Train: 58.24%, Valid: 53.77% Test: 55.04%\n",
      "Run: 05, Epoch: 22, Loss: 0.9623, Train: 60.07%, Valid: 55.56% Test: 55.92%\n",
      "Run: 05, Epoch: 23, Loss: 0.9924, Train: 64.29%, Valid: 59.67% Test: 61.40%\n",
      "Run: 05, Epoch: 24, Loss: 0.9809, Train: 65.02%, Valid: 60.77% Test: 59.43%\n",
      "Run: 05, Epoch: 25, Loss: 0.9692, Train: 63.92%, Valid: 58.44% Test: 59.21%\n",
      "Run: 05, Epoch: 26, Loss: 0.9319, Train: 61.63%, Valid: 58.16% Test: 55.70%\n",
      "Run: 05, Epoch: 27, Loss: 0.9360, Train: 62.82%, Valid: 58.02% Test: 56.14%\n",
      "Run: 05, Epoch: 28, Loss: 0.9178, Train: 63.64%, Valid: 59.12% Test: 59.87%\n",
      "Run: 05, Epoch: 29, Loss: 0.8888, Train: 68.32%, Valid: 62.83% Test: 61.18%\n",
      "Run: 05, Epoch: 30, Loss: 0.8946, Train: 69.32%, Valid: 65.02% Test: 66.01%\n",
      "Run: 05, Epoch: 31, Loss: 0.8693, Train: 70.88%, Valid: 66.26% Test: 66.23%\n",
      "Run: 05, Epoch: 32, Loss: 0.8853, Train: 70.97%, Valid: 65.57% Test: 66.23%\n",
      "Run: 05, Epoch: 33, Loss: 0.8647, Train: 70.42%, Valid: 65.71% Test: 64.04%\n",
      "Run: 05, Epoch: 34, Loss: 0.8298, Train: 69.41%, Valid: 64.75% Test: 64.04%\n",
      "Run: 05, Epoch: 35, Loss: 0.8482, Train: 67.67%, Valid: 64.06% Test: 63.16%\n",
      "Run: 05, Epoch: 36, Loss: 0.8630, Train: 70.42%, Valid: 66.94% Test: 66.23%\n",
      "Run: 05, Epoch: 37, Loss: 0.8344, Train: 73.72%, Valid: 68.72% Test: 67.54%\n",
      "Run: 05, Epoch: 38, Loss: 0.8277, Train: 72.53%, Valid: 70.37% Test: 69.08%\n",
      "Run: 05, Epoch: 39, Loss: 0.8402, Train: 71.34%, Valid: 67.22% Test: 68.64%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 05, Epoch: 40, Loss: 0.8222, Train: 74.08%, Valid: 69.96% Test: 71.27%\n",
      "Run: 05, Epoch: 41, Loss: 0.8917, Train: 74.08%, Valid: 68.59% Test: 71.49%\n",
      "Run: 05, Epoch: 42, Loss: 0.8006, Train: 73.63%, Valid: 69.55% Test: 68.42%\n",
      "Run: 05, Epoch: 43, Loss: 0.7890, Train: 74.91%, Valid: 71.19% Test: 69.74%\n",
      "Run: 05, Epoch: 44, Loss: 0.7946, Train: 73.26%, Valid: 69.82% Test: 68.42%\n",
      "Run: 05, Epoch: 45, Loss: 0.7999, Train: 69.87%, Valid: 65.71% Test: 67.11%\n",
      "Run: 05, Epoch: 46, Loss: 0.8035, Train: 72.53%, Valid: 69.00% Test: 70.18%\n",
      "Run: 05, Epoch: 47, Loss: 0.7561, Train: 73.81%, Valid: 71.47% Test: 73.03%\n",
      "Run: 05, Epoch: 48, Loss: 0.8143, Train: 73.72%, Valid: 72.98% Test: 71.93%\n",
      "Run: 05, Epoch: 49, Loss: 0.8117, Train: 74.18%, Valid: 73.53% Test: 73.68%\n",
      "Run: 05, Epoch: 50, Loss: 0.7761, Train: 74.73%, Valid: 72.57% Test: 72.37%\n",
      "Run: 05, Epoch: 51, Loss: 0.8130, Train: 72.53%, Valid: 68.18% Test: 68.64%\n",
      "Run: 05, Epoch: 52, Loss: 0.7647, Train: 70.70%, Valid: 68.18% Test: 67.11%\n",
      "Run: 05, Epoch: 53, Loss: 0.7844, Train: 72.89%, Valid: 70.10% Test: 70.18%\n",
      "Run: 05, Epoch: 54, Loss: 0.7578, Train: 74.63%, Valid: 72.57% Test: 72.37%\n",
      "Run: 05, Epoch: 55, Loss: 0.7705, Train: 75.37%, Valid: 72.15% Test: 72.15%\n",
      "Run: 05, Epoch: 56, Loss: 0.7418, Train: 73.63%, Valid: 70.78% Test: 69.08%\n",
      "Run: 05, Epoch: 57, Loss: 0.7228, Train: 72.16%, Valid: 69.55% Test: 68.42%\n",
      "Run: 05, Epoch: 58, Loss: 0.7601, Train: 74.45%, Valid: 73.11% Test: 69.96%\n",
      "Run: 05, Epoch: 59, Loss: 0.7415, Train: 76.47%, Valid: 75.03% Test: 75.66%\n",
      "Run: 05, Epoch: 60, Loss: 0.7486, Train: 77.01%, Valid: 75.58% Test: 76.75%\n",
      "Run: 05, Epoch: 61, Loss: 0.7098, Train: 77.84%, Valid: 75.86% Test: 77.63%\n",
      "Run: 05, Epoch: 62, Loss: 0.7429, Train: 78.30%, Valid: 74.62% Test: 74.34%\n",
      "Run: 05, Epoch: 63, Loss: 0.6924, Train: 76.01%, Valid: 73.11% Test: 73.68%\n",
      "Run: 05, Epoch: 64, Loss: 0.7378, Train: 77.29%, Valid: 73.66% Test: 74.56%\n",
      "Run: 05, Epoch: 65, Loss: 0.7309, Train: 78.30%, Valid: 76.54% Test: 75.00%\n",
      "Run: 05, Epoch: 66, Loss: 0.7270, Train: 74.73%, Valid: 73.39% Test: 72.15%\n",
      "Run: 05, Epoch: 67, Loss: 0.7056, Train: 72.71%, Valid: 71.06% Test: 69.30%\n",
      "Run: 05, Epoch: 68, Loss: 0.7398, Train: 74.91%, Valid: 74.90% Test: 70.61%\n",
      "Run: 05, Epoch: 69, Loss: 0.7077, Train: 76.92%, Valid: 75.17% Test: 71.05%\n",
      "Run: 05, Epoch: 70, Loss: 0.6978, Train: 77.29%, Valid: 73.39% Test: 74.56%\n",
      "Run: 05, Epoch: 71, Loss: 0.6740, Train: 79.30%, Valid: 73.94% Test: 77.19%\n",
      "Run: 05, Epoch: 72, Loss: 0.6868, Train: 78.85%, Valid: 74.49% Test: 76.54%\n",
      "Run: 05, Epoch: 73, Loss: 0.7192, Train: 78.11%, Valid: 75.45% Test: 75.66%\n",
      "Run: 05, Epoch: 74, Loss: 0.7136, Train: 78.30%, Valid: 75.99% Test: 75.88%\n",
      "Run: 05, Epoch: 75, Loss: 0.7095, Train: 78.02%, Valid: 76.41% Test: 76.32%\n",
      "Run: 05, Epoch: 76, Loss: 0.6759, Train: 77.84%, Valid: 76.41% Test: 74.56%\n",
      "Run: 05, Epoch: 77, Loss: 0.6827, Train: 77.75%, Valid: 76.95% Test: 74.12%\n",
      "Run: 05, Epoch: 78, Loss: 0.6580, Train: 78.66%, Valid: 76.41% Test: 75.66%\n",
      "Run: 05, Epoch: 79, Loss: 0.6727, Train: 79.49%, Valid: 78.74% Test: 77.19%\n",
      "Run: 05, Epoch: 80, Loss: 0.6773, Train: 79.03%, Valid: 77.37% Test: 75.88%\n",
      "Run: 05, Epoch: 81, Loss: 0.6962, Train: 79.21%, Valid: 77.23% Test: 76.97%\n",
      "Run: 05, Epoch: 82, Loss: 0.6752, Train: 79.95%, Valid: 76.54% Test: 76.75%\n",
      "Run: 05, Epoch: 83, Loss: 0.6746, Train: 79.76%, Valid: 76.27% Test: 76.10%\n",
      "Run: 05, Epoch: 84, Loss: 0.6695, Train: 80.49%, Valid: 76.27% Test: 78.73%\n",
      "Run: 05, Epoch: 85, Loss: 0.6811, Train: 81.41%, Valid: 77.78% Test: 76.10%\n",
      "Run: 05, Epoch: 86, Loss: 0.6679, Train: 79.30%, Valid: 76.68% Test: 76.54%\n",
      "Run: 05, Epoch: 87, Loss: 0.6690, Train: 79.67%, Valid: 78.46% Test: 77.19%\n",
      "Run: 05, Epoch: 88, Loss: 0.6899, Train: 79.12%, Valid: 78.05% Test: 74.12%\n",
      "Run: 05, Epoch: 89, Loss: 0.6593, Train: 78.94%, Valid: 78.60% Test: 74.12%\n",
      "Run: 05, Epoch: 90, Loss: 0.6612, Train: 80.59%, Valid: 78.19% Test: 77.63%\n",
      "Run: 05, Epoch: 91, Loss: 0.6346, Train: 81.50%, Valid: 78.74% Test: 77.63%\n",
      "Run: 05, Epoch: 92, Loss: 0.6526, Train: 81.68%, Valid: 79.15% Test: 79.39%\n",
      "Run: 05, Epoch: 93, Loss: 0.6818, Train: 80.77%, Valid: 78.46% Test: 78.51%\n",
      "Run: 05, Epoch: 94, Loss: 0.6281, Train: 81.50%, Valid: 77.50% Test: 79.39%\n",
      "Run: 05, Epoch: 95, Loss: 0.6732, Train: 82.69%, Valid: 79.29% Test: 79.82%\n",
      "Run: 05, Epoch: 96, Loss: 0.6627, Train: 82.42%, Valid: 79.42% Test: 76.54%\n",
      "Run: 05, Epoch: 97, Loss: 0.6508, Train: 82.33%, Valid: 79.29% Test: 75.44%\n",
      "Run: 05, Epoch: 98, Loss: 0.6487, Train: 82.14%, Valid: 78.46% Test: 76.10%\n",
      "Run: 05, Epoch: 99, Loss: 0.6337, Train: 82.33%, Valid: 79.01% Test: 77.63%\n",
      "Run: 05, Epoch: 100, Loss: 0.6446, Train: 83.06%, Valid: 78.33% Test: 78.07%\n",
      "Run 05:\n",
      "Highest Train: 83.06\n",
      "Highest Valid: 79.42\n",
      "  Final Train: 82.42\n",
      "   Final Test: 76.54\n",
      "Run: 06, Epoch: 01, Loss: 1.6934, Train: 19.87%, Valid: 19.48% Test: 24.12%\n",
      "Run: 06, Epoch: 02, Loss: 1.5242, Train: 21.79%, Valid: 20.16% Test: 18.20%\n",
      "Run: 06, Epoch: 03, Loss: 1.4407, Train: 23.63%, Valid: 22.36% Test: 19.52%\n",
      "Run: 06, Epoch: 04, Loss: 1.3677, Train: 27.11%, Valid: 25.93% Test: 22.37%\n",
      "Run: 06, Epoch: 05, Loss: 1.3189, Train: 35.26%, Valid: 35.80% Test: 34.43%\n",
      "Run: 06, Epoch: 06, Loss: 1.2743, Train: 33.52%, Valid: 32.65% Test: 36.84%\n",
      "Run: 06, Epoch: 07, Loss: 1.2355, Train: 34.43%, Valid: 33.47% Test: 38.38%\n",
      "Run: 06, Epoch: 08, Loss: 1.2213, Train: 38.55%, Valid: 35.94% Test: 42.32%\n",
      "Run: 06, Epoch: 09, Loss: 1.1812, Train: 42.77%, Valid: 41.56% Test: 44.30%\n",
      "Run: 06, Epoch: 10, Loss: 1.1546, Train: 51.74%, Valid: 48.83% Test: 51.97%\n",
      "Run: 06, Epoch: 11, Loss: 1.1154, Train: 59.34%, Valid: 57.06% Test: 57.02%\n",
      "Run: 06, Epoch: 12, Loss: 1.0984, Train: 62.36%, Valid: 57.34% Test: 58.99%\n",
      "Run: 06, Epoch: 13, Loss: 1.0843, Train: 63.55%, Valid: 58.57% Test: 61.40%\n",
      "Run: 06, Epoch: 14, Loss: 1.0576, Train: 64.38%, Valid: 61.04% Test: 61.40%\n",
      "Run: 06, Epoch: 15, Loss: 1.0150, Train: 62.27%, Valid: 60.49% Test: 60.53%\n",
      "Run: 06, Epoch: 16, Loss: 1.0128, Train: 59.25%, Valid: 59.26% Test: 58.77%\n",
      "Run: 06, Epoch: 17, Loss: 0.9875, Train: 60.07%, Valid: 60.22% Test: 59.65%\n",
      "Run: 06, Epoch: 18, Loss: 0.9767, Train: 64.56%, Valid: 61.59% Test: 63.82%\n",
      "Run: 06, Epoch: 19, Loss: 0.9566, Train: 69.60%, Valid: 64.75% Test: 67.76%\n",
      "Run: 06, Epoch: 20, Loss: 0.9415, Train: 71.15%, Valid: 68.04% Test: 68.64%\n",
      "Run: 06, Epoch: 21, Loss: 0.9181, Train: 71.25%, Valid: 65.57% Test: 66.23%\n",
      "Run: 06, Epoch: 22, Loss: 0.8969, Train: 73.81%, Valid: 67.22% Test: 68.42%\n",
      "Run: 06, Epoch: 23, Loss: 0.8890, Train: 71.15%, Valid: 66.26% Test: 66.67%\n",
      "Run: 06, Epoch: 24, Loss: 0.8985, Train: 69.32%, Valid: 66.39% Test: 64.91%\n",
      "Run: 06, Epoch: 25, Loss: 0.8903, Train: 67.40%, Valid: 64.61% Test: 61.84%\n",
      "Run: 06, Epoch: 26, Loss: 0.8816, Train: 68.59%, Valid: 64.47% Test: 62.72%\n",
      "Run: 06, Epoch: 27, Loss: 0.8648, Train: 70.70%, Valid: 67.35% Test: 65.13%\n",
      "Run: 06, Epoch: 28, Loss: 0.8678, Train: 74.73%, Valid: 68.86% Test: 69.52%\n",
      "Run: 06, Epoch: 29, Loss: 0.8260, Train: 74.63%, Valid: 68.59% Test: 70.39%\n",
      "Run: 06, Epoch: 30, Loss: 0.8429, Train: 71.34%, Valid: 68.31% Test: 69.52%\n",
      "Run: 06, Epoch: 31, Loss: 0.8045, Train: 71.70%, Valid: 67.90% Test: 67.32%\n",
      "Run: 06, Epoch: 32, Loss: 0.8239, Train: 72.25%, Valid: 66.53% Test: 66.89%\n",
      "Run: 06, Epoch: 33, Loss: 0.8043, Train: 72.34%, Valid: 67.49% Test: 66.67%\n",
      "Run: 06, Epoch: 34, Loss: 0.8124, Train: 74.45%, Valid: 69.68% Test: 67.76%\n",
      "Run: 06, Epoch: 35, Loss: 0.7949, Train: 76.37%, Valid: 70.92% Test: 70.18%\n",
      "Run: 06, Epoch: 36, Loss: 0.7715, Train: 77.84%, Valid: 72.70% Test: 74.12%\n",
      "Run: 06, Epoch: 37, Loss: 0.7878, Train: 76.47%, Valid: 70.64% Test: 71.71%\n",
      "Run: 06, Epoch: 38, Loss: 0.7818, Train: 75.55%, Valid: 70.23% Test: 70.39%\n",
      "Run: 06, Epoch: 39, Loss: 0.7646, Train: 75.73%, Valid: 70.64% Test: 73.03%\n",
      "Run: 06, Epoch: 40, Loss: 0.7728, Train: 78.75%, Valid: 72.70% Test: 74.78%\n",
      "Run: 06, Epoch: 41, Loss: 0.7715, Train: 76.74%, Valid: 71.88% Test: 73.46%\n",
      "Run: 06, Epoch: 42, Loss: 0.7828, Train: 76.01%, Valid: 73.25% Test: 72.15%\n",
      "Run: 06, Epoch: 43, Loss: 0.7629, Train: 78.02%, Valid: 71.88% Test: 74.34%\n",
      "Run: 06, Epoch: 44, Loss: 0.7522, Train: 78.02%, Valid: 71.88% Test: 74.56%\n",
      "Run: 06, Epoch: 45, Loss: 0.7597, Train: 79.40%, Valid: 72.57% Test: 75.00%\n",
      "Run: 06, Epoch: 46, Loss: 0.7564, Train: 78.21%, Valid: 72.98% Test: 75.66%\n",
      "Run: 06, Epoch: 47, Loss: 0.7463, Train: 77.01%, Valid: 71.60% Test: 75.66%\n",
      "Run: 06, Epoch: 48, Loss: 0.7816, Train: 78.48%, Valid: 72.57% Test: 75.00%\n",
      "Run: 06, Epoch: 49, Loss: 0.7472, Train: 78.75%, Valid: 72.57% Test: 75.00%\n",
      "Run: 06, Epoch: 50, Loss: 0.7227, Train: 79.03%, Valid: 73.53% Test: 74.56%\n",
      "Run: 06, Epoch: 51, Loss: 0.7211, Train: 79.49%, Valid: 73.80% Test: 76.10%\n",
      "Run: 06, Epoch: 52, Loss: 0.7023, Train: 80.04%, Valid: 72.70% Test: 75.66%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 06, Epoch: 53, Loss: 0.7508, Train: 79.30%, Valid: 73.80% Test: 75.22%\n",
      "Run: 06, Epoch: 54, Loss: 0.6968, Train: 79.30%, Valid: 73.94% Test: 76.97%\n",
      "Run: 06, Epoch: 55, Loss: 0.6947, Train: 77.20%, Valid: 73.39% Test: 76.97%\n",
      "Run: 06, Epoch: 56, Loss: 0.7344, Train: 76.83%, Valid: 73.11% Test: 76.54%\n",
      "Run: 06, Epoch: 57, Loss: 0.6946, Train: 79.21%, Valid: 74.21% Test: 76.75%\n",
      "Run: 06, Epoch: 58, Loss: 0.7161, Train: 80.68%, Valid: 74.90% Test: 74.78%\n",
      "Run: 06, Epoch: 59, Loss: 0.6933, Train: 80.77%, Valid: 73.11% Test: 76.10%\n",
      "Run: 06, Epoch: 60, Loss: 0.6789, Train: 80.77%, Valid: 72.84% Test: 75.66%\n",
      "Run: 06, Epoch: 61, Loss: 0.7112, Train: 81.87%, Valid: 74.21% Test: 76.97%\n",
      "Run: 06, Epoch: 62, Loss: 0.6625, Train: 81.78%, Valid: 74.90% Test: 77.85%\n",
      "Run: 06, Epoch: 63, Loss: 0.6906, Train: 79.95%, Valid: 73.39% Test: 78.73%\n",
      "Run: 06, Epoch: 64, Loss: 0.6908, Train: 80.40%, Valid: 73.53% Test: 78.07%\n",
      "Run: 06, Epoch: 65, Loss: 0.6860, Train: 81.32%, Valid: 74.07% Test: 77.85%\n",
      "Run: 06, Epoch: 66, Loss: 0.6759, Train: 81.32%, Valid: 75.03% Test: 76.97%\n",
      "Run: 06, Epoch: 67, Loss: 0.6568, Train: 81.78%, Valid: 75.31% Test: 76.75%\n",
      "Run: 06, Epoch: 68, Loss: 0.6752, Train: 81.04%, Valid: 75.58% Test: 78.07%\n",
      "Run: 06, Epoch: 69, Loss: 0.6517, Train: 81.32%, Valid: 75.86% Test: 77.19%\n",
      "Run: 06, Epoch: 70, Loss: 0.7021, Train: 81.23%, Valid: 76.13% Test: 79.17%\n",
      "Run: 06, Epoch: 71, Loss: 0.6699, Train: 79.85%, Valid: 73.80% Test: 78.29%\n",
      "Run: 06, Epoch: 72, Loss: 0.6846, Train: 80.86%, Valid: 75.17% Test: 78.73%\n",
      "Run: 06, Epoch: 73, Loss: 0.6676, Train: 81.78%, Valid: 75.58% Test: 77.63%\n",
      "Run: 06, Epoch: 74, Loss: 0.6756, Train: 81.87%, Valid: 74.21% Test: 76.75%\n",
      "Run: 06, Epoch: 75, Loss: 0.6796, Train: 81.23%, Valid: 75.17% Test: 76.54%\n",
      "Run: 06, Epoch: 76, Loss: 0.6416, Train: 81.68%, Valid: 75.72% Test: 76.75%\n",
      "Run: 06, Epoch: 77, Loss: 0.6375, Train: 81.96%, Valid: 75.31% Test: 76.10%\n",
      "Run: 06, Epoch: 78, Loss: 0.6436, Train: 83.15%, Valid: 77.09% Test: 76.97%\n",
      "Run: 06, Epoch: 79, Loss: 0.6448, Train: 81.41%, Valid: 76.27% Test: 77.19%\n",
      "Run: 06, Epoch: 80, Loss: 0.6439, Train: 80.59%, Valid: 75.86% Test: 76.10%\n",
      "Run: 06, Epoch: 81, Loss: 0.6441, Train: 82.97%, Valid: 76.27% Test: 78.95%\n",
      "Run: 06, Epoch: 82, Loss: 0.6127, Train: 83.70%, Valid: 75.45% Test: 79.61%\n",
      "Run: 06, Epoch: 83, Loss: 0.6216, Train: 82.88%, Valid: 75.45% Test: 79.61%\n",
      "Run: 06, Epoch: 84, Loss: 0.6382, Train: 82.97%, Valid: 74.90% Test: 79.82%\n",
      "Run: 06, Epoch: 85, Loss: 0.6551, Train: 80.68%, Valid: 75.72% Test: 78.29%\n",
      "Run: 06, Epoch: 86, Loss: 0.6314, Train: 82.88%, Valid: 75.72% Test: 79.39%\n",
      "Run: 06, Epoch: 87, Loss: 0.6056, Train: 83.88%, Valid: 77.37% Test: 78.73%\n",
      "Run: 06, Epoch: 88, Loss: 0.6330, Train: 84.07%, Valid: 76.54% Test: 77.63%\n",
      "Run: 06, Epoch: 89, Loss: 0.6245, Train: 83.15%, Valid: 76.27% Test: 78.51%\n",
      "Run: 06, Epoch: 90, Loss: 0.6074, Train: 83.24%, Valid: 75.99% Test: 78.51%\n",
      "Run: 06, Epoch: 91, Loss: 0.6042, Train: 83.15%, Valid: 75.99% Test: 78.95%\n",
      "Run: 06, Epoch: 92, Loss: 0.6248, Train: 83.33%, Valid: 77.09% Test: 79.61%\n",
      "Run: 06, Epoch: 93, Loss: 0.6338, Train: 78.85%, Valid: 74.21% Test: 76.10%\n",
      "Run: 06, Epoch: 94, Loss: 0.6357, Train: 80.04%, Valid: 74.62% Test: 76.97%\n",
      "Run: 06, Epoch: 95, Loss: 0.5848, Train: 82.51%, Valid: 75.31% Test: 74.78%\n",
      "Run: 06, Epoch: 96, Loss: 0.6028, Train: 81.41%, Valid: 74.76% Test: 76.32%\n",
      "Run: 06, Epoch: 97, Loss: 0.6370, Train: 81.32%, Valid: 75.17% Test: 79.39%\n",
      "Run: 06, Epoch: 98, Loss: 0.6413, Train: 83.15%, Valid: 78.33% Test: 80.04%\n",
      "Run: 06, Epoch: 99, Loss: 0.5910, Train: 81.68%, Valid: 75.72% Test: 76.54%\n",
      "Run: 06, Epoch: 100, Loss: 0.5846, Train: 80.77%, Valid: 74.76% Test: 75.00%\n",
      "Run 06:\n",
      "Highest Train: 84.07\n",
      "Highest Valid: 78.33\n",
      "  Final Train: 83.15\n",
      "   Final Test: 80.04\n",
      "Run: 07, Epoch: 01, Loss: 1.6342, Train: 20.42%, Valid: 17.42% Test: 22.59%\n",
      "Run: 07, Epoch: 02, Loss: 1.4946, Train: 31.50%, Valid: 31.69% Test: 29.39%\n",
      "Run: 07, Epoch: 03, Loss: 1.4015, Train: 22.99%, Valid: 24.42% Test: 20.18%\n",
      "Run: 07, Epoch: 04, Loss: 1.3435, Train: 23.08%, Valid: 24.42% Test: 20.18%\n",
      "Run: 07, Epoch: 05, Loss: 1.2995, Train: 32.78%, Valid: 33.88% Test: 29.82%\n",
      "Run: 07, Epoch: 06, Loss: 1.2819, Train: 36.81%, Valid: 33.47% Test: 33.99%\n",
      "Run: 07, Epoch: 07, Loss: 1.2378, Train: 42.77%, Valid: 37.86% Test: 38.16%\n",
      "Run: 07, Epoch: 08, Loss: 1.2090, Train: 51.19%, Valid: 44.31% Test: 48.25%\n",
      "Run: 07, Epoch: 09, Loss: 1.1836, Train: 54.58%, Valid: 48.83% Test: 50.88%\n",
      "Run: 07, Epoch: 10, Loss: 1.1512, Train: 52.93%, Valid: 49.11% Test: 48.46%\n",
      "Run: 07, Epoch: 11, Loss: 1.1351, Train: 51.37%, Valid: 49.93% Test: 48.68%\n",
      "Run: 07, Epoch: 12, Loss: 1.0818, Train: 53.21%, Valid: 52.26% Test: 50.88%\n",
      "Run: 07, Epoch: 13, Loss: 1.0909, Train: 55.22%, Valid: 53.77% Test: 50.88%\n",
      "Run: 07, Epoch: 14, Loss: 1.0564, Train: 56.96%, Valid: 54.60% Test: 53.07%\n",
      "Run: 07, Epoch: 15, Loss: 1.0462, Train: 59.07%, Valid: 56.65% Test: 57.02%\n",
      "Run: 07, Epoch: 16, Loss: 1.0319, Train: 57.88%, Valid: 55.56% Test: 55.70%\n",
      "Run: 07, Epoch: 17, Loss: 0.9988, Train: 57.88%, Valid: 55.28% Test: 57.02%\n",
      "Run: 07, Epoch: 18, Loss: 0.9839, Train: 58.97%, Valid: 55.69% Test: 57.24%\n",
      "Run: 07, Epoch: 19, Loss: 0.9709, Train: 58.97%, Valid: 56.10% Test: 57.46%\n",
      "Run: 07, Epoch: 20, Loss: 0.9450, Train: 60.35%, Valid: 59.40% Test: 61.84%\n",
      "Run: 07, Epoch: 21, Loss: 0.9277, Train: 62.00%, Valid: 61.45% Test: 64.25%\n",
      "Run: 07, Epoch: 22, Loss: 0.9237, Train: 63.28%, Valid: 59.67% Test: 61.18%\n",
      "Run: 07, Epoch: 23, Loss: 0.9040, Train: 67.12%, Valid: 64.20% Test: 63.60%\n",
      "Run: 07, Epoch: 24, Loss: 0.8872, Train: 68.41%, Valid: 63.92% Test: 63.82%\n",
      "Run: 07, Epoch: 25, Loss: 0.8804, Train: 70.24%, Valid: 64.20% Test: 65.57%\n",
      "Run: 07, Epoch: 26, Loss: 0.8595, Train: 67.22%, Valid: 62.14% Test: 65.13%\n",
      "Run: 07, Epoch: 27, Loss: 0.8442, Train: 68.32%, Valid: 62.55% Test: 65.13%\n",
      "Run: 07, Epoch: 28, Loss: 0.8335, Train: 69.14%, Valid: 66.67% Test: 65.35%\n",
      "Run: 07, Epoch: 29, Loss: 0.8315, Train: 69.87%, Valid: 66.26% Test: 65.79%\n",
      "Run: 07, Epoch: 30, Loss: 0.8455, Train: 71.06%, Valid: 66.80% Test: 67.54%\n",
      "Run: 07, Epoch: 31, Loss: 0.8292, Train: 73.53%, Valid: 69.27% Test: 68.42%\n",
      "Run: 07, Epoch: 32, Loss: 0.8464, Train: 71.25%, Valid: 66.67% Test: 67.32%\n",
      "Run: 07, Epoch: 33, Loss: 0.8068, Train: 74.73%, Valid: 68.72% Test: 68.86%\n",
      "Run: 07, Epoch: 34, Loss: 0.8223, Train: 75.82%, Valid: 70.23% Test: 72.37%\n",
      "Run: 07, Epoch: 35, Loss: 0.7678, Train: 75.64%, Valid: 69.41% Test: 69.08%\n",
      "Run: 07, Epoch: 36, Loss: 0.7853, Train: 68.96%, Valid: 65.98% Test: 66.67%\n",
      "Run: 07, Epoch: 37, Loss: 0.7623, Train: 73.63%, Valid: 69.68% Test: 69.30%\n",
      "Run: 07, Epoch: 38, Loss: 0.7785, Train: 74.82%, Valid: 69.82% Test: 72.37%\n",
      "Run: 07, Epoch: 39, Loss: 0.7722, Train: 76.83%, Valid: 73.53% Test: 75.00%\n",
      "Run: 07, Epoch: 40, Loss: 0.7571, Train: 76.10%, Valid: 74.21% Test: 76.10%\n",
      "Run: 07, Epoch: 41, Loss: 0.7400, Train: 75.73%, Valid: 72.15% Test: 72.81%\n",
      "Run: 07, Epoch: 42, Loss: 0.7626, Train: 75.46%, Valid: 70.23% Test: 69.96%\n",
      "Run: 07, Epoch: 43, Loss: 0.7350, Train: 77.84%, Valid: 72.57% Test: 74.12%\n",
      "Run: 07, Epoch: 44, Loss: 0.7006, Train: 79.12%, Valid: 74.07% Test: 75.88%\n",
      "Run: 07, Epoch: 45, Loss: 0.7411, Train: 77.93%, Valid: 73.39% Test: 75.44%\n",
      "Run: 07, Epoch: 46, Loss: 0.7164, Train: 77.38%, Valid: 70.10% Test: 72.37%\n",
      "Run: 07, Epoch: 47, Loss: 0.7070, Train: 74.82%, Valid: 66.26% Test: 69.74%\n",
      "Run: 07, Epoch: 48, Loss: 0.7553, Train: 75.55%, Valid: 68.04% Test: 70.61%\n",
      "Run: 07, Epoch: 49, Loss: 0.7375, Train: 75.37%, Valid: 69.68% Test: 70.39%\n",
      "Run: 07, Epoch: 50, Loss: 0.7201, Train: 77.47%, Valid: 73.94% Test: 73.90%\n",
      "Run: 07, Epoch: 51, Loss: 0.7345, Train: 78.94%, Valid: 74.49% Test: 75.44%\n",
      "Run: 07, Epoch: 52, Loss: 0.6941, Train: 79.30%, Valid: 73.94% Test: 75.88%\n",
      "Run: 07, Epoch: 53, Loss: 0.6828, Train: 76.74%, Valid: 72.84% Test: 75.22%\n",
      "Run: 07, Epoch: 54, Loss: 0.7008, Train: 76.10%, Valid: 72.15% Test: 74.56%\n",
      "Run: 07, Epoch: 55, Loss: 0.6849, Train: 78.57%, Valid: 72.84% Test: 75.66%\n",
      "Run: 07, Epoch: 56, Loss: 0.6843, Train: 74.82%, Valid: 69.96% Test: 69.52%\n",
      "Run: 07, Epoch: 57, Loss: 0.6852, Train: 78.57%, Valid: 72.15% Test: 72.37%\n",
      "Run: 07, Epoch: 58, Loss: 0.6409, Train: 79.95%, Valid: 75.03% Test: 76.75%\n",
      "Run: 07, Epoch: 59, Loss: 0.6531, Train: 75.37%, Valid: 71.60% Test: 74.12%\n",
      "Run: 07, Epoch: 60, Loss: 0.6908, Train: 74.54%, Valid: 71.60% Test: 72.81%\n",
      "Run: 07, Epoch: 61, Loss: 0.6633, Train: 78.11%, Valid: 74.49% Test: 76.10%\n",
      "Run: 07, Epoch: 62, Loss: 0.6912, Train: 81.04%, Valid: 76.41% Test: 78.51%\n",
      "Run: 07, Epoch: 63, Loss: 0.6598, Train: 81.96%, Valid: 76.13% Test: 78.29%\n",
      "Run: 07, Epoch: 64, Loss: 0.6405, Train: 81.78%, Valid: 77.23% Test: 79.61%\n",
      "Run: 07, Epoch: 65, Loss: 0.6785, Train: 83.61%, Valid: 78.46% Test: 79.82%\n",
      "Run: 07, Epoch: 66, Loss: 0.6605, Train: 82.14%, Valid: 76.82% Test: 78.29%\n",
      "Run: 07, Epoch: 67, Loss: 0.6693, Train: 80.49%, Valid: 75.03% Test: 77.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 07, Epoch: 68, Loss: 0.6313, Train: 80.59%, Valid: 73.80% Test: 77.41%\n",
      "Run: 07, Epoch: 69, Loss: 0.6456, Train: 80.86%, Valid: 73.11% Test: 77.41%\n",
      "Run: 07, Epoch: 70, Loss: 0.6613, Train: 80.68%, Valid: 74.76% Test: 76.54%\n",
      "Run: 07, Epoch: 71, Loss: 0.6504, Train: 83.15%, Valid: 78.74% Test: 80.92%\n",
      "Run: 07, Epoch: 72, Loss: 0.6315, Train: 79.03%, Valid: 76.68% Test: 79.61%\n",
      "Run: 07, Epoch: 73, Loss: 0.6568, Train: 78.66%, Valid: 75.72% Test: 78.95%\n",
      "Run: 07, Epoch: 74, Loss: 0.6655, Train: 83.79%, Valid: 78.46% Test: 81.14%\n",
      "Run: 07, Epoch: 75, Loss: 0.6463, Train: 82.05%, Valid: 75.72% Test: 80.48%\n",
      "Run: 07, Epoch: 76, Loss: 0.6379, Train: 80.40%, Valid: 74.90% Test: 77.85%\n",
      "Run: 07, Epoch: 77, Loss: 0.6155, Train: 80.04%, Valid: 73.39% Test: 77.63%\n",
      "Run: 07, Epoch: 78, Loss: 0.6278, Train: 81.23%, Valid: 74.62% Test: 78.51%\n",
      "Run: 07, Epoch: 79, Loss: 0.6668, Train: 81.68%, Valid: 75.86% Test: 78.73%\n",
      "Run: 07, Epoch: 80, Loss: 0.6033, Train: 82.05%, Valid: 75.99% Test: 78.07%\n",
      "Run: 07, Epoch: 81, Loss: 0.5980, Train: 81.14%, Valid: 76.13% Test: 78.51%\n",
      "Run: 07, Epoch: 82, Loss: 0.6578, Train: 80.68%, Valid: 76.82% Test: 77.41%\n",
      "Run: 07, Epoch: 83, Loss: 0.6163, Train: 80.95%, Valid: 76.82% Test: 79.17%\n",
      "Run: 07, Epoch: 84, Loss: 0.6508, Train: 82.42%, Valid: 76.13% Test: 80.48%\n",
      "Run: 07, Epoch: 85, Loss: 0.6352, Train: 81.78%, Valid: 76.54% Test: 79.61%\n",
      "Run: 07, Epoch: 86, Loss: 0.6031, Train: 82.33%, Valid: 76.54% Test: 78.73%\n",
      "Run: 07, Epoch: 87, Loss: 0.5983, Train: 83.97%, Valid: 76.82% Test: 79.17%\n",
      "Run: 07, Epoch: 88, Loss: 0.5913, Train: 83.97%, Valid: 77.78% Test: 80.04%\n",
      "Run: 07, Epoch: 89, Loss: 0.5837, Train: 83.06%, Valid: 79.15% Test: 77.63%\n",
      "Run: 07, Epoch: 90, Loss: 0.6051, Train: 82.69%, Valid: 78.46% Test: 77.19%\n",
      "Run: 07, Epoch: 91, Loss: 0.6162, Train: 82.88%, Valid: 78.74% Test: 80.04%\n",
      "Run: 07, Epoch: 92, Loss: 0.5947, Train: 81.87%, Valid: 76.82% Test: 78.95%\n",
      "Run: 07, Epoch: 93, Loss: 0.5662, Train: 81.96%, Valid: 75.99% Test: 78.29%\n",
      "Run: 07, Epoch: 94, Loss: 0.5955, Train: 82.42%, Valid: 77.64% Test: 78.07%\n",
      "Run: 07, Epoch: 95, Loss: 0.5868, Train: 81.78%, Valid: 76.27% Test: 79.61%\n",
      "Run: 07, Epoch: 96, Loss: 0.5958, Train: 80.04%, Valid: 75.17% Test: 78.07%\n",
      "Run: 07, Epoch: 97, Loss: 0.6045, Train: 81.50%, Valid: 78.19% Test: 78.73%\n",
      "Run: 07, Epoch: 98, Loss: 0.5958, Train: 82.97%, Valid: 78.33% Test: 79.61%\n",
      "Run: 07, Epoch: 99, Loss: 0.5785, Train: 83.06%, Valid: 78.19% Test: 79.61%\n",
      "Run: 07, Epoch: 100, Loss: 0.5889, Train: 82.78%, Valid: 78.33% Test: 80.04%\n",
      "Run 07:\n",
      "Highest Train: 83.97\n",
      "Highest Valid: 79.15\n",
      "  Final Train: 83.06\n",
      "   Final Test: 77.63\n",
      "Run: 08, Epoch: 01, Loss: 1.6423, Train: 22.53%, Valid: 24.42% Test: 21.27%\n",
      "Run: 08, Epoch: 02, Loss: 1.4977, Train: 32.05%, Valid: 29.22% Test: 23.90%\n",
      "Run: 08, Epoch: 03, Loss: 1.4260, Train: 32.51%, Valid: 29.49% Test: 24.12%\n",
      "Run: 08, Epoch: 04, Loss: 1.3699, Train: 35.44%, Valid: 36.08% Test: 35.96%\n",
      "Run: 08, Epoch: 05, Loss: 1.3093, Train: 35.90%, Valid: 34.98% Test: 41.67%\n",
      "Run: 08, Epoch: 06, Loss: 1.2935, Train: 38.19%, Valid: 36.49% Test: 35.53%\n",
      "Run: 08, Epoch: 07, Loss: 1.2508, Train: 38.00%, Valid: 36.90% Test: 36.84%\n",
      "Run: 08, Epoch: 08, Loss: 1.2128, Train: 36.45%, Valid: 35.12% Test: 34.21%\n",
      "Run: 08, Epoch: 09, Loss: 1.2081, Train: 40.57%, Valid: 38.00% Test: 35.75%\n",
      "Run: 08, Epoch: 10, Loss: 1.1557, Train: 42.77%, Valid: 40.19% Test: 37.06%\n",
      "Run: 08, Epoch: 11, Loss: 1.1221, Train: 39.74%, Valid: 37.86% Test: 35.31%\n",
      "Run: 08, Epoch: 12, Loss: 1.1031, Train: 37.18%, Valid: 35.39% Test: 32.46%\n",
      "Run: 08, Epoch: 13, Loss: 1.1094, Train: 37.64%, Valid: 36.49% Test: 33.11%\n",
      "Run: 08, Epoch: 14, Loss: 1.0767, Train: 37.45%, Valid: 36.90% Test: 33.11%\n",
      "Run: 08, Epoch: 15, Loss: 1.0650, Train: 42.12%, Valid: 42.11% Test: 38.60%\n",
      "Run: 08, Epoch: 16, Loss: 1.0505, Train: 42.40%, Valid: 42.52% Test: 39.91%\n",
      "Run: 08, Epoch: 17, Loss: 1.0189, Train: 39.29%, Valid: 38.82% Test: 35.96%\n",
      "Run: 08, Epoch: 18, Loss: 1.0426, Train: 43.04%, Valid: 42.52% Test: 40.57%\n",
      "Run: 08, Epoch: 19, Loss: 1.0049, Train: 52.66%, Valid: 50.75% Test: 46.71%\n",
      "Run: 08, Epoch: 20, Loss: 0.9698, Train: 57.60%, Valid: 56.10% Test: 51.54%\n",
      "Run: 08, Epoch: 21, Loss: 0.9544, Train: 59.89%, Valid: 57.61% Test: 55.70%\n",
      "Run: 08, Epoch: 22, Loss: 0.9679, Train: 60.62%, Valid: 59.67% Test: 57.68%\n",
      "Run: 08, Epoch: 23, Loss: 0.9445, Train: 62.45%, Valid: 61.73% Test: 55.04%\n",
      "Run: 08, Epoch: 24, Loss: 0.9165, Train: 62.18%, Valid: 60.22% Test: 52.63%\n",
      "Run: 08, Epoch: 25, Loss: 0.9091, Train: 61.36%, Valid: 60.77% Test: 54.17%\n",
      "Run: 08, Epoch: 26, Loss: 0.8638, Train: 65.02%, Valid: 63.10% Test: 58.11%\n",
      "Run: 08, Epoch: 27, Loss: 0.8692, Train: 66.12%, Valid: 64.61% Test: 60.09%\n",
      "Run: 08, Epoch: 28, Loss: 0.8446, Train: 65.84%, Valid: 64.33% Test: 60.96%\n",
      "Run: 08, Epoch: 29, Loss: 0.8887, Train: 68.86%, Valid: 66.12% Test: 62.06%\n",
      "Run: 08, Epoch: 30, Loss: 0.8569, Train: 71.15%, Valid: 67.76% Test: 63.16%\n",
      "Run: 08, Epoch: 31, Loss: 0.8569, Train: 72.44%, Valid: 68.59% Test: 63.38%\n",
      "Run: 08, Epoch: 32, Loss: 0.8477, Train: 74.82%, Valid: 71.88% Test: 66.67%\n",
      "Run: 08, Epoch: 33, Loss: 0.8359, Train: 74.45%, Valid: 71.88% Test: 67.11%\n",
      "Run: 08, Epoch: 34, Loss: 0.8327, Train: 73.44%, Valid: 71.33% Test: 66.67%\n",
      "Run: 08, Epoch: 35, Loss: 0.8174, Train: 72.07%, Valid: 68.59% Test: 63.38%\n",
      "Run: 08, Epoch: 36, Loss: 0.8149, Train: 71.98%, Valid: 69.82% Test: 64.04%\n",
      "Run: 08, Epoch: 37, Loss: 0.8193, Train: 74.54%, Valid: 70.37% Test: 67.98%\n",
      "Run: 08, Epoch: 38, Loss: 0.8032, Train: 77.20%, Valid: 72.43% Test: 72.81%\n",
      "Run: 08, Epoch: 39, Loss: 0.7839, Train: 78.11%, Valid: 73.39% Test: 70.61%\n",
      "Run: 08, Epoch: 40, Loss: 0.8000, Train: 78.02%, Valid: 72.02% Test: 70.39%\n",
      "Run: 08, Epoch: 41, Loss: 0.7578, Train: 75.92%, Valid: 72.57% Test: 67.11%\n",
      "Run: 08, Epoch: 42, Loss: 0.7991, Train: 74.18%, Valid: 70.51% Test: 66.23%\n",
      "Run: 08, Epoch: 43, Loss: 0.7505, Train: 72.62%, Valid: 69.68% Test: 62.50%\n",
      "Run: 08, Epoch: 44, Loss: 0.7640, Train: 73.08%, Valid: 70.23% Test: 63.38%\n",
      "Run: 08, Epoch: 45, Loss: 0.7461, Train: 71.79%, Valid: 69.00% Test: 63.38%\n",
      "Run: 08, Epoch: 46, Loss: 0.7235, Train: 69.78%, Valid: 68.86% Test: 62.94%\n",
      "Run: 08, Epoch: 47, Loss: 0.7552, Train: 71.06%, Valid: 69.14% Test: 61.84%\n",
      "Run: 08, Epoch: 48, Loss: 0.7498, Train: 74.08%, Valid: 69.27% Test: 60.53%\n",
      "Run: 08, Epoch: 49, Loss: 0.7149, Train: 74.54%, Valid: 70.37% Test: 62.50%\n",
      "Run: 08, Epoch: 50, Loss: 0.7351, Train: 74.82%, Valid: 72.15% Test: 66.23%\n",
      "Run: 08, Epoch: 51, Loss: 0.7428, Train: 73.17%, Valid: 71.19% Test: 66.89%\n",
      "Run: 08, Epoch: 52, Loss: 0.7046, Train: 75.46%, Valid: 72.29% Test: 69.52%\n",
      "Run: 08, Epoch: 53, Loss: 0.7179, Train: 78.85%, Valid: 74.21% Test: 71.27%\n",
      "Run: 08, Epoch: 54, Loss: 0.7296, Train: 80.22%, Valid: 75.58% Test: 72.59%\n",
      "Run: 08, Epoch: 55, Loss: 0.7039, Train: 79.21%, Valid: 74.21% Test: 72.15%\n",
      "Run: 08, Epoch: 56, Loss: 0.6690, Train: 80.13%, Valid: 75.58% Test: 73.68%\n",
      "Run: 08, Epoch: 57, Loss: 0.7555, Train: 80.95%, Valid: 76.82% Test: 76.32%\n",
      "Run: 08, Epoch: 58, Loss: 0.6901, Train: 81.23%, Valid: 75.31% Test: 75.88%\n",
      "Run: 08, Epoch: 59, Loss: 0.7172, Train: 81.59%, Valid: 75.99% Test: 75.22%\n",
      "Run: 08, Epoch: 60, Loss: 0.6954, Train: 81.32%, Valid: 75.99% Test: 75.22%\n",
      "Run: 08, Epoch: 61, Loss: 0.6900, Train: 80.22%, Valid: 75.72% Test: 73.68%\n",
      "Run: 08, Epoch: 62, Loss: 0.6831, Train: 80.95%, Valid: 76.54% Test: 73.90%\n",
      "Run: 08, Epoch: 63, Loss: 0.6742, Train: 80.31%, Valid: 76.27% Test: 73.46%\n",
      "Run: 08, Epoch: 64, Loss: 0.6452, Train: 82.23%, Valid: 76.95% Test: 75.66%\n",
      "Run: 08, Epoch: 65, Loss: 0.6909, Train: 80.31%, Valid: 75.86% Test: 73.46%\n",
      "Run: 08, Epoch: 66, Loss: 0.6927, Train: 79.49%, Valid: 74.35% Test: 72.37%\n",
      "Run: 08, Epoch: 67, Loss: 0.6920, Train: 81.78%, Valid: 75.72% Test: 73.03%\n",
      "Run: 08, Epoch: 68, Loss: 0.6672, Train: 81.96%, Valid: 75.99% Test: 73.25%\n",
      "Run: 08, Epoch: 69, Loss: 0.6661, Train: 80.77%, Valid: 74.21% Test: 73.90%\n",
      "Run: 08, Epoch: 70, Loss: 0.6731, Train: 76.92%, Valid: 70.64% Test: 72.81%\n",
      "Run: 08, Epoch: 71, Loss: 0.6737, Train: 75.55%, Valid: 71.47% Test: 73.03%\n",
      "Run: 08, Epoch: 72, Loss: 0.6494, Train: 79.40%, Valid: 75.31% Test: 75.88%\n",
      "Run: 08, Epoch: 73, Loss: 0.6603, Train: 81.32%, Valid: 76.82% Test: 76.75%\n",
      "Run: 08, Epoch: 74, Loss: 0.6695, Train: 82.33%, Valid: 78.33% Test: 76.10%\n",
      "Run: 08, Epoch: 75, Loss: 0.6577, Train: 83.42%, Valid: 78.74% Test: 75.44%\n",
      "Run: 08, Epoch: 76, Loss: 0.6513, Train: 83.70%, Valid: 78.46% Test: 75.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 08, Epoch: 77, Loss: 0.6574, Train: 83.24%, Valid: 76.82% Test: 75.22%\n",
      "Run: 08, Epoch: 78, Loss: 0.6381, Train: 82.78%, Valid: 76.54% Test: 74.12%\n",
      "Run: 08, Epoch: 79, Loss: 0.6307, Train: 82.33%, Valid: 75.17% Test: 74.34%\n",
      "Run: 08, Epoch: 80, Loss: 0.6345, Train: 82.51%, Valid: 74.21% Test: 76.10%\n",
      "Run: 08, Epoch: 81, Loss: 0.6098, Train: 80.95%, Valid: 74.07% Test: 76.54%\n",
      "Run: 08, Epoch: 82, Loss: 0.6589, Train: 81.41%, Valid: 74.62% Test: 76.75%\n",
      "Run: 08, Epoch: 83, Loss: 0.6440, Train: 82.97%, Valid: 76.13% Test: 76.32%\n",
      "Run: 08, Epoch: 84, Loss: 0.6390, Train: 83.24%, Valid: 77.50% Test: 75.44%\n",
      "Run: 08, Epoch: 85, Loss: 0.6267, Train: 82.05%, Valid: 75.03% Test: 75.66%\n",
      "Run: 08, Epoch: 86, Loss: 0.6325, Train: 82.42%, Valid: 77.09% Test: 75.22%\n",
      "Run: 08, Epoch: 87, Loss: 0.6145, Train: 83.42%, Valid: 78.33% Test: 76.97%\n",
      "Run: 08, Epoch: 88, Loss: 0.6152, Train: 84.34%, Valid: 76.54% Test: 74.34%\n",
      "Run: 08, Epoch: 89, Loss: 0.6009, Train: 83.06%, Valid: 75.31% Test: 74.56%\n",
      "Run: 08, Epoch: 90, Loss: 0.6012, Train: 81.78%, Valid: 76.13% Test: 74.56%\n",
      "Run: 08, Epoch: 91, Loss: 0.6163, Train: 82.60%, Valid: 75.99% Test: 72.37%\n",
      "Run: 08, Epoch: 92, Loss: 0.5792, Train: 83.79%, Valid: 77.37% Test: 73.25%\n",
      "Run: 08, Epoch: 93, Loss: 0.5986, Train: 84.52%, Valid: 78.46% Test: 75.66%\n",
      "Run: 08, Epoch: 94, Loss: 0.5814, Train: 83.97%, Valid: 78.33% Test: 76.75%\n",
      "Run: 08, Epoch: 95, Loss: 0.5901, Train: 83.88%, Valid: 77.78% Test: 76.32%\n",
      "Run: 08, Epoch: 96, Loss: 0.5925, Train: 83.70%, Valid: 76.68% Test: 77.41%\n",
      "Run: 08, Epoch: 97, Loss: 0.5790, Train: 83.33%, Valid: 77.23% Test: 77.85%\n",
      "Run: 08, Epoch: 98, Loss: 0.5800, Train: 84.52%, Valid: 77.37% Test: 75.22%\n",
      "Run: 08, Epoch: 99, Loss: 0.5887, Train: 84.89%, Valid: 78.19% Test: 73.90%\n",
      "Run: 08, Epoch: 100, Loss: 0.5897, Train: 84.07%, Valid: 79.70% Test: 77.41%\n",
      "Run 08:\n",
      "Highest Train: 84.89\n",
      "Highest Valid: 79.70\n",
      "  Final Train: 84.07\n",
      "   Final Test: 77.41\n",
      "Run: 09, Epoch: 01, Loss: 1.6188, Train: 20.51%, Valid: 19.07% Test: 19.74%\n",
      "Run: 09, Epoch: 02, Loss: 1.5136, Train: 37.64%, Valid: 35.39% Test: 33.99%\n",
      "Run: 09, Epoch: 03, Loss: 1.4217, Train: 22.99%, Valid: 22.50% Test: 23.25%\n",
      "Run: 09, Epoch: 04, Loss: 1.3591, Train: 27.01%, Valid: 25.93% Test: 27.85%\n",
      "Run: 09, Epoch: 05, Loss: 1.3335, Train: 42.31%, Valid: 40.88% Test: 39.25%\n",
      "Run: 09, Epoch: 06, Loss: 1.2965, Train: 41.30%, Valid: 40.47% Test: 40.57%\n",
      "Run: 09, Epoch: 07, Loss: 1.2649, Train: 42.77%, Valid: 40.05% Test: 43.42%\n",
      "Run: 09, Epoch: 08, Loss: 1.2326, Train: 42.40%, Valid: 40.74% Test: 43.64%\n",
      "Run: 09, Epoch: 09, Loss: 1.1953, Train: 39.38%, Valid: 39.64% Test: 42.32%\n",
      "Run: 09, Epoch: 10, Loss: 1.1664, Train: 33.24%, Valid: 32.65% Test: 35.31%\n",
      "Run: 09, Epoch: 11, Loss: 1.1536, Train: 33.24%, Valid: 32.78% Test: 35.09%\n",
      "Run: 09, Epoch: 12, Loss: 1.1296, Train: 37.55%, Valid: 38.27% Test: 39.47%\n",
      "Run: 09, Epoch: 13, Loss: 1.1257, Train: 43.04%, Valid: 44.72% Test: 44.74%\n",
      "Run: 09, Epoch: 14, Loss: 1.0976, Train: 44.78%, Valid: 46.50% Test: 48.25%\n",
      "Run: 09, Epoch: 15, Loss: 1.0612, Train: 45.42%, Valid: 46.64% Test: 47.15%\n",
      "Run: 09, Epoch: 16, Loss: 1.0484, Train: 46.43%, Valid: 48.42% Test: 49.12%\n",
      "Run: 09, Epoch: 17, Loss: 1.0586, Train: 49.91%, Valid: 51.99% Test: 52.85%\n",
      "Run: 09, Epoch: 18, Loss: 1.0080, Train: 51.47%, Valid: 53.09% Test: 54.17%\n",
      "Run: 09, Epoch: 19, Loss: 0.9945, Train: 53.75%, Valid: 53.77% Test: 55.04%\n",
      "Run: 09, Epoch: 20, Loss: 1.0013, Train: 54.67%, Valid: 54.73% Test: 56.36%\n",
      "Run: 09, Epoch: 21, Loss: 0.9954, Train: 60.07%, Valid: 58.57% Test: 61.40%\n",
      "Run: 09, Epoch: 22, Loss: 0.9870, Train: 58.61%, Valid: 56.65% Test: 60.09%\n",
      "Run: 09, Epoch: 23, Loss: 0.9685, Train: 58.33%, Valid: 57.48% Test: 60.75%\n",
      "Run: 09, Epoch: 24, Loss: 0.9468, Train: 60.90%, Valid: 61.04% Test: 60.96%\n",
      "Run: 09, Epoch: 25, Loss: 0.9366, Train: 64.10%, Valid: 64.88% Test: 64.25%\n",
      "Run: 09, Epoch: 26, Loss: 0.9524, Train: 64.84%, Valid: 64.47% Test: 62.72%\n",
      "Run: 09, Epoch: 27, Loss: 0.9204, Train: 64.29%, Valid: 63.79% Test: 62.72%\n",
      "Run: 09, Epoch: 28, Loss: 0.9107, Train: 64.38%, Valid: 63.24% Test: 64.04%\n",
      "Run: 09, Epoch: 29, Loss: 0.9540, Train: 65.48%, Valid: 64.47% Test: 64.91%\n",
      "Run: 09, Epoch: 30, Loss: 0.8850, Train: 66.30%, Valid: 67.49% Test: 65.79%\n",
      "Run: 09, Epoch: 31, Loss: 0.8631, Train: 68.32%, Valid: 66.67% Test: 67.54%\n",
      "Run: 09, Epoch: 32, Loss: 0.8646, Train: 71.34%, Valid: 69.00% Test: 71.49%\n",
      "Run: 09, Epoch: 33, Loss: 0.8446, Train: 69.87%, Valid: 68.18% Test: 69.52%\n",
      "Run: 09, Epoch: 34, Loss: 0.8829, Train: 71.52%, Valid: 69.27% Test: 70.18%\n",
      "Run: 09, Epoch: 35, Loss: 0.8788, Train: 70.42%, Valid: 68.86% Test: 67.54%\n",
      "Run: 09, Epoch: 36, Loss: 0.8359, Train: 71.25%, Valid: 69.41% Test: 70.83%\n",
      "Run: 09, Epoch: 37, Loss: 0.8348, Train: 73.17%, Valid: 69.96% Test: 72.15%\n",
      "Run: 09, Epoch: 38, Loss: 0.8342, Train: 74.63%, Valid: 71.74% Test: 72.59%\n",
      "Run: 09, Epoch: 39, Loss: 0.8201, Train: 72.99%, Valid: 70.92% Test: 70.39%\n",
      "Run: 09, Epoch: 40, Loss: 0.8261, Train: 73.26%, Valid: 70.92% Test: 69.96%\n",
      "Run: 09, Epoch: 41, Loss: 0.8353, Train: 76.10%, Valid: 73.66% Test: 72.15%\n",
      "Run: 09, Epoch: 42, Loss: 0.8293, Train: 77.20%, Valid: 73.80% Test: 74.78%\n",
      "Run: 09, Epoch: 43, Loss: 0.8005, Train: 75.09%, Valid: 70.37% Test: 73.90%\n",
      "Run: 09, Epoch: 44, Loss: 0.8046, Train: 72.53%, Valid: 66.94% Test: 70.83%\n",
      "Run: 09, Epoch: 45, Loss: 0.8123, Train: 72.53%, Valid: 68.18% Test: 70.18%\n",
      "Run: 09, Epoch: 46, Loss: 0.7954, Train: 76.19%, Valid: 70.64% Test: 71.05%\n",
      "Run: 09, Epoch: 47, Loss: 0.7927, Train: 76.28%, Valid: 70.51% Test: 69.52%\n",
      "Run: 09, Epoch: 48, Loss: 0.7819, Train: 76.01%, Valid: 70.64% Test: 70.61%\n",
      "Run: 09, Epoch: 49, Loss: 0.8169, Train: 76.74%, Valid: 73.11% Test: 74.78%\n",
      "Run: 09, Epoch: 50, Loss: 0.8109, Train: 76.83%, Valid: 73.39% Test: 74.34%\n",
      "Run: 09, Epoch: 51, Loss: 0.8339, Train: 78.02%, Valid: 74.76% Test: 74.12%\n",
      "Run: 09, Epoch: 52, Loss: 0.7687, Train: 76.28%, Valid: 71.33% Test: 71.27%\n",
      "Run: 09, Epoch: 53, Loss: 0.7354, Train: 72.89%, Valid: 68.59% Test: 67.98%\n",
      "Run: 09, Epoch: 54, Loss: 0.7611, Train: 71.89%, Valid: 67.76% Test: 66.67%\n",
      "Run: 09, Epoch: 55, Loss: 0.7545, Train: 72.25%, Valid: 67.49% Test: 66.23%\n",
      "Run: 09, Epoch: 56, Loss: 0.7630, Train: 75.18%, Valid: 71.88% Test: 69.96%\n",
      "Run: 09, Epoch: 57, Loss: 0.7769, Train: 79.40%, Valid: 75.31% Test: 75.44%\n",
      "Run: 09, Epoch: 58, Loss: 0.7621, Train: 77.38%, Valid: 75.17% Test: 73.68%\n",
      "Run: 09, Epoch: 59, Loss: 0.7577, Train: 73.63%, Valid: 73.25% Test: 69.74%\n",
      "Run: 09, Epoch: 60, Loss: 0.7564, Train: 72.34%, Valid: 70.23% Test: 67.54%\n",
      "Run: 09, Epoch: 61, Loss: 0.7518, Train: 72.99%, Valid: 70.23% Test: 69.74%\n",
      "Run: 09, Epoch: 62, Loss: 0.7347, Train: 76.65%, Valid: 73.11% Test: 72.81%\n",
      "Run: 09, Epoch: 63, Loss: 0.7002, Train: 77.38%, Valid: 73.11% Test: 72.81%\n",
      "Run: 09, Epoch: 64, Loss: 0.7115, Train: 77.66%, Valid: 73.80% Test: 74.34%\n",
      "Run: 09, Epoch: 65, Loss: 0.7178, Train: 80.31%, Valid: 75.58% Test: 76.54%\n",
      "Run: 09, Epoch: 66, Loss: 0.7453, Train: 79.49%, Valid: 75.99% Test: 74.34%\n",
      "Run: 09, Epoch: 67, Loss: 0.7349, Train: 79.12%, Valid: 75.72% Test: 74.34%\n",
      "Run: 09, Epoch: 68, Loss: 0.6900, Train: 79.67%, Valid: 76.82% Test: 76.32%\n",
      "Run: 09, Epoch: 69, Loss: 0.7010, Train: 78.85%, Valid: 77.91% Test: 75.00%\n",
      "Run: 09, Epoch: 70, Loss: 0.7185, Train: 78.75%, Valid: 77.09% Test: 74.34%\n",
      "Run: 09, Epoch: 71, Loss: 0.6974, Train: 78.85%, Valid: 76.13% Test: 74.34%\n",
      "Run: 09, Epoch: 72, Loss: 0.7060, Train: 78.75%, Valid: 73.94% Test: 73.46%\n",
      "Run: 09, Epoch: 73, Loss: 0.6867, Train: 78.02%, Valid: 72.29% Test: 72.37%\n",
      "Run: 09, Epoch: 74, Loss: 0.7063, Train: 77.66%, Valid: 72.43% Test: 71.93%\n",
      "Run: 09, Epoch: 75, Loss: 0.7064, Train: 79.49%, Valid: 73.53% Test: 73.68%\n",
      "Run: 09, Epoch: 76, Loss: 0.6698, Train: 79.95%, Valid: 75.86% Test: 74.34%\n",
      "Run: 09, Epoch: 77, Loss: 0.6933, Train: 80.31%, Valid: 76.54% Test: 74.78%\n",
      "Run: 09, Epoch: 78, Loss: 0.6624, Train: 80.13%, Valid: 74.90% Test: 73.68%\n",
      "Run: 09, Epoch: 79, Loss: 0.6787, Train: 78.48%, Valid: 75.58% Test: 73.68%\n",
      "Run: 09, Epoch: 80, Loss: 0.6593, Train: 77.56%, Valid: 75.17% Test: 72.59%\n",
      "Run: 09, Epoch: 81, Loss: 0.6841, Train: 77.75%, Valid: 75.17% Test: 72.81%\n",
      "Run: 09, Epoch: 82, Loss: 0.6591, Train: 80.04%, Valid: 76.95% Test: 73.90%\n",
      "Run: 09, Epoch: 83, Loss: 0.6501, Train: 82.78%, Valid: 78.88% Test: 74.78%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 09, Epoch: 84, Loss: 0.6561, Train: 82.14%, Valid: 78.33% Test: 74.78%\n",
      "Run: 09, Epoch: 85, Loss: 0.6233, Train: 80.77%, Valid: 76.95% Test: 76.32%\n",
      "Run: 09, Epoch: 86, Loss: 0.6494, Train: 76.56%, Valid: 70.92% Test: 73.03%\n",
      "Run: 09, Epoch: 87, Loss: 0.6184, Train: 74.08%, Valid: 69.68% Test: 72.37%\n",
      "Run: 09, Epoch: 88, Loss: 0.6872, Train: 77.47%, Valid: 72.43% Test: 72.59%\n",
      "Run: 09, Epoch: 89, Loss: 0.6196, Train: 80.40%, Valid: 76.41% Test: 75.22%\n",
      "Run: 09, Epoch: 90, Loss: 0.6189, Train: 80.86%, Valid: 78.19% Test: 77.63%\n",
      "Run: 09, Epoch: 91, Loss: 0.6720, Train: 81.87%, Valid: 78.74% Test: 76.32%\n",
      "Run: 09, Epoch: 92, Loss: 0.6746, Train: 80.86%, Valid: 76.95% Test: 74.78%\n",
      "Run: 09, Epoch: 93, Loss: 0.6575, Train: 80.77%, Valid: 77.23% Test: 73.90%\n",
      "Run: 09, Epoch: 94, Loss: 0.6311, Train: 81.87%, Valid: 77.78% Test: 76.75%\n",
      "Run: 09, Epoch: 95, Loss: 0.6354, Train: 82.88%, Valid: 78.88% Test: 76.75%\n",
      "Run: 09, Epoch: 96, Loss: 0.5926, Train: 81.87%, Valid: 77.50% Test: 75.88%\n",
      "Run: 09, Epoch: 97, Loss: 0.6313, Train: 80.77%, Valid: 76.82% Test: 74.12%\n",
      "Run: 09, Epoch: 98, Loss: 0.6563, Train: 81.50%, Valid: 77.23% Test: 75.22%\n",
      "Run: 09, Epoch: 99, Loss: 0.6343, Train: 80.40%, Valid: 77.37% Test: 74.56%\n",
      "Run: 09, Epoch: 100, Loss: 0.6170, Train: 82.05%, Valid: 77.78% Test: 74.78%\n",
      "Run 09:\n",
      "Highest Train: 82.88\n",
      "Highest Valid: 78.88\n",
      "  Final Train: 82.78\n",
      "   Final Test: 74.78\n",
      "Run: 10, Epoch: 01, Loss: 1.6690, Train: 21.89%, Valid: 23.32% Test: 24.56%\n",
      "Run: 10, Epoch: 02, Loss: 1.5120, Train: 35.16%, Valid: 33.06% Test: 35.31%\n",
      "Run: 10, Epoch: 03, Loss: 1.4200, Train: 27.84%, Valid: 23.18% Test: 26.32%\n",
      "Run: 10, Epoch: 04, Loss: 1.3920, Train: 24.36%, Valid: 23.05% Test: 22.81%\n",
      "Run: 10, Epoch: 05, Loss: 1.3603, Train: 31.04%, Valid: 29.77% Test: 32.24%\n",
      "Run: 10, Epoch: 06, Loss: 1.3039, Train: 34.07%, Valid: 34.02% Test: 37.06%\n",
      "Run: 10, Epoch: 07, Loss: 1.2641, Train: 42.22%, Valid: 44.03% Test: 43.20%\n",
      "Run: 10, Epoch: 08, Loss: 1.2638, Train: 41.58%, Valid: 43.62% Test: 42.54%\n",
      "Run: 10, Epoch: 09, Loss: 1.2418, Train: 41.94%, Valid: 42.80% Test: 41.89%\n",
      "Run: 10, Epoch: 10, Loss: 1.2112, Train: 43.32%, Valid: 42.52% Test: 43.86%\n",
      "Run: 10, Epoch: 11, Loss: 1.1896, Train: 42.22%, Valid: 40.88% Test: 43.86%\n",
      "Run: 10, Epoch: 12, Loss: 1.1527, Train: 44.69%, Valid: 40.19% Test: 42.98%\n",
      "Run: 10, Epoch: 13, Loss: 1.1461, Train: 46.52%, Valid: 41.15% Test: 43.20%\n",
      "Run: 10, Epoch: 14, Loss: 1.1348, Train: 49.08%, Valid: 45.40% Test: 47.81%\n",
      "Run: 10, Epoch: 15, Loss: 1.1312, Train: 53.66%, Valid: 51.03% Test: 53.51%\n",
      "Run: 10, Epoch: 16, Loss: 1.0920, Train: 54.95%, Valid: 54.05% Test: 55.70%\n",
      "Run: 10, Epoch: 17, Loss: 1.0666, Train: 54.30%, Valid: 54.46% Test: 55.26%\n",
      "Run: 10, Epoch: 18, Loss: 1.0567, Train: 55.04%, Valid: 54.32% Test: 53.51%\n",
      "Run: 10, Epoch: 19, Loss: 1.0544, Train: 53.39%, Valid: 51.58% Test: 51.32%\n",
      "Run: 10, Epoch: 20, Loss: 1.0365, Train: 54.49%, Valid: 51.85% Test: 51.97%\n",
      "Run: 10, Epoch: 21, Loss: 1.0371, Train: 59.71%, Valid: 53.91% Test: 58.11%\n",
      "Run: 10, Epoch: 22, Loss: 0.9778, Train: 60.07%, Valid: 53.09% Test: 56.14%\n",
      "Run: 10, Epoch: 23, Loss: 1.0120, Train: 58.88%, Valid: 53.77% Test: 57.02%\n",
      "Run: 10, Epoch: 24, Loss: 1.0161, Train: 59.25%, Valid: 54.18% Test: 56.36%\n",
      "Run: 10, Epoch: 25, Loss: 0.9777, Train: 60.53%, Valid: 55.14% Test: 57.24%\n",
      "Run: 10, Epoch: 26, Loss: 0.9840, Train: 61.63%, Valid: 55.14% Test: 57.89%\n",
      "Run: 10, Epoch: 27, Loss: 0.9763, Train: 62.09%, Valid: 58.16% Test: 61.62%\n",
      "Run: 10, Epoch: 28, Loss: 0.9751, Train: 61.54%, Valid: 58.16% Test: 60.96%\n",
      "Run: 10, Epoch: 29, Loss: 0.9445, Train: 61.08%, Valid: 59.26% Test: 61.62%\n",
      "Run: 10, Epoch: 30, Loss: 0.9202, Train: 62.45%, Valid: 61.59% Test: 62.28%\n",
      "Run: 10, Epoch: 31, Loss: 0.9128, Train: 64.84%, Valid: 61.32% Test: 62.06%\n",
      "Run: 10, Epoch: 32, Loss: 0.9222, Train: 67.03%, Valid: 61.45% Test: 66.45%\n",
      "Run: 10, Epoch: 33, Loss: 0.9097, Train: 69.14%, Valid: 63.37% Test: 68.42%\n",
      "Run: 10, Epoch: 34, Loss: 0.8948, Train: 68.77%, Valid: 62.41% Test: 67.54%\n",
      "Run: 10, Epoch: 35, Loss: 0.8923, Train: 65.57%, Valid: 60.08% Test: 63.60%\n",
      "Run: 10, Epoch: 36, Loss: 0.8927, Train: 67.67%, Valid: 62.28% Test: 66.01%\n",
      "Run: 10, Epoch: 37, Loss: 0.8636, Train: 70.51%, Valid: 65.57% Test: 69.96%\n",
      "Run: 10, Epoch: 38, Loss: 0.8954, Train: 69.87%, Valid: 64.33% Test: 68.86%\n",
      "Run: 10, Epoch: 39, Loss: 0.8839, Train: 68.96%, Valid: 66.12% Test: 67.76%\n",
      "Run: 10, Epoch: 40, Loss: 0.8424, Train: 70.24%, Valid: 68.04% Test: 69.52%\n",
      "Run: 10, Epoch: 41, Loss: 0.8448, Train: 70.60%, Valid: 67.08% Test: 69.30%\n",
      "Run: 10, Epoch: 42, Loss: 0.8654, Train: 72.53%, Valid: 69.14% Test: 73.25%\n",
      "Run: 10, Epoch: 43, Loss: 0.8703, Train: 73.81%, Valid: 69.96% Test: 72.59%\n",
      "Run: 10, Epoch: 44, Loss: 0.8104, Train: 71.70%, Valid: 66.67% Test: 68.20%\n",
      "Run: 10, Epoch: 45, Loss: 0.8272, Train: 71.79%, Valid: 67.35% Test: 70.83%\n",
      "Run: 10, Epoch: 46, Loss: 0.8282, Train: 72.44%, Valid: 66.26% Test: 70.83%\n",
      "Run: 10, Epoch: 47, Loss: 0.8139, Train: 76.47%, Valid: 69.82% Test: 73.03%\n",
      "Run: 10, Epoch: 48, Loss: 0.8137, Train: 76.92%, Valid: 72.98% Test: 74.12%\n",
      "Run: 10, Epoch: 49, Loss: 0.7846, Train: 76.19%, Valid: 72.02% Test: 73.46%\n",
      "Run: 10, Epoch: 50, Loss: 0.7970, Train: 74.45%, Valid: 70.37% Test: 74.12%\n",
      "Run: 10, Epoch: 51, Loss: 0.8025, Train: 74.27%, Valid: 71.33% Test: 72.81%\n",
      "Run: 10, Epoch: 52, Loss: 0.7886, Train: 76.37%, Valid: 72.70% Test: 74.12%\n",
      "Run: 10, Epoch: 53, Loss: 0.8041, Train: 77.20%, Valid: 73.80% Test: 76.10%\n",
      "Run: 10, Epoch: 54, Loss: 0.7677, Train: 76.74%, Valid: 75.17% Test: 75.88%\n",
      "Run: 10, Epoch: 55, Loss: 0.7586, Train: 77.66%, Valid: 74.76% Test: 75.88%\n",
      "Run: 10, Epoch: 56, Loss: 0.8197, Train: 77.38%, Valid: 74.07% Test: 74.12%\n",
      "Run: 10, Epoch: 57, Loss: 0.7706, Train: 73.81%, Valid: 71.88% Test: 71.71%\n",
      "Run: 10, Epoch: 58, Loss: 0.7571, Train: 72.53%, Valid: 70.51% Test: 72.15%\n",
      "Run: 10, Epoch: 59, Loss: 0.7567, Train: 73.44%, Valid: 71.88% Test: 73.46%\n",
      "Run: 10, Epoch: 60, Loss: 0.7462, Train: 75.09%, Valid: 72.43% Test: 74.34%\n",
      "Run: 10, Epoch: 61, Loss: 0.7443, Train: 75.82%, Valid: 72.84% Test: 75.44%\n",
      "Run: 10, Epoch: 62, Loss: 0.7246, Train: 76.65%, Valid: 74.07% Test: 75.44%\n",
      "Run: 10, Epoch: 63, Loss: 0.7419, Train: 76.65%, Valid: 74.76% Test: 76.54%\n",
      "Run: 10, Epoch: 64, Loss: 0.7617, Train: 76.19%, Valid: 73.11% Test: 75.44%\n",
      "Run: 10, Epoch: 65, Loss: 0.7266, Train: 76.56%, Valid: 73.39% Test: 75.88%\n",
      "Run: 10, Epoch: 66, Loss: 0.6943, Train: 77.47%, Valid: 74.49% Test: 75.22%\n",
      "Run: 10, Epoch: 67, Loss: 0.6983, Train: 77.93%, Valid: 73.39% Test: 76.10%\n",
      "Run: 10, Epoch: 68, Loss: 0.7073, Train: 77.56%, Valid: 73.94% Test: 77.85%\n",
      "Run: 10, Epoch: 69, Loss: 0.7431, Train: 79.03%, Valid: 76.41% Test: 77.19%\n",
      "Run: 10, Epoch: 70, Loss: 0.7513, Train: 76.47%, Valid: 74.49% Test: 74.12%\n",
      "Run: 10, Epoch: 71, Loss: 0.6843, Train: 75.00%, Valid: 72.84% Test: 73.68%\n",
      "Run: 10, Epoch: 72, Loss: 0.7264, Train: 76.92%, Valid: 74.21% Test: 75.44%\n",
      "Run: 10, Epoch: 73, Loss: 0.7171, Train: 79.76%, Valid: 76.27% Test: 77.85%\n",
      "Run: 10, Epoch: 74, Loss: 0.7004, Train: 80.40%, Valid: 75.58% Test: 76.32%\n",
      "Run: 10, Epoch: 75, Loss: 0.6966, Train: 77.66%, Valid: 73.53% Test: 72.81%\n",
      "Run: 10, Epoch: 76, Loss: 0.7096, Train: 75.64%, Valid: 71.33% Test: 71.93%\n",
      "Run: 10, Epoch: 77, Loss: 0.6862, Train: 77.47%, Valid: 74.62% Test: 75.44%\n",
      "Run: 10, Epoch: 78, Loss: 0.6939, Train: 79.40%, Valid: 76.54% Test: 75.88%\n",
      "Run: 10, Epoch: 79, Loss: 0.7215, Train: 79.67%, Valid: 76.82% Test: 76.75%\n",
      "Run: 10, Epoch: 80, Loss: 0.7056, Train: 80.22%, Valid: 77.50% Test: 77.85%\n",
      "Run: 10, Epoch: 81, Loss: 0.7083, Train: 79.40%, Valid: 76.41% Test: 76.54%\n",
      "Run: 10, Epoch: 82, Loss: 0.6856, Train: 79.49%, Valid: 76.82% Test: 78.07%\n",
      "Run: 10, Epoch: 83, Loss: 0.6991, Train: 79.49%, Valid: 76.54% Test: 79.17%\n",
      "Run: 10, Epoch: 84, Loss: 0.6783, Train: 78.39%, Valid: 75.72% Test: 77.63%\n",
      "Run: 10, Epoch: 85, Loss: 0.6859, Train: 80.13%, Valid: 77.64% Test: 77.19%\n",
      "Run: 10, Epoch: 86, Loss: 0.6370, Train: 80.40%, Valid: 76.68% Test: 75.88%\n",
      "Run: 10, Epoch: 87, Loss: 0.7013, Train: 81.78%, Valid: 79.29% Test: 78.29%\n",
      "Run: 10, Epoch: 88, Loss: 0.6629, Train: 82.42%, Valid: 80.11% Test: 78.73%\n",
      "Run: 10, Epoch: 89, Loss: 0.6878, Train: 82.42%, Valid: 79.01% Test: 77.63%\n",
      "Run: 10, Epoch: 90, Loss: 0.6729, Train: 82.60%, Valid: 78.60% Test: 76.97%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 10, Epoch: 91, Loss: 0.6705, Train: 82.42%, Valid: 78.60% Test: 76.75%\n",
      "Run: 10, Epoch: 92, Loss: 0.6677, Train: 82.78%, Valid: 77.78% Test: 78.73%\n",
      "Run: 10, Epoch: 93, Loss: 0.6958, Train: 82.97%, Valid: 78.46% Test: 79.39%\n",
      "Run: 10, Epoch: 94, Loss: 0.6247, Train: 82.69%, Valid: 79.29% Test: 78.07%\n",
      "Run: 10, Epoch: 95, Loss: 0.6501, Train: 81.32%, Valid: 79.56% Test: 78.29%\n",
      "Run: 10, Epoch: 96, Loss: 0.6595, Train: 81.87%, Valid: 77.37% Test: 76.10%\n",
      "Run: 10, Epoch: 97, Loss: 0.6349, Train: 81.96%, Valid: 78.46% Test: 79.39%\n",
      "Run: 10, Epoch: 98, Loss: 0.6504, Train: 81.68%, Valid: 79.97% Test: 80.04%\n",
      "Run: 10, Epoch: 99, Loss: 0.6137, Train: 82.88%, Valid: 80.25% Test: 79.82%\n",
      "Run: 10, Epoch: 100, Loss: 0.6099, Train: 81.87%, Valid: 79.70% Test: 78.29%\n",
      "Run 10:\n",
      "Highest Train: 82.97\n",
      "Highest Valid: 80.25\n",
      "  Final Train: 82.88\n",
      "   Final Test: 79.82\n",
      "All runs:\n",
      "Highest Train: 83.60 ± 1.17\n",
      "Highest Valid: 78.88 ± 1.04\n",
      "  Final Train: 82.93 ± 1.04\n",
      "   Final Test: 78.14 ± 2.18\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
    "         'batch_size': 32, 'hidden_channels': 32, 'dropout': 0.5, 'epochs': 100, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-6, 'lr': 0.01,'hidden_channels_mlp': 20,'dropout_mlp': 0.5,'num_layers_mlp': 3}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = Planetoid(root='/tmp/cora', name='Cora',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    X = data.topo\n",
    "    y_true = data.y\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    model = SAGE(data.num_features, args.hidden_channels,10, args.num_layers,args.dropout)\n",
    "    mlp_model = MLP(X.size(-1), args.hidden_channels_mlp, 5,args.num_layers_mlp, args.dropout_mlp)\n",
    "    #print(mlp_model.parameters())\n",
    "    mlp_2 = MLP2(15, 100, dataset.num_classes,3, 0.0)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        idx_train=[data.train_mask[i][run] for i in range(len(data.y))]\n",
    "        train_idx = np.where(idx_train)[0]\n",
    "        idx_val=[data.val_mask[i][run] for i in range(len(data.y))]\n",
    "        valid_idx = np.where(idx_val)[0]\n",
    "        idx_test=[data.test_mask[i][run] for i in range(len(data.y))]\n",
    "        test_idx = np.where(idx_test)[0]\n",
    "        \n",
    "        model.reset_parameters()\n",
    "        mlp_model.reset_parameters_mlp()\n",
    "        mlp_2.reset_parameters_mlp2()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        optimizer_mlp=torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "        optimizer_mlp2=torch.optim.Adam(mlp_2.parameters(), lr=0.01)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2)\n",
    "            result = test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ed5ea",
   "metadata": {},
   "source": [
    "# TOPO-GSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37120290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2277, 2325], y=[2277], train_mask=[2277, 10], val_mask=[2277, 10], test_mask=[2277, 10], adj_t=[2277, 2277, nnz=36101], topo=[2277, 38])\n"
     ]
    }
   ],
   "source": [
    "dataset = WikipediaNetwork(root='/tmp/chameleon', name='chameleon',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "data.topo=Topo_fe\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a12165a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x157230760>\n",
      "Run 01:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 68.31\n",
      "  Final Train: 99.18\n",
      "   Final Test: 65.13\n",
      "Run 02:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 62.69\n",
      "  Final Train: 99.82\n",
      "   Final Test: 68.20\n",
      "Run 03:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 68.45\n",
      "  Final Train: 99.82\n",
      "   Final Test: 63.60\n",
      "Run 04:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 65.84\n",
      "  Final Train: 99.82\n",
      "   Final Test: 66.45\n",
      "Run 05:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 66.80\n",
      "  Final Train: 99.91\n",
      "   Final Test: 65.57\n",
      "Run 06:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 66.80\n",
      "  Final Train: 100.00\n",
      "   Final Test: 66.67\n",
      "Run 07:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 65.98\n",
      "  Final Train: 97.89\n",
      "   Final Test: 64.04\n",
      "Run 08:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 65.84\n",
      "  Final Train: 98.90\n",
      "   Final Test: 65.13\n",
      "Run 09:\n",
      "Highest Train: 99.91\n",
      "Highest Valid: 67.08\n",
      "  Final Train: 99.91\n",
      "   Final Test: 64.25\n",
      "Run 10:\n",
      "Highest Train: 99.82\n",
      "Highest Valid: 65.02\n",
      "  Final Train: 99.63\n",
      "   Final Test: 66.01\n",
      "All runs:\n",
      "Highest Train: 99.95 ± 0.06\n",
      "Highest Valid: 66.28 ± 1.67\n",
      "  Final Train: 99.49 ± 0.66\n",
      "   Final Test: 65.50 ± 1.39\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
    "         'batch_size': 32, 'hidden_channels': 32, 'dropout': 0.5, 'epochs': 100, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-6, 'lr': 0.01,'hidden_channels_mlp': 20,'dropout_mlp': 0.5,'num_layers_mlp': 3}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = Planetoid(root='/tmp/cora', name='Cora',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    X = data.topo\n",
    "    y_true = data.y\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    model = SAGE(data.num_features, args.hidden_channels,10, args.num_layers,args.dropout)\n",
    "    mlp_model = MLP(X.size(-1), args.hidden_channels_mlp, 5,args.num_layers_mlp, args.dropout_mlp)\n",
    "    #print(mlp_model.parameters())\n",
    "    mlp_2 = MLP2(15, 100, dataset.num_classes,3, 0.0)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        idx_train=[data.train_mask[i][run] for i in range(len(data.y))]\n",
    "        train_idx = np.where(idx_train)[0]\n",
    "        idx_val=[data.val_mask[i][run] for i in range(len(data.y))]\n",
    "        valid_idx = np.where(idx_val)[0]\n",
    "        idx_test=[data.test_mask[i][run] for i in range(len(data.y))]\n",
    "        test_idx = np.where(idx_test)[0]\n",
    "        \n",
    "        model.reset_parameters()\n",
    "        mlp_model.reset_parameters_mlp()\n",
    "        mlp_2.reset_parameters_mlp2()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        optimizer_mlp=torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "        optimizer_mlp2=torch.optim.Adam(mlp_2.parameters(), lr=0.01)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2)\n",
    "            result = test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                #print(f'Run: {run + 1:02d}, 'f'Epoch: {epoch:02d}, ' f'Loss: {loss:.4f}, 'f'Train: {100 * train_acc:.2f}%, '\n",
    "                #      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                #      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cb0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29f6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
