{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af88d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "from networkx import ego_graph\n",
    "\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "#from logger import Logger\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7babc9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], adj_t=[19717, 19717, nnz=88648])\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "print(data)\n",
    "#split_idx = dataset.get_idx_split()\n",
    "#train_idx = split_idx['train'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91fdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "train_index = np.where(data.train_mask)[0]\n",
    "print(len(train_index))\n",
    "valid_index = np.where(data.val_mask)[0]\n",
    "print(len(valid_index))\n",
    "test_index = np.where(data.test_mask)[0]\n",
    "print(len(test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0b82f",
   "metadata": {},
   "source": [
    "# GSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9ef33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "47468ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout,heads):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.heads=heads\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    #print(len(out))\n",
    "    #print(data.y.squeeze(1)[train_idx])\n",
    "    loss = F.nll_loss(out, data.y.squeeze()[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def ACC(Prediction, Label):\n",
    "    correct = Prediction.view(-1).eq(Label).sum().item()\n",
    "    total=len(Label)\n",
    "    return correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, train_idx,valid_idx,test_idx):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    y_pred=y_pred.view(-1)\n",
    "    train_acc=ACC(data.y[train_idx],y_pred[train_idx])\n",
    "    valid_acc=ACC(data.y[valid_idx],y_pred[valid_idx])\n",
    "    test_acc =ACC(data.y[test_idx],y_pred[test_idx])\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b23796d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x15dff41f0>\n",
      "Run: 01, Epoch: 01, Loss: 1.1315, Train: 61.67%, Valid: 58.20% Test: 57.10%\n",
      "Run: 01, Epoch: 02, Loss: 0.5678, Train: 63.33%, Valid: 58.80% Test: 57.10%\n",
      "Run: 01, Epoch: 03, Loss: 0.4080, Train: 65.00%, Valid: 61.00% Test: 59.40%\n",
      "Run: 01, Epoch: 04, Loss: 0.3180, Train: 66.67%, Valid: 63.20% Test: 61.10%\n",
      "Run: 01, Epoch: 05, Loss: 0.2252, Train: 71.67%, Valid: 62.80% Test: 62.40%\n",
      "Run: 01, Epoch: 06, Loss: 0.2026, Train: 83.33%, Valid: 62.60% Test: 63.30%\n",
      "Run: 01, Epoch: 07, Loss: 0.1549, Train: 90.00%, Valid: 64.00% Test: 64.80%\n",
      "Run: 01, Epoch: 08, Loss: 0.1158, Train: 95.00%, Valid: 65.60% Test: 66.80%\n",
      "Run: 01, Epoch: 09, Loss: 0.1462, Train: 96.67%, Valid: 69.40% Test: 68.20%\n",
      "Run: 01, Epoch: 10, Loss: 0.0747, Train: 96.67%, Valid: 70.00% Test: 69.40%\n",
      "Run: 01, Epoch: 11, Loss: 0.0925, Train: 98.33%, Valid: 71.00% Test: 71.00%\n",
      "Run: 01, Epoch: 12, Loss: 0.0722, Train: 98.33%, Valid: 71.60% Test: 71.80%\n",
      "Run: 01, Epoch: 13, Loss: 0.0627, Train: 98.33%, Valid: 72.20% Test: 72.70%\n",
      "Run: 01, Epoch: 14, Loss: 0.0605, Train: 100.00%, Valid: 72.80% Test: 72.70%\n",
      "Run: 01, Epoch: 15, Loss: 0.0496, Train: 100.00%, Valid: 72.80% Test: 72.70%\n",
      "Run: 01, Epoch: 16, Loss: 0.0482, Train: 100.00%, Valid: 73.20% Test: 72.70%\n",
      "Run: 01, Epoch: 17, Loss: 0.0359, Train: 100.00%, Valid: 73.60% Test: 72.80%\n",
      "Run: 01, Epoch: 18, Loss: 0.0258, Train: 100.00%, Valid: 74.00% Test: 73.00%\n",
      "Run: 01, Epoch: 19, Loss: 0.0402, Train: 100.00%, Valid: 74.20% Test: 73.10%\n",
      "Run: 01, Epoch: 20, Loss: 0.0146, Train: 100.00%, Valid: 74.60% Test: 73.40%\n",
      "Run: 01, Epoch: 21, Loss: 0.0375, Train: 100.00%, Valid: 74.40% Test: 73.40%\n",
      "Run: 01, Epoch: 22, Loss: 0.0178, Train: 100.00%, Valid: 74.40% Test: 73.10%\n",
      "Run: 01, Epoch: 23, Loss: 0.0278, Train: 100.00%, Valid: 74.80% Test: 72.90%\n",
      "Run: 01, Epoch: 24, Loss: 0.0225, Train: 100.00%, Valid: 74.80% Test: 72.80%\n",
      "Run: 01, Epoch: 25, Loss: 0.0229, Train: 100.00%, Valid: 75.00% Test: 72.90%\n",
      "Run: 01, Epoch: 26, Loss: 0.0152, Train: 100.00%, Valid: 75.20% Test: 72.90%\n",
      "Run: 01, Epoch: 27, Loss: 0.0080, Train: 100.00%, Valid: 74.80% Test: 72.60%\n",
      "Run: 01, Epoch: 28, Loss: 0.0063, Train: 100.00%, Valid: 75.20% Test: 72.40%\n",
      "Run: 01, Epoch: 29, Loss: 0.0177, Train: 100.00%, Valid: 75.60% Test: 72.30%\n",
      "Run: 01, Epoch: 30, Loss: 0.0061, Train: 100.00%, Valid: 75.60% Test: 72.30%\n",
      "Run: 01, Epoch: 31, Loss: 0.0204, Train: 100.00%, Valid: 75.40% Test: 72.00%\n",
      "Run: 01, Epoch: 32, Loss: 0.0080, Train: 100.00%, Valid: 75.40% Test: 72.00%\n",
      "Run: 01, Epoch: 33, Loss: 0.0126, Train: 100.00%, Valid: 75.40% Test: 72.20%\n",
      "Run: 01, Epoch: 34, Loss: 0.0033, Train: 100.00%, Valid: 75.20% Test: 72.10%\n",
      "Run: 01, Epoch: 35, Loss: 0.0097, Train: 100.00%, Valid: 75.20% Test: 72.00%\n",
      "Run: 01, Epoch: 36, Loss: 0.0308, Train: 100.00%, Valid: 75.40% Test: 72.20%\n",
      "Run: 01, Epoch: 37, Loss: 0.0073, Train: 100.00%, Valid: 75.40% Test: 72.20%\n",
      "Run: 01, Epoch: 38, Loss: 0.0172, Train: 100.00%, Valid: 75.40% Test: 72.10%\n",
      "Run: 01, Epoch: 39, Loss: 0.0120, Train: 100.00%, Valid: 75.40% Test: 72.00%\n",
      "Run: 01, Epoch: 40, Loss: 0.0044, Train: 100.00%, Valid: 75.40% Test: 72.00%\n",
      "Run: 01, Epoch: 41, Loss: 0.0208, Train: 100.00%, Valid: 75.40% Test: 72.00%\n",
      "Run: 01, Epoch: 42, Loss: 0.0055, Train: 100.00%, Valid: 75.40% Test: 72.00%\n",
      "Run: 01, Epoch: 43, Loss: 0.0074, Train: 100.00%, Valid: 75.00% Test: 72.00%\n",
      "Run: 01, Epoch: 44, Loss: 0.0045, Train: 100.00%, Valid: 75.00% Test: 71.80%\n",
      "Run: 01, Epoch: 45, Loss: 0.0025, Train: 100.00%, Valid: 74.80% Test: 72.00%\n",
      "Run: 01, Epoch: 46, Loss: 0.0023, Train: 100.00%, Valid: 74.40% Test: 72.00%\n",
      "Run: 01, Epoch: 47, Loss: 0.0057, Train: 100.00%, Valid: 74.40% Test: 71.80%\n",
      "Run: 01, Epoch: 48, Loss: 0.0080, Train: 100.00%, Valid: 73.80% Test: 71.70%\n",
      "Run: 01, Epoch: 49, Loss: 0.0038, Train: 100.00%, Valid: 73.20% Test: 71.40%\n",
      "Run: 01, Epoch: 50, Loss: 0.0011, Train: 100.00%, Valid: 73.00% Test: 71.20%\n",
      "Run: 01, Epoch: 51, Loss: 0.0010, Train: 100.00%, Valid: 72.80% Test: 71.00%\n",
      "Run: 01, Epoch: 52, Loss: 0.0211, Train: 100.00%, Valid: 72.60% Test: 70.70%\n",
      "Run: 01, Epoch: 53, Loss: 0.0226, Train: 100.00%, Valid: 72.60% Test: 70.70%\n",
      "Run: 01, Epoch: 54, Loss: 0.0211, Train: 100.00%, Valid: 72.60% Test: 70.60%\n",
      "Run: 01, Epoch: 55, Loss: 0.0023, Train: 100.00%, Valid: 72.60% Test: 70.50%\n",
      "Run: 01, Epoch: 56, Loss: 0.0022, Train: 100.00%, Valid: 72.60% Test: 70.40%\n",
      "Run: 01, Epoch: 57, Loss: 0.0036, Train: 100.00%, Valid: 72.40% Test: 70.20%\n",
      "Run: 01, Epoch: 58, Loss: 0.0175, Train: 100.00%, Valid: 72.40% Test: 70.40%\n",
      "Run: 01, Epoch: 59, Loss: 0.0040, Train: 100.00%, Valid: 72.40% Test: 70.50%\n",
      "Run: 01, Epoch: 60, Loss: 0.0011, Train: 100.00%, Valid: 72.40% Test: 70.20%\n",
      "Run: 01, Epoch: 61, Loss: 0.0017, Train: 100.00%, Valid: 72.40% Test: 70.20%\n",
      "Run: 01, Epoch: 62, Loss: 0.0097, Train: 100.00%, Valid: 72.40% Test: 70.10%\n",
      "Run: 01, Epoch: 63, Loss: 0.0019, Train: 100.00%, Valid: 72.20% Test: 70.00%\n",
      "Run: 01, Epoch: 64, Loss: 0.0076, Train: 100.00%, Valid: 72.60% Test: 70.10%\n",
      "Run: 01, Epoch: 65, Loss: 0.0373, Train: 100.00%, Valid: 72.80% Test: 70.10%\n",
      "Run: 01, Epoch: 66, Loss: 0.0009, Train: 100.00%, Valid: 72.80% Test: 70.20%\n",
      "Run: 01, Epoch: 67, Loss: 0.0034, Train: 100.00%, Valid: 72.80% Test: 70.30%\n",
      "Run: 01, Epoch: 68, Loss: 0.0004, Train: 100.00%, Valid: 72.80% Test: 70.50%\n",
      "Run: 01, Epoch: 69, Loss: 0.0045, Train: 100.00%, Valid: 72.80% Test: 70.80%\n",
      "Run: 01, Epoch: 70, Loss: 0.0046, Train: 100.00%, Valid: 72.40% Test: 70.80%\n",
      "Run: 01, Epoch: 71, Loss: 0.0012, Train: 100.00%, Valid: 72.20% Test: 70.80%\n",
      "Run: 01, Epoch: 72, Loss: 0.0096, Train: 100.00%, Valid: 72.20% Test: 70.80%\n",
      "Run: 01, Epoch: 73, Loss: 0.0014, Train: 100.00%, Valid: 72.20% Test: 70.80%\n",
      "Run: 01, Epoch: 74, Loss: 0.0025, Train: 100.00%, Valid: 72.40% Test: 71.20%\n",
      "Run: 01, Epoch: 75, Loss: 0.0025, Train: 100.00%, Valid: 72.40% Test: 71.20%\n",
      "Run: 01, Epoch: 76, Loss: 0.0164, Train: 100.00%, Valid: 72.20% Test: 71.50%\n",
      "Run: 01, Epoch: 77, Loss: 0.0009, Train: 100.00%, Valid: 72.20% Test: 71.70%\n",
      "Run: 01, Epoch: 78, Loss: 0.0015, Train: 100.00%, Valid: 72.20% Test: 71.70%\n",
      "Run: 01, Epoch: 79, Loss: 0.0007, Train: 100.00%, Valid: 72.20% Test: 71.60%\n",
      "Run: 01, Epoch: 80, Loss: 0.0008, Train: 100.00%, Valid: 72.20% Test: 71.50%\n",
      "Run: 01, Epoch: 81, Loss: 0.0004, Train: 100.00%, Valid: 72.00% Test: 71.60%\n",
      "Run: 01, Epoch: 82, Loss: 0.0087, Train: 100.00%, Valid: 71.80% Test: 71.80%\n",
      "Run: 01, Epoch: 83, Loss: 0.0010, Train: 100.00%, Valid: 72.00% Test: 71.80%\n",
      "Run: 01, Epoch: 84, Loss: 0.0193, Train: 100.00%, Valid: 72.20% Test: 71.80%\n",
      "Run: 01, Epoch: 85, Loss: 0.0183, Train: 100.00%, Valid: 72.00% Test: 71.80%\n",
      "Run: 01, Epoch: 86, Loss: 0.0013, Train: 100.00%, Valid: 72.20% Test: 71.90%\n",
      "Run: 01, Epoch: 87, Loss: 0.0036, Train: 100.00%, Valid: 72.20% Test: 71.90%\n",
      "Run: 01, Epoch: 88, Loss: 0.0167, Train: 100.00%, Valid: 72.20% Test: 71.90%\n",
      "Run: 01, Epoch: 89, Loss: 0.0006, Train: 100.00%, Valid: 72.20% Test: 71.90%\n",
      "Run: 01, Epoch: 90, Loss: 0.0022, Train: 100.00%, Valid: 72.00% Test: 71.90%\n",
      "Run: 01, Epoch: 91, Loss: 0.0134, Train: 100.00%, Valid: 72.20% Test: 71.90%\n",
      "Run: 01, Epoch: 92, Loss: 0.0009, Train: 100.00%, Valid: 72.20% Test: 72.00%\n",
      "Run: 01, Epoch: 93, Loss: 0.0165, Train: 100.00%, Valid: 72.00% Test: 71.90%\n",
      "Run: 01, Epoch: 94, Loss: 0.0014, Train: 100.00%, Valid: 72.20% Test: 72.00%\n",
      "Run: 01, Epoch: 95, Loss: 0.0049, Train: 100.00%, Valid: 72.20% Test: 72.00%\n",
      "Run: 01, Epoch: 96, Loss: 0.0048, Train: 100.00%, Valid: 72.20% Test: 72.10%\n",
      "Run: 01, Epoch: 97, Loss: 0.0005, Train: 100.00%, Valid: 72.00% Test: 72.10%\n",
      "Run: 01, Epoch: 98, Loss: 0.0009, Train: 100.00%, Valid: 71.80% Test: 72.10%\n",
      "Run: 01, Epoch: 99, Loss: 0.0038, Train: 100.00%, Valid: 72.00% Test: 72.10%\n",
      "Run: 01, Epoch: 100, Loss: 0.0151, Train: 100.00%, Valid: 72.00% Test: 72.00%\n",
      "Run 01:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 75.60\n",
      "  Final Train: 100.00\n",
      "   Final Test: 72.30\n",
      "Run: 02, Epoch: 01, Loss: 1.2483, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 02, Epoch: 02, Loss: 0.6635, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 02, Epoch: 03, Loss: 0.4764, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 02, Epoch: 04, Loss: 0.3181, Train: 41.67%, Valid: 41.60% Test: 40.90%\n",
      "Run: 02, Epoch: 05, Loss: 0.2603, Train: 66.67%, Valid: 45.00% Test: 44.10%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 02, Epoch: 06, Loss: 0.2346, Train: 95.00%, Valid: 58.00% Test: 55.40%\n",
      "Run: 02, Epoch: 07, Loss: 0.1690, Train: 98.33%, Valid: 65.60% Test: 65.90%\n",
      "Run: 02, Epoch: 08, Loss: 0.1183, Train: 98.33%, Valid: 70.60% Test: 70.50%\n",
      "Run: 02, Epoch: 09, Loss: 0.0918, Train: 98.33%, Valid: 71.80% Test: 72.00%\n",
      "Run: 02, Epoch: 10, Loss: 0.1222, Train: 98.33%, Valid: 72.80% Test: 73.30%\n",
      "Run: 02, Epoch: 11, Loss: 0.0967, Train: 100.00%, Valid: 73.20% Test: 73.50%\n",
      "Run: 02, Epoch: 12, Loss: 0.0640, Train: 100.00%, Valid: 73.60% Test: 73.40%\n",
      "Run: 02, Epoch: 13, Loss: 0.0446, Train: 100.00%, Valid: 73.60% Test: 73.30%\n",
      "Run: 02, Epoch: 14, Loss: 0.0502, Train: 100.00%, Valid: 73.40% Test: 73.80%\n",
      "Run: 02, Epoch: 15, Loss: 0.0519, Train: 100.00%, Valid: 73.20% Test: 73.50%\n",
      "Run: 02, Epoch: 16, Loss: 0.0323, Train: 100.00%, Valid: 72.60% Test: 73.50%\n",
      "Run: 02, Epoch: 17, Loss: 0.0314, Train: 100.00%, Valid: 72.60% Test: 73.40%\n",
      "Run: 02, Epoch: 18, Loss: 0.0248, Train: 100.00%, Valid: 72.60% Test: 73.70%\n",
      "Run: 02, Epoch: 19, Loss: 0.0250, Train: 100.00%, Valid: 72.80% Test: 73.70%\n",
      "Run: 02, Epoch: 20, Loss: 0.0475, Train: 100.00%, Valid: 72.40% Test: 73.80%\n",
      "Run: 02, Epoch: 21, Loss: 0.0256, Train: 100.00%, Valid: 71.80% Test: 73.80%\n",
      "Run: 02, Epoch: 22, Loss: 0.0204, Train: 100.00%, Valid: 72.20% Test: 73.30%\n",
      "Run: 02, Epoch: 23, Loss: 0.0152, Train: 100.00%, Valid: 71.60% Test: 73.10%\n",
      "Run: 02, Epoch: 24, Loss: 0.0107, Train: 100.00%, Valid: 72.60% Test: 73.00%\n",
      "Run: 02, Epoch: 25, Loss: 0.0197, Train: 100.00%, Valid: 73.20% Test: 72.80%\n",
      "Run: 02, Epoch: 26, Loss: 0.0100, Train: 100.00%, Valid: 73.40% Test: 72.80%\n",
      "Run: 02, Epoch: 27, Loss: 0.0119, Train: 100.00%, Valid: 73.60% Test: 72.50%\n",
      "Run: 02, Epoch: 28, Loss: 0.0503, Train: 100.00%, Valid: 73.80% Test: 72.50%\n",
      "Run: 02, Epoch: 29, Loss: 0.0163, Train: 100.00%, Valid: 73.80% Test: 72.40%\n",
      "Run: 02, Epoch: 30, Loss: 0.0033, Train: 100.00%, Valid: 74.20% Test: 72.50%\n",
      "Run: 02, Epoch: 31, Loss: 0.0301, Train: 100.00%, Valid: 74.00% Test: 72.50%\n",
      "Run: 02, Epoch: 32, Loss: 0.0058, Train: 100.00%, Valid: 73.80% Test: 72.60%\n",
      "Run: 02, Epoch: 33, Loss: 0.0075, Train: 100.00%, Valid: 73.60% Test: 72.70%\n",
      "Run: 02, Epoch: 34, Loss: 0.0238, Train: 100.00%, Valid: 73.40% Test: 72.80%\n",
      "Run: 02, Epoch: 35, Loss: 0.0075, Train: 100.00%, Valid: 73.40% Test: 73.00%\n",
      "Run: 02, Epoch: 36, Loss: 0.0257, Train: 100.00%, Valid: 73.80% Test: 72.90%\n",
      "Run: 02, Epoch: 37, Loss: 0.0051, Train: 100.00%, Valid: 73.60% Test: 73.00%\n",
      "Run: 02, Epoch: 38, Loss: 0.0205, Train: 100.00%, Valid: 73.80% Test: 73.00%\n",
      "Run: 02, Epoch: 39, Loss: 0.0069, Train: 100.00%, Valid: 74.20% Test: 73.10%\n",
      "Run: 02, Epoch: 40, Loss: 0.0060, Train: 100.00%, Valid: 74.40% Test: 72.80%\n",
      "Run: 02, Epoch: 41, Loss: 0.0233, Train: 100.00%, Valid: 74.40% Test: 72.70%\n",
      "Run: 02, Epoch: 42, Loss: 0.0128, Train: 100.00%, Valid: 74.20% Test: 72.80%\n",
      "Run: 02, Epoch: 43, Loss: 0.0237, Train: 100.00%, Valid: 73.60% Test: 72.70%\n",
      "Run: 02, Epoch: 44, Loss: 0.0037, Train: 100.00%, Valid: 73.60% Test: 72.20%\n",
      "Run: 02, Epoch: 45, Loss: 0.0023, Train: 100.00%, Valid: 74.00% Test: 72.30%\n",
      "Run: 02, Epoch: 46, Loss: 0.0094, Train: 100.00%, Valid: 73.80% Test: 72.30%\n",
      "Run: 02, Epoch: 47, Loss: 0.0299, Train: 100.00%, Valid: 73.60% Test: 72.10%\n",
      "Run: 02, Epoch: 48, Loss: 0.0035, Train: 100.00%, Valid: 73.60% Test: 72.00%\n",
      "Run: 02, Epoch: 49, Loss: 0.0030, Train: 100.00%, Valid: 73.00% Test: 71.90%\n",
      "Run: 02, Epoch: 50, Loss: 0.0090, Train: 100.00%, Valid: 72.60% Test: 72.10%\n",
      "Run: 02, Epoch: 51, Loss: 0.0041, Train: 100.00%, Valid: 72.40% Test: 72.00%\n",
      "Run: 02, Epoch: 52, Loss: 0.0022, Train: 100.00%, Valid: 72.20% Test: 72.00%\n",
      "Run: 02, Epoch: 53, Loss: 0.0045, Train: 100.00%, Valid: 72.00% Test: 71.60%\n",
      "Run: 02, Epoch: 54, Loss: 0.0033, Train: 100.00%, Valid: 72.00% Test: 71.60%\n",
      "Run: 02, Epoch: 55, Loss: 0.0030, Train: 100.00%, Valid: 71.80% Test: 71.50%\n",
      "Run: 02, Epoch: 56, Loss: 0.0054, Train: 100.00%, Valid: 71.80% Test: 71.50%\n",
      "Run: 02, Epoch: 57, Loss: 0.0035, Train: 100.00%, Valid: 71.80% Test: 71.40%\n",
      "Run: 02, Epoch: 58, Loss: 0.0069, Train: 100.00%, Valid: 71.60% Test: 71.20%\n",
      "Run: 02, Epoch: 59, Loss: 0.0179, Train: 100.00%, Valid: 71.60% Test: 71.20%\n",
      "Run: 02, Epoch: 60, Loss: 0.0118, Train: 100.00%, Valid: 71.20% Test: 71.10%\n",
      "Run: 02, Epoch: 61, Loss: 0.0062, Train: 100.00%, Valid: 71.00% Test: 71.20%\n",
      "Run: 02, Epoch: 62, Loss: 0.0013, Train: 100.00%, Valid: 71.00% Test: 71.10%\n",
      "Run: 02, Epoch: 63, Loss: 0.0003, Train: 100.00%, Valid: 70.80% Test: 71.00%\n",
      "Run: 02, Epoch: 64, Loss: 0.0007, Train: 100.00%, Valid: 70.40% Test: 70.80%\n",
      "Run: 02, Epoch: 65, Loss: 0.0040, Train: 100.00%, Valid: 70.40% Test: 70.90%\n",
      "Run: 02, Epoch: 66, Loss: 0.0026, Train: 100.00%, Valid: 70.20% Test: 70.70%\n",
      "Run: 02, Epoch: 67, Loss: 0.0025, Train: 100.00%, Valid: 70.20% Test: 70.80%\n",
      "Run: 02, Epoch: 68, Loss: 0.0233, Train: 100.00%, Valid: 70.20% Test: 70.80%\n",
      "Run: 02, Epoch: 69, Loss: 0.0022, Train: 100.00%, Valid: 70.20% Test: 70.80%\n",
      "Run: 02, Epoch: 70, Loss: 0.0326, Train: 100.00%, Valid: 70.20% Test: 70.80%\n",
      "Run: 02, Epoch: 71, Loss: 0.0046, Train: 100.00%, Valid: 70.60% Test: 70.80%\n",
      "Run: 02, Epoch: 72, Loss: 0.0018, Train: 100.00%, Valid: 70.80% Test: 71.00%\n",
      "Run: 02, Epoch: 73, Loss: 0.0021, Train: 100.00%, Valid: 70.80% Test: 71.10%\n",
      "Run: 02, Epoch: 74, Loss: 0.0061, Train: 100.00%, Valid: 70.80% Test: 70.80%\n",
      "Run: 02, Epoch: 75, Loss: 0.0089, Train: 100.00%, Valid: 70.80% Test: 71.00%\n",
      "Run: 02, Epoch: 76, Loss: 0.0042, Train: 100.00%, Valid: 70.80% Test: 71.20%\n",
      "Run: 02, Epoch: 77, Loss: 0.0016, Train: 100.00%, Valid: 70.40% Test: 71.20%\n",
      "Run: 02, Epoch: 78, Loss: 0.0151, Train: 100.00%, Valid: 70.20% Test: 71.10%\n",
      "Run: 02, Epoch: 79, Loss: 0.0005, Train: 100.00%, Valid: 70.20% Test: 71.10%\n",
      "Run: 02, Epoch: 80, Loss: 0.0007, Train: 100.00%, Valid: 70.40% Test: 71.10%\n",
      "Run: 02, Epoch: 81, Loss: 0.0142, Train: 100.00%, Valid: 70.40% Test: 71.00%\n",
      "Run: 02, Epoch: 82, Loss: 0.0055, Train: 100.00%, Valid: 70.40% Test: 71.00%\n",
      "Run: 02, Epoch: 83, Loss: 0.0020, Train: 100.00%, Valid: 70.80% Test: 70.90%\n",
      "Run: 02, Epoch: 84, Loss: 0.0216, Train: 100.00%, Valid: 70.80% Test: 71.00%\n",
      "Run: 02, Epoch: 85, Loss: 0.0006, Train: 100.00%, Valid: 70.60% Test: 71.10%\n",
      "Run: 02, Epoch: 86, Loss: 0.0003, Train: 100.00%, Valid: 70.60% Test: 71.20%\n",
      "Run: 02, Epoch: 87, Loss: 0.0054, Train: 100.00%, Valid: 70.80% Test: 71.20%\n",
      "Run: 02, Epoch: 88, Loss: 0.0012, Train: 100.00%, Valid: 70.80% Test: 71.20%\n",
      "Run: 02, Epoch: 89, Loss: 0.0236, Train: 100.00%, Valid: 70.80% Test: 71.30%\n",
      "Run: 02, Epoch: 90, Loss: 0.0009, Train: 100.00%, Valid: 70.80% Test: 71.40%\n",
      "Run: 02, Epoch: 91, Loss: 0.0063, Train: 100.00%, Valid: 70.80% Test: 71.20%\n",
      "Run: 02, Epoch: 92, Loss: 0.0007, Train: 100.00%, Valid: 70.80% Test: 71.20%\n",
      "Run: 02, Epoch: 93, Loss: 0.0098, Train: 100.00%, Valid: 71.20% Test: 71.10%\n",
      "Run: 02, Epoch: 94, Loss: 0.0018, Train: 100.00%, Valid: 71.40% Test: 70.80%\n",
      "Run: 02, Epoch: 95, Loss: 0.0127, Train: 100.00%, Valid: 71.40% Test: 70.60%\n",
      "Run: 02, Epoch: 96, Loss: 0.0022, Train: 100.00%, Valid: 70.80% Test: 70.80%\n",
      "Run: 02, Epoch: 97, Loss: 0.0032, Train: 100.00%, Valid: 70.40% Test: 70.70%\n",
      "Run: 02, Epoch: 98, Loss: 0.0033, Train: 100.00%, Valid: 70.40% Test: 70.50%\n",
      "Run: 02, Epoch: 99, Loss: 0.0027, Train: 100.00%, Valid: 70.40% Test: 70.60%\n",
      "Run: 02, Epoch: 100, Loss: 0.0029, Train: 100.00%, Valid: 70.40% Test: 70.60%\n",
      "Run 02:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 74.40\n",
      "  Final Train: 100.00\n",
      "   Final Test: 72.80\n",
      "Run: 03, Epoch: 01, Loss: 1.0801, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 03, Epoch: 02, Loss: 0.6143, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 03, Epoch: 03, Loss: 0.3997, Train: 35.00%, Valid: 41.60% Test: 40.70%\n",
      "Run: 03, Epoch: 04, Loss: 0.3196, Train: 41.67%, Valid: 43.00% Test: 41.90%\n",
      "Run: 03, Epoch: 05, Loss: 0.2615, Train: 56.67%, Valid: 45.40% Test: 45.10%\n",
      "Run: 03, Epoch: 06, Loss: 0.1762, Train: 61.67%, Valid: 48.60% Test: 48.10%\n",
      "Run: 03, Epoch: 07, Loss: 0.1684, Train: 68.33%, Valid: 51.00% Test: 49.50%\n",
      "Run: 03, Epoch: 08, Loss: 0.1363, Train: 71.67%, Valid: 51.60% Test: 50.70%\n",
      "Run: 03, Epoch: 09, Loss: 0.1313, Train: 76.67%, Valid: 52.00% Test: 50.90%\n",
      "Run: 03, Epoch: 10, Loss: 0.0583, Train: 83.33%, Valid: 52.60% Test: 51.30%\n",
      "Run: 03, Epoch: 11, Loss: 0.0965, Train: 86.67%, Valid: 53.00% Test: 52.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 03, Epoch: 12, Loss: 0.0844, Train: 93.33%, Valid: 54.60% Test: 53.00%\n",
      "Run: 03, Epoch: 13, Loss: 0.0569, Train: 96.67%, Valid: 56.60% Test: 53.70%\n",
      "Run: 03, Epoch: 14, Loss: 0.0391, Train: 98.33%, Valid: 58.00% Test: 55.00%\n",
      "Run: 03, Epoch: 15, Loss: 0.0565, Train: 100.00%, Valid: 59.80% Test: 56.90%\n",
      "Run: 03, Epoch: 16, Loss: 0.0268, Train: 100.00%, Valid: 61.60% Test: 58.70%\n",
      "Run: 03, Epoch: 17, Loss: 0.0573, Train: 100.00%, Valid: 62.60% Test: 60.00%\n",
      "Run: 03, Epoch: 18, Loss: 0.0449, Train: 100.00%, Valid: 63.40% Test: 61.30%\n",
      "Run: 03, Epoch: 19, Loss: 0.0163, Train: 100.00%, Valid: 64.60% Test: 62.70%\n",
      "Run: 03, Epoch: 20, Loss: 0.0192, Train: 100.00%, Valid: 65.40% Test: 63.70%\n",
      "Run: 03, Epoch: 21, Loss: 0.0303, Train: 100.00%, Valid: 67.60% Test: 65.40%\n",
      "Run: 03, Epoch: 22, Loss: 0.0151, Train: 100.00%, Valid: 68.20% Test: 66.60%\n",
      "Run: 03, Epoch: 23, Loss: 0.0384, Train: 100.00%, Valid: 69.00% Test: 68.50%\n",
      "Run: 03, Epoch: 24, Loss: 0.0516, Train: 100.00%, Valid: 69.40% Test: 68.90%\n",
      "Run: 03, Epoch: 25, Loss: 0.0233, Train: 100.00%, Valid: 69.60% Test: 69.80%\n",
      "Run: 03, Epoch: 26, Loss: 0.0302, Train: 100.00%, Valid: 70.00% Test: 69.60%\n",
      "Run: 03, Epoch: 27, Loss: 0.0052, Train: 100.00%, Valid: 70.00% Test: 70.00%\n",
      "Run: 03, Epoch: 28, Loss: 0.0062, Train: 100.00%, Valid: 70.40% Test: 69.80%\n",
      "Run: 03, Epoch: 29, Loss: 0.0098, Train: 100.00%, Valid: 70.80% Test: 69.80%\n",
      "Run: 03, Epoch: 30, Loss: 0.0205, Train: 100.00%, Valid: 71.20% Test: 69.70%\n",
      "Run: 03, Epoch: 31, Loss: 0.0343, Train: 100.00%, Valid: 71.00% Test: 69.90%\n",
      "Run: 03, Epoch: 32, Loss: 0.0083, Train: 100.00%, Valid: 71.40% Test: 69.90%\n",
      "Run: 03, Epoch: 33, Loss: 0.0060, Train: 100.00%, Valid: 71.20% Test: 69.90%\n",
      "Run: 03, Epoch: 34, Loss: 0.0064, Train: 100.00%, Valid: 70.80% Test: 70.00%\n",
      "Run: 03, Epoch: 35, Loss: 0.0181, Train: 100.00%, Valid: 71.00% Test: 70.40%\n",
      "Run: 03, Epoch: 36, Loss: 0.0118, Train: 100.00%, Valid: 70.60% Test: 70.00%\n",
      "Run: 03, Epoch: 37, Loss: 0.0061, Train: 100.00%, Valid: 71.20% Test: 70.00%\n",
      "Run: 03, Epoch: 38, Loss: 0.0220, Train: 100.00%, Valid: 71.40% Test: 69.90%\n",
      "Run: 03, Epoch: 39, Loss: 0.0047, Train: 100.00%, Valid: 71.40% Test: 70.10%\n",
      "Run: 03, Epoch: 40, Loss: 0.0289, Train: 100.00%, Valid: 71.60% Test: 70.20%\n",
      "Run: 03, Epoch: 41, Loss: 0.0362, Train: 100.00%, Valid: 71.40% Test: 70.10%\n",
      "Run: 03, Epoch: 42, Loss: 0.0282, Train: 100.00%, Valid: 71.60% Test: 69.90%\n",
      "Run: 03, Epoch: 43, Loss: 0.0044, Train: 100.00%, Valid: 71.80% Test: 70.60%\n",
      "Run: 03, Epoch: 44, Loss: 0.0015, Train: 100.00%, Valid: 71.80% Test: 70.50%\n",
      "Run: 03, Epoch: 45, Loss: 0.0287, Train: 100.00%, Valid: 72.00% Test: 70.60%\n",
      "Run: 03, Epoch: 46, Loss: 0.0076, Train: 100.00%, Valid: 71.80% Test: 70.50%\n",
      "Run: 03, Epoch: 47, Loss: 0.0051, Train: 100.00%, Valid: 71.80% Test: 70.30%\n",
      "Run: 03, Epoch: 48, Loss: 0.0050, Train: 100.00%, Valid: 71.80% Test: 70.40%\n",
      "Run: 03, Epoch: 49, Loss: 0.0034, Train: 100.00%, Valid: 71.80% Test: 70.30%\n",
      "Run: 03, Epoch: 50, Loss: 0.0194, Train: 100.00%, Valid: 71.80% Test: 70.30%\n",
      "Run: 03, Epoch: 51, Loss: 0.0257, Train: 100.00%, Valid: 72.00% Test: 70.50%\n",
      "Run: 03, Epoch: 52, Loss: 0.0014, Train: 100.00%, Valid: 72.40% Test: 70.30%\n",
      "Run: 03, Epoch: 53, Loss: 0.0274, Train: 100.00%, Valid: 72.60% Test: 70.20%\n",
      "Run: 03, Epoch: 54, Loss: 0.0063, Train: 100.00%, Valid: 72.80% Test: 70.20%\n",
      "Run: 03, Epoch: 55, Loss: 0.0023, Train: 100.00%, Valid: 72.60% Test: 70.10%\n",
      "Run: 03, Epoch: 56, Loss: 0.0275, Train: 100.00%, Valid: 73.00% Test: 70.00%\n",
      "Run: 03, Epoch: 57, Loss: 0.0066, Train: 100.00%, Valid: 72.40% Test: 70.30%\n",
      "Run: 03, Epoch: 58, Loss: 0.0344, Train: 100.00%, Valid: 72.80% Test: 70.60%\n",
      "Run: 03, Epoch: 59, Loss: 0.0152, Train: 100.00%, Valid: 73.40% Test: 70.80%\n",
      "Run: 03, Epoch: 60, Loss: 0.0017, Train: 100.00%, Valid: 73.60% Test: 70.80%\n",
      "Run: 03, Epoch: 61, Loss: 0.0043, Train: 100.00%, Valid: 73.00% Test: 70.90%\n",
      "Run: 03, Epoch: 62, Loss: 0.0158, Train: 100.00%, Valid: 72.80% Test: 71.20%\n",
      "Run: 03, Epoch: 63, Loss: 0.0036, Train: 100.00%, Valid: 72.40% Test: 71.60%\n",
      "Run: 03, Epoch: 64, Loss: 0.0093, Train: 100.00%, Valid: 72.20% Test: 71.40%\n",
      "Run: 03, Epoch: 65, Loss: 0.0210, Train: 100.00%, Valid: 72.00% Test: 71.40%\n",
      "Run: 03, Epoch: 66, Loss: 0.0023, Train: 100.00%, Valid: 71.60% Test: 71.40%\n",
      "Run: 03, Epoch: 67, Loss: 0.0071, Train: 100.00%, Valid: 71.60% Test: 71.30%\n",
      "Run: 03, Epoch: 68, Loss: 0.0008, Train: 100.00%, Valid: 71.60% Test: 71.50%\n",
      "Run: 03, Epoch: 69, Loss: 0.0178, Train: 100.00%, Valid: 71.80% Test: 71.50%\n",
      "Run: 03, Epoch: 70, Loss: 0.0005, Train: 100.00%, Valid: 71.80% Test: 71.40%\n",
      "Run: 03, Epoch: 71, Loss: 0.0016, Train: 100.00%, Valid: 72.00% Test: 71.40%\n",
      "Run: 03, Epoch: 72, Loss: 0.0030, Train: 100.00%, Valid: 72.00% Test: 71.40%\n",
      "Run: 03, Epoch: 73, Loss: 0.0024, Train: 100.00%, Valid: 71.80% Test: 71.50%\n",
      "Run: 03, Epoch: 74, Loss: 0.0258, Train: 100.00%, Valid: 72.00% Test: 71.50%\n",
      "Run: 03, Epoch: 75, Loss: 0.0062, Train: 100.00%, Valid: 71.80% Test: 71.40%\n",
      "Run: 03, Epoch: 76, Loss: 0.0197, Train: 100.00%, Valid: 72.00% Test: 71.50%\n",
      "Run: 03, Epoch: 77, Loss: 0.0024, Train: 100.00%, Valid: 72.00% Test: 71.40%\n",
      "Run: 03, Epoch: 78, Loss: 0.0048, Train: 100.00%, Valid: 72.20% Test: 71.50%\n",
      "Run: 03, Epoch: 79, Loss: 0.0062, Train: 100.00%, Valid: 72.20% Test: 71.50%\n",
      "Run: 03, Epoch: 80, Loss: 0.0034, Train: 100.00%, Valid: 72.20% Test: 71.50%\n",
      "Run: 03, Epoch: 81, Loss: 0.0003, Train: 100.00%, Valid: 72.20% Test: 71.50%\n",
      "Run: 03, Epoch: 82, Loss: 0.0014, Train: 100.00%, Valid: 72.40% Test: 71.80%\n",
      "Run: 03, Epoch: 83, Loss: 0.0023, Train: 100.00%, Valid: 72.40% Test: 71.80%\n",
      "Run: 03, Epoch: 84, Loss: 0.0111, Train: 100.00%, Valid: 72.40% Test: 71.90%\n",
      "Run: 03, Epoch: 85, Loss: 0.0251, Train: 100.00%, Valid: 72.20% Test: 71.80%\n",
      "Run: 03, Epoch: 86, Loss: 0.0033, Train: 100.00%, Valid: 72.20% Test: 71.90%\n",
      "Run: 03, Epoch: 87, Loss: 0.0022, Train: 100.00%, Valid: 72.20% Test: 72.00%\n",
      "Run: 03, Epoch: 88, Loss: 0.0044, Train: 100.00%, Valid: 72.20% Test: 72.00%\n",
      "Run: 03, Epoch: 89, Loss: 0.0091, Train: 100.00%, Valid: 72.20% Test: 72.10%\n",
      "Run: 03, Epoch: 90, Loss: 0.0037, Train: 100.00%, Valid: 72.00% Test: 72.10%\n",
      "Run: 03, Epoch: 91, Loss: 0.0129, Train: 100.00%, Valid: 71.60% Test: 72.10%\n",
      "Run: 03, Epoch: 92, Loss: 0.0017, Train: 100.00%, Valid: 71.40% Test: 71.80%\n",
      "Run: 03, Epoch: 93, Loss: 0.0133, Train: 100.00%, Valid: 71.40% Test: 71.90%\n",
      "Run: 03, Epoch: 94, Loss: 0.0027, Train: 100.00%, Valid: 71.40% Test: 72.00%\n",
      "Run: 03, Epoch: 95, Loss: 0.0001, Train: 100.00%, Valid: 71.40% Test: 71.90%\n",
      "Run: 03, Epoch: 96, Loss: 0.0028, Train: 100.00%, Valid: 71.40% Test: 71.70%\n",
      "Run: 03, Epoch: 97, Loss: 0.0070, Train: 100.00%, Valid: 71.40% Test: 71.70%\n",
      "Run: 03, Epoch: 98, Loss: 0.0022, Train: 100.00%, Valid: 71.60% Test: 71.70%\n",
      "Run: 03, Epoch: 99, Loss: 0.0005, Train: 100.00%, Valid: 71.60% Test: 71.40%\n",
      "Run: 03, Epoch: 100, Loss: 0.0101, Train: 100.00%, Valid: 71.80% Test: 71.40%\n",
      "Run 03:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 73.60\n",
      "  Final Train: 100.00\n",
      "   Final Test: 70.80\n",
      "Run: 04, Epoch: 01, Loss: 1.2679, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 04, Epoch: 02, Loss: 0.6368, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 04, Epoch: 03, Loss: 0.5355, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 04, Epoch: 04, Loss: 0.3041, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 04, Epoch: 05, Loss: 0.3177, Train: 36.67%, Valid: 42.20% Test: 41.00%\n",
      "Run: 04, Epoch: 06, Loss: 0.2674, Train: 53.33%, Valid: 44.60% Test: 43.30%\n",
      "Run: 04, Epoch: 07, Loss: 0.1974, Train: 78.33%, Valid: 47.60% Test: 47.50%\n",
      "Run: 04, Epoch: 08, Loss: 0.1568, Train: 93.33%, Valid: 52.00% Test: 51.50%\n",
      "Run: 04, Epoch: 09, Loss: 0.1227, Train: 96.67%, Valid: 54.80% Test: 54.90%\n",
      "Run: 04, Epoch: 10, Loss: 0.1308, Train: 96.67%, Valid: 61.40% Test: 58.40%\n",
      "Run: 04, Epoch: 11, Loss: 0.1008, Train: 96.67%, Valid: 63.80% Test: 62.10%\n",
      "Run: 04, Epoch: 12, Loss: 0.1076, Train: 98.33%, Valid: 67.00% Test: 65.00%\n",
      "Run: 04, Epoch: 13, Loss: 0.0891, Train: 98.33%, Valid: 69.20% Test: 67.70%\n",
      "Run: 04, Epoch: 14, Loss: 0.0886, Train: 98.33%, Valid: 71.60% Test: 69.20%\n",
      "Run: 04, Epoch: 15, Loss: 0.0385, Train: 98.33%, Valid: 71.20% Test: 71.10%\n",
      "Run: 04, Epoch: 16, Loss: 0.0642, Train: 98.33%, Valid: 71.80% Test: 71.30%\n",
      "Run: 04, Epoch: 17, Loss: 0.0793, Train: 98.33%, Valid: 72.60% Test: 72.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 04, Epoch: 18, Loss: 0.0556, Train: 100.00%, Valid: 73.20% Test: 73.10%\n",
      "Run: 04, Epoch: 19, Loss: 0.0317, Train: 100.00%, Valid: 73.80% Test: 72.90%\n",
      "Run: 04, Epoch: 20, Loss: 0.0481, Train: 100.00%, Valid: 74.20% Test: 73.10%\n",
      "Run: 04, Epoch: 21, Loss: 0.0319, Train: 100.00%, Valid: 73.80% Test: 73.40%\n",
      "Run: 04, Epoch: 22, Loss: 0.0314, Train: 100.00%, Valid: 73.40% Test: 73.60%\n",
      "Run: 04, Epoch: 23, Loss: 0.0210, Train: 100.00%, Valid: 73.60% Test: 73.10%\n",
      "Run: 04, Epoch: 24, Loss: 0.0124, Train: 100.00%, Valid: 73.40% Test: 73.00%\n",
      "Run: 04, Epoch: 25, Loss: 0.0244, Train: 100.00%, Valid: 73.80% Test: 72.70%\n",
      "Run: 04, Epoch: 26, Loss: 0.0063, Train: 100.00%, Valid: 73.60% Test: 72.50%\n",
      "Run: 04, Epoch: 27, Loss: 0.0123, Train: 100.00%, Valid: 73.80% Test: 72.20%\n",
      "Run: 04, Epoch: 28, Loss: 0.0161, Train: 100.00%, Valid: 74.20% Test: 72.10%\n",
      "Run: 04, Epoch: 29, Loss: 0.0155, Train: 100.00%, Valid: 74.20% Test: 71.70%\n",
      "Run: 04, Epoch: 30, Loss: 0.0242, Train: 100.00%, Valid: 74.40% Test: 71.30%\n",
      "Run: 04, Epoch: 31, Loss: 0.0153, Train: 100.00%, Valid: 74.60% Test: 71.30%\n",
      "Run: 04, Epoch: 32, Loss: 0.0279, Train: 100.00%, Valid: 74.40% Test: 71.30%\n",
      "Run: 04, Epoch: 33, Loss: 0.0157, Train: 100.00%, Valid: 74.20% Test: 71.30%\n",
      "Run: 04, Epoch: 34, Loss: 0.0388, Train: 100.00%, Valid: 74.20% Test: 71.70%\n",
      "Run: 04, Epoch: 35, Loss: 0.0058, Train: 100.00%, Valid: 74.00% Test: 71.60%\n",
      "Run: 04, Epoch: 36, Loss: 0.0228, Train: 100.00%, Valid: 74.00% Test: 71.50%\n",
      "Run: 04, Epoch: 37, Loss: 0.0286, Train: 100.00%, Valid: 74.40% Test: 71.50%\n",
      "Run: 04, Epoch: 38, Loss: 0.0133, Train: 100.00%, Valid: 74.40% Test: 71.70%\n",
      "Run: 04, Epoch: 39, Loss: 0.0208, Train: 100.00%, Valid: 74.20% Test: 71.40%\n",
      "Run: 04, Epoch: 40, Loss: 0.0147, Train: 100.00%, Valid: 74.20% Test: 71.80%\n",
      "Run: 04, Epoch: 41, Loss: 0.0142, Train: 100.00%, Valid: 74.00% Test: 71.70%\n",
      "Run: 04, Epoch: 42, Loss: 0.0021, Train: 100.00%, Valid: 74.20% Test: 71.70%\n",
      "Run: 04, Epoch: 43, Loss: 0.0251, Train: 100.00%, Valid: 74.20% Test: 71.80%\n",
      "Run: 04, Epoch: 44, Loss: 0.0065, Train: 100.00%, Valid: 74.20% Test: 71.80%\n",
      "Run: 04, Epoch: 45, Loss: 0.0013, Train: 100.00%, Valid: 73.80% Test: 72.00%\n",
      "Run: 04, Epoch: 46, Loss: 0.0031, Train: 100.00%, Valid: 73.80% Test: 72.20%\n",
      "Run: 04, Epoch: 47, Loss: 0.0153, Train: 100.00%, Valid: 73.80% Test: 72.20%\n",
      "Run: 04, Epoch: 48, Loss: 0.0070, Train: 100.00%, Valid: 73.80% Test: 72.10%\n",
      "Run: 04, Epoch: 49, Loss: 0.0259, Train: 100.00%, Valid: 73.80% Test: 72.10%\n",
      "Run: 04, Epoch: 50, Loss: 0.0014, Train: 100.00%, Valid: 73.80% Test: 72.10%\n",
      "Run: 04, Epoch: 51, Loss: 0.0051, Train: 100.00%, Valid: 73.80% Test: 72.10%\n",
      "Run: 04, Epoch: 52, Loss: 0.0074, Train: 100.00%, Valid: 73.80% Test: 72.00%\n",
      "Run: 04, Epoch: 53, Loss: 0.0013, Train: 100.00%, Valid: 73.80% Test: 72.00%\n",
      "Run: 04, Epoch: 54, Loss: 0.0053, Train: 100.00%, Valid: 74.00% Test: 71.90%\n",
      "Run: 04, Epoch: 55, Loss: 0.0079, Train: 100.00%, Valid: 74.00% Test: 71.90%\n",
      "Run: 04, Epoch: 56, Loss: 0.0031, Train: 100.00%, Valid: 74.00% Test: 71.80%\n",
      "Run: 04, Epoch: 57, Loss: 0.0023, Train: 100.00%, Valid: 73.80% Test: 71.50%\n",
      "Run: 04, Epoch: 58, Loss: 0.0019, Train: 100.00%, Valid: 73.80% Test: 71.50%\n",
      "Run: 04, Epoch: 59, Loss: 0.0073, Train: 100.00%, Valid: 73.60% Test: 71.50%\n",
      "Run: 04, Epoch: 60, Loss: 0.0055, Train: 100.00%, Valid: 73.40% Test: 71.60%\n",
      "Run: 04, Epoch: 61, Loss: 0.0245, Train: 100.00%, Valid: 73.60% Test: 71.70%\n",
      "Run: 04, Epoch: 62, Loss: 0.0168, Train: 100.00%, Valid: 73.60% Test: 71.80%\n",
      "Run: 04, Epoch: 63, Loss: 0.0212, Train: 100.00%, Valid: 73.40% Test: 71.90%\n",
      "Run: 04, Epoch: 64, Loss: 0.0037, Train: 100.00%, Valid: 73.40% Test: 71.80%\n",
      "Run: 04, Epoch: 65, Loss: 0.0111, Train: 100.00%, Valid: 73.40% Test: 71.80%\n",
      "Run: 04, Epoch: 66, Loss: 0.0183, Train: 100.00%, Valid: 73.00% Test: 71.80%\n",
      "Run: 04, Epoch: 67, Loss: 0.0077, Train: 100.00%, Valid: 73.20% Test: 71.90%\n",
      "Run: 04, Epoch: 68, Loss: 0.0039, Train: 100.00%, Valid: 73.20% Test: 72.00%\n",
      "Run: 04, Epoch: 69, Loss: 0.0026, Train: 100.00%, Valid: 73.00% Test: 71.90%\n",
      "Run: 04, Epoch: 70, Loss: 0.0015, Train: 100.00%, Valid: 73.20% Test: 71.90%\n",
      "Run: 04, Epoch: 71, Loss: 0.0017, Train: 100.00%, Valid: 73.00% Test: 71.90%\n",
      "Run: 04, Epoch: 72, Loss: 0.0016, Train: 100.00%, Valid: 72.80% Test: 71.90%\n",
      "Run: 04, Epoch: 73, Loss: 0.0023, Train: 100.00%, Valid: 72.80% Test: 72.00%\n",
      "Run: 04, Epoch: 74, Loss: 0.0013, Train: 100.00%, Valid: 72.60% Test: 72.10%\n",
      "Run: 04, Epoch: 75, Loss: 0.0007, Train: 100.00%, Valid: 72.60% Test: 72.00%\n",
      "Run: 04, Epoch: 76, Loss: 0.0008, Train: 100.00%, Valid: 72.60% Test: 72.00%\n",
      "Run: 04, Epoch: 77, Loss: 0.0043, Train: 100.00%, Valid: 72.60% Test: 71.90%\n",
      "Run: 04, Epoch: 78, Loss: 0.0020, Train: 100.00%, Valid: 72.60% Test: 71.90%\n",
      "Run: 04, Epoch: 79, Loss: 0.0016, Train: 100.00%, Valid: 72.60% Test: 71.90%\n",
      "Run: 04, Epoch: 80, Loss: 0.0003, Train: 100.00%, Valid: 72.60% Test: 71.90%\n",
      "Run: 04, Epoch: 81, Loss: 0.0105, Train: 100.00%, Valid: 72.60% Test: 72.00%\n",
      "Run: 04, Epoch: 82, Loss: 0.0022, Train: 100.00%, Valid: 72.60% Test: 72.00%\n",
      "Run: 04, Epoch: 83, Loss: 0.0010, Train: 100.00%, Valid: 72.60% Test: 72.00%\n",
      "Run: 04, Epoch: 84, Loss: 0.0073, Train: 100.00%, Valid: 72.60% Test: 72.00%\n",
      "Run: 04, Epoch: 85, Loss: 0.0006, Train: 100.00%, Valid: 72.40% Test: 72.10%\n",
      "Run: 04, Epoch: 86, Loss: 0.0190, Train: 100.00%, Valid: 72.40% Test: 72.20%\n",
      "Run: 04, Epoch: 87, Loss: 0.0188, Train: 100.00%, Valid: 72.20% Test: 71.80%\n",
      "Run: 04, Epoch: 88, Loss: 0.0024, Train: 100.00%, Valid: 72.20% Test: 71.70%\n",
      "Run: 04, Epoch: 89, Loss: 0.0019, Train: 100.00%, Valid: 72.20% Test: 71.70%\n",
      "Run: 04, Epoch: 90, Loss: 0.0050, Train: 100.00%, Valid: 72.20% Test: 71.60%\n",
      "Run: 04, Epoch: 91, Loss: 0.0025, Train: 100.00%, Valid: 72.00% Test: 71.70%\n",
      "Run: 04, Epoch: 92, Loss: 0.0243, Train: 100.00%, Valid: 72.20% Test: 71.50%\n",
      "Run: 04, Epoch: 93, Loss: 0.0017, Train: 100.00%, Valid: 72.40% Test: 71.60%\n",
      "Run: 04, Epoch: 94, Loss: 0.0034, Train: 100.00%, Valid: 72.20% Test: 71.60%\n",
      "Run: 04, Epoch: 95, Loss: 0.0067, Train: 100.00%, Valid: 72.20% Test: 71.70%\n",
      "Run: 04, Epoch: 96, Loss: 0.0134, Train: 100.00%, Valid: 72.20% Test: 71.60%\n",
      "Run: 04, Epoch: 97, Loss: 0.0073, Train: 100.00%, Valid: 72.20% Test: 71.30%\n",
      "Run: 04, Epoch: 98, Loss: 0.0043, Train: 100.00%, Valid: 72.40% Test: 71.20%\n",
      "Run: 04, Epoch: 99, Loss: 0.0058, Train: 100.00%, Valid: 72.40% Test: 71.20%\n",
      "Run: 04, Epoch: 100, Loss: 0.0004, Train: 100.00%, Valid: 72.20% Test: 71.20%\n",
      "Run 04:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 74.60\n",
      "  Final Train: 100.00\n",
      "   Final Test: 71.30\n",
      "Run: 05, Epoch: 01, Loss: 1.3142, Train: 33.33%, Valid: 38.80% Test: 41.30%\n",
      "Run: 05, Epoch: 02, Loss: 0.6243, Train: 33.33%, Valid: 39.00% Test: 41.40%\n",
      "Run: 05, Epoch: 03, Loss: 0.4639, Train: 36.67%, Valid: 39.00% Test: 41.50%\n",
      "Run: 05, Epoch: 04, Loss: 0.3371, Train: 38.33%, Valid: 39.20% Test: 41.50%\n",
      "Run: 05, Epoch: 05, Loss: 0.3110, Train: 40.00%, Valid: 39.20% Test: 41.80%\n",
      "Run: 05, Epoch: 06, Loss: 0.1543, Train: 43.33%, Valid: 39.20% Test: 42.00%\n",
      "Run: 05, Epoch: 07, Loss: 0.1602, Train: 50.00%, Valid: 39.60% Test: 42.10%\n",
      "Run: 05, Epoch: 08, Loss: 0.1411, Train: 56.67%, Valid: 41.00% Test: 43.40%\n",
      "Run: 05, Epoch: 09, Loss: 0.0959, Train: 61.67%, Valid: 41.40% Test: 44.10%\n",
      "Run: 05, Epoch: 10, Loss: 0.1060, Train: 70.00%, Valid: 43.40% Test: 45.40%\n",
      "Run: 05, Epoch: 11, Loss: 0.0904, Train: 78.33%, Valid: 44.80% Test: 46.70%\n",
      "Run: 05, Epoch: 12, Loss: 0.1057, Train: 91.67%, Valid: 48.20% Test: 49.30%\n",
      "Run: 05, Epoch: 13, Loss: 0.0670, Train: 96.67%, Valid: 51.40% Test: 51.40%\n",
      "Run: 05, Epoch: 14, Loss: 0.0250, Train: 96.67%, Valid: 54.40% Test: 53.60%\n",
      "Run: 05, Epoch: 15, Loss: 0.0194, Train: 96.67%, Valid: 56.60% Test: 56.00%\n",
      "Run: 05, Epoch: 16, Loss: 0.0342, Train: 98.33%, Valid: 59.60% Test: 58.60%\n",
      "Run: 05, Epoch: 17, Loss: 0.0496, Train: 100.00%, Valid: 61.80% Test: 60.80%\n",
      "Run: 05, Epoch: 18, Loss: 0.0426, Train: 100.00%, Valid: 64.40% Test: 63.50%\n",
      "Run: 05, Epoch: 19, Loss: 0.0506, Train: 100.00%, Valid: 65.80% Test: 65.90%\n",
      "Run: 05, Epoch: 20, Loss: 0.0331, Train: 100.00%, Valid: 66.40% Test: 66.50%\n",
      "Run: 05, Epoch: 21, Loss: 0.0309, Train: 100.00%, Valid: 67.20% Test: 67.10%\n",
      "Run: 05, Epoch: 22, Loss: 0.0389, Train: 100.00%, Valid: 66.60% Test: 67.40%\n",
      "Run: 05, Epoch: 23, Loss: 0.0056, Train: 100.00%, Valid: 67.00% Test: 67.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 05, Epoch: 24, Loss: 0.0144, Train: 100.00%, Valid: 67.40% Test: 68.50%\n",
      "Run: 05, Epoch: 25, Loss: 0.0203, Train: 100.00%, Valid: 67.60% Test: 68.80%\n",
      "Run: 05, Epoch: 26, Loss: 0.0191, Train: 100.00%, Valid: 68.40% Test: 69.10%\n",
      "Run: 05, Epoch: 27, Loss: 0.0168, Train: 100.00%, Valid: 69.40% Test: 68.90%\n",
      "Run: 05, Epoch: 28, Loss: 0.0070, Train: 100.00%, Valid: 69.60% Test: 68.90%\n",
      "Run: 05, Epoch: 29, Loss: 0.0067, Train: 100.00%, Valid: 69.60% Test: 69.10%\n",
      "Run: 05, Epoch: 30, Loss: 0.0040, Train: 100.00%, Valid: 70.80% Test: 70.00%\n",
      "Run: 05, Epoch: 31, Loss: 0.0258, Train: 100.00%, Valid: 71.20% Test: 69.80%\n",
      "Run: 05, Epoch: 32, Loss: 0.0159, Train: 100.00%, Valid: 71.00% Test: 69.90%\n",
      "Run: 05, Epoch: 33, Loss: 0.0092, Train: 100.00%, Valid: 71.40% Test: 70.00%\n",
      "Run: 05, Epoch: 34, Loss: 0.0082, Train: 100.00%, Valid: 70.60% Test: 69.90%\n",
      "Run: 05, Epoch: 35, Loss: 0.0040, Train: 100.00%, Valid: 70.80% Test: 70.20%\n",
      "Run: 05, Epoch: 36, Loss: 0.0116, Train: 100.00%, Valid: 71.40% Test: 70.70%\n",
      "Run: 05, Epoch: 37, Loss: 0.0299, Train: 100.00%, Valid: 71.60% Test: 70.90%\n",
      "Run: 05, Epoch: 38, Loss: 0.0034, Train: 100.00%, Valid: 71.60% Test: 70.90%\n",
      "Run: 05, Epoch: 39, Loss: 0.0056, Train: 100.00%, Valid: 71.40% Test: 71.10%\n",
      "Run: 05, Epoch: 40, Loss: 0.0021, Train: 100.00%, Valid: 71.80% Test: 71.20%\n",
      "Run: 05, Epoch: 41, Loss: 0.0119, Train: 100.00%, Valid: 71.60% Test: 71.10%\n",
      "Run: 05, Epoch: 42, Loss: 0.0168, Train: 100.00%, Valid: 71.60% Test: 71.20%\n",
      "Run: 05, Epoch: 43, Loss: 0.0036, Train: 100.00%, Valid: 71.80% Test: 71.20%\n",
      "Run: 05, Epoch: 44, Loss: 0.0030, Train: 100.00%, Valid: 72.00% Test: 71.40%\n",
      "Run: 05, Epoch: 45, Loss: 0.0142, Train: 100.00%, Valid: 72.00% Test: 71.00%\n",
      "Run: 05, Epoch: 46, Loss: 0.0130, Train: 100.00%, Valid: 72.40% Test: 71.20%\n",
      "Run: 05, Epoch: 47, Loss: 0.0048, Train: 100.00%, Valid: 72.00% Test: 71.20%\n",
      "Run: 05, Epoch: 48, Loss: 0.0039, Train: 100.00%, Valid: 71.80% Test: 71.20%\n",
      "Run: 05, Epoch: 49, Loss: 0.0041, Train: 100.00%, Valid: 72.00% Test: 71.00%\n",
      "Run: 05, Epoch: 50, Loss: 0.0082, Train: 100.00%, Valid: 71.60% Test: 71.00%\n",
      "Run: 05, Epoch: 51, Loss: 0.0013, Train: 100.00%, Valid: 72.00% Test: 71.30%\n",
      "Run: 05, Epoch: 52, Loss: 0.0026, Train: 100.00%, Valid: 72.20% Test: 71.20%\n",
      "Run: 05, Epoch: 53, Loss: 0.0019, Train: 100.00%, Valid: 72.20% Test: 71.40%\n",
      "Run: 05, Epoch: 54, Loss: 0.0015, Train: 100.00%, Valid: 72.00% Test: 71.40%\n",
      "Run: 05, Epoch: 55, Loss: 0.0032, Train: 100.00%, Valid: 72.00% Test: 71.50%\n",
      "Run: 05, Epoch: 56, Loss: 0.0062, Train: 100.00%, Valid: 72.00% Test: 71.50%\n",
      "Run: 05, Epoch: 57, Loss: 0.0069, Train: 100.00%, Valid: 71.80% Test: 71.50%\n",
      "Run: 05, Epoch: 58, Loss: 0.0043, Train: 100.00%, Valid: 71.80% Test: 71.50%\n",
      "Run: 05, Epoch: 59, Loss: 0.0060, Train: 100.00%, Valid: 71.60% Test: 71.30%\n",
      "Run: 05, Epoch: 60, Loss: 0.0022, Train: 100.00%, Valid: 71.40% Test: 70.80%\n",
      "Run: 05, Epoch: 61, Loss: 0.0008, Train: 100.00%, Valid: 71.20% Test: 70.90%\n",
      "Run: 05, Epoch: 62, Loss: 0.0012, Train: 100.00%, Valid: 70.80% Test: 70.90%\n",
      "Run: 05, Epoch: 63, Loss: 0.0006, Train: 100.00%, Valid: 70.40% Test: 70.90%\n",
      "Run: 05, Epoch: 64, Loss: 0.0009, Train: 100.00%, Valid: 70.20% Test: 70.90%\n",
      "Run: 05, Epoch: 65, Loss: 0.0077, Train: 100.00%, Valid: 70.80% Test: 70.80%\n",
      "Run: 05, Epoch: 66, Loss: 0.0076, Train: 100.00%, Valid: 70.80% Test: 70.90%\n",
      "Run: 05, Epoch: 67, Loss: 0.0059, Train: 100.00%, Valid: 71.00% Test: 70.90%\n",
      "Run: 05, Epoch: 68, Loss: 0.0046, Train: 100.00%, Valid: 70.80% Test: 70.80%\n",
      "Run: 05, Epoch: 69, Loss: 0.0020, Train: 100.00%, Valid: 70.80% Test: 70.80%\n",
      "Run: 05, Epoch: 70, Loss: 0.0027, Train: 100.00%, Valid: 70.60% Test: 70.80%\n",
      "Run: 05, Epoch: 71, Loss: 0.0050, Train: 100.00%, Valid: 70.60% Test: 70.70%\n",
      "Run: 05, Epoch: 72, Loss: 0.0018, Train: 100.00%, Valid: 70.60% Test: 70.70%\n",
      "Run: 05, Epoch: 73, Loss: 0.0020, Train: 100.00%, Valid: 71.00% Test: 70.80%\n",
      "Run: 05, Epoch: 74, Loss: 0.0173, Train: 100.00%, Valid: 70.60% Test: 70.60%\n",
      "Run: 05, Epoch: 75, Loss: 0.0035, Train: 100.00%, Valid: 70.40% Test: 70.60%\n",
      "Run: 05, Epoch: 76, Loss: 0.0019, Train: 100.00%, Valid: 70.40% Test: 70.60%\n",
      "Run: 05, Epoch: 77, Loss: 0.0038, Train: 100.00%, Valid: 70.60% Test: 70.50%\n",
      "Run: 05, Epoch: 78, Loss: 0.0039, Train: 100.00%, Valid: 70.80% Test: 70.70%\n",
      "Run: 05, Epoch: 79, Loss: 0.0039, Train: 100.00%, Valid: 70.80% Test: 70.90%\n",
      "Run: 05, Epoch: 80, Loss: 0.0042, Train: 100.00%, Valid: 70.80% Test: 71.00%\n",
      "Run: 05, Epoch: 81, Loss: 0.0026, Train: 100.00%, Valid: 70.80% Test: 71.00%\n",
      "Run: 05, Epoch: 82, Loss: 0.0008, Train: 100.00%, Valid: 70.80% Test: 70.90%\n",
      "Run: 05, Epoch: 83, Loss: 0.0130, Train: 100.00%, Valid: 71.00% Test: 71.40%\n",
      "Run: 05, Epoch: 84, Loss: 0.0050, Train: 100.00%, Valid: 71.00% Test: 71.50%\n",
      "Run: 05, Epoch: 85, Loss: 0.0058, Train: 100.00%, Valid: 71.00% Test: 71.50%\n",
      "Run: 05, Epoch: 86, Loss: 0.0007, Train: 100.00%, Valid: 71.20% Test: 71.20%\n",
      "Run: 05, Epoch: 87, Loss: 0.0012, Train: 100.00%, Valid: 71.20% Test: 71.40%\n",
      "Run: 05, Epoch: 88, Loss: 0.0040, Train: 100.00%, Valid: 71.20% Test: 71.40%\n",
      "Run: 05, Epoch: 89, Loss: 0.0026, Train: 100.00%, Valid: 71.20% Test: 71.40%\n",
      "Run: 05, Epoch: 90, Loss: 0.0003, Train: 100.00%, Valid: 70.80% Test: 71.50%\n",
      "Run: 05, Epoch: 91, Loss: 0.0138, Train: 100.00%, Valid: 70.80% Test: 71.50%\n",
      "Run: 05, Epoch: 92, Loss: 0.0005, Train: 100.00%, Valid: 70.40% Test: 71.60%\n",
      "Run: 05, Epoch: 93, Loss: 0.0028, Train: 100.00%, Valid: 70.40% Test: 71.60%\n",
      "Run: 05, Epoch: 94, Loss: 0.0016, Train: 100.00%, Valid: 70.60% Test: 71.20%\n",
      "Run: 05, Epoch: 95, Loss: 0.0014, Train: 100.00%, Valid: 70.60% Test: 71.20%\n",
      "Run: 05, Epoch: 96, Loss: 0.0019, Train: 100.00%, Valid: 70.80% Test: 71.20%\n",
      "Run: 05, Epoch: 97, Loss: 0.0031, Train: 100.00%, Valid: 70.80% Test: 71.20%\n",
      "Run: 05, Epoch: 98, Loss: 0.0013, Train: 100.00%, Valid: 70.80% Test: 71.20%\n",
      "Run: 05, Epoch: 99, Loss: 0.0095, Train: 100.00%, Valid: 70.80% Test: 71.10%\n",
      "Run: 05, Epoch: 100, Loss: 0.0046, Train: 100.00%, Valid: 70.80% Test: 71.10%\n",
      "Run 05:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 72.40\n",
      "  Final Train: 100.00\n",
      "   Final Test: 71.20\n",
      "Run: 06, Epoch: 01, Loss: 1.1184, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 06, Epoch: 02, Loss: 0.7117, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 06, Epoch: 03, Loss: 0.4530, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 06, Epoch: 04, Loss: 0.3900, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 06, Epoch: 05, Loss: 0.2984, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 06, Epoch: 06, Loss: 0.2044, Train: 36.67%, Valid: 19.60% Test: 18.00%\n",
      "Run: 06, Epoch: 07, Loss: 0.1619, Train: 50.00%, Valid: 19.80% Test: 18.00%\n",
      "Run: 06, Epoch: 08, Loss: 0.1174, Train: 61.67%, Valid: 20.60% Test: 19.20%\n",
      "Run: 06, Epoch: 09, Loss: 0.1572, Train: 81.67%, Valid: 23.00% Test: 20.80%\n",
      "Run: 06, Epoch: 10, Loss: 0.1083, Train: 91.67%, Valid: 26.60% Test: 23.60%\n",
      "Run: 06, Epoch: 11, Loss: 0.0871, Train: 95.00%, Valid: 32.00% Test: 27.80%\n",
      "Run: 06, Epoch: 12, Loss: 0.0535, Train: 96.67%, Valid: 35.60% Test: 31.20%\n",
      "Run: 06, Epoch: 13, Loss: 0.0526, Train: 96.67%, Valid: 39.20% Test: 35.60%\n",
      "Run: 06, Epoch: 14, Loss: 0.0637, Train: 100.00%, Valid: 42.20% Test: 38.80%\n",
      "Run: 06, Epoch: 15, Loss: 0.0426, Train: 100.00%, Valid: 44.80% Test: 42.50%\n",
      "Run: 06, Epoch: 16, Loss: 0.0265, Train: 100.00%, Valid: 45.80% Test: 45.80%\n",
      "Run: 06, Epoch: 17, Loss: 0.0282, Train: 100.00%, Valid: 48.00% Test: 48.80%\n",
      "Run: 06, Epoch: 18, Loss: 0.0465, Train: 100.00%, Valid: 50.40% Test: 51.30%\n",
      "Run: 06, Epoch: 19, Loss: 0.0287, Train: 100.00%, Valid: 52.60% Test: 52.70%\n",
      "Run: 06, Epoch: 20, Loss: 0.0287, Train: 100.00%, Valid: 54.20% Test: 54.20%\n",
      "Run: 06, Epoch: 21, Loss: 0.0205, Train: 100.00%, Valid: 55.20% Test: 56.70%\n",
      "Run: 06, Epoch: 22, Loss: 0.0134, Train: 100.00%, Valid: 57.20% Test: 57.40%\n",
      "Run: 06, Epoch: 23, Loss: 0.0319, Train: 100.00%, Valid: 58.60% Test: 57.50%\n",
      "Run: 06, Epoch: 24, Loss: 0.0193, Train: 100.00%, Valid: 59.60% Test: 58.60%\n",
      "Run: 06, Epoch: 25, Loss: 0.0408, Train: 100.00%, Valid: 60.80% Test: 59.40%\n",
      "Run: 06, Epoch: 26, Loss: 0.0120, Train: 100.00%, Valid: 61.40% Test: 59.50%\n",
      "Run: 06, Epoch: 27, Loss: 0.0060, Train: 100.00%, Valid: 61.60% Test: 59.60%\n",
      "Run: 06, Epoch: 28, Loss: 0.0278, Train: 100.00%, Valid: 62.00% Test: 60.20%\n",
      "Run: 06, Epoch: 29, Loss: 0.0100, Train: 100.00%, Valid: 62.40% Test: 61.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 06, Epoch: 30, Loss: 0.0493, Train: 100.00%, Valid: 63.00% Test: 61.40%\n",
      "Run: 06, Epoch: 31, Loss: 0.0169, Train: 100.00%, Valid: 62.80% Test: 61.80%\n",
      "Run: 06, Epoch: 32, Loss: 0.0224, Train: 100.00%, Valid: 62.80% Test: 62.30%\n",
      "Run: 06, Epoch: 33, Loss: 0.0057, Train: 100.00%, Valid: 63.60% Test: 62.90%\n",
      "Run: 06, Epoch: 34, Loss: 0.0261, Train: 100.00%, Valid: 64.20% Test: 63.20%\n",
      "Run: 06, Epoch: 35, Loss: 0.0184, Train: 100.00%, Valid: 64.60% Test: 63.50%\n",
      "Run: 06, Epoch: 36, Loss: 0.0173, Train: 100.00%, Valid: 64.40% Test: 63.80%\n",
      "Run: 06, Epoch: 37, Loss: 0.0064, Train: 100.00%, Valid: 64.60% Test: 64.00%\n",
      "Run: 06, Epoch: 38, Loss: 0.0054, Train: 100.00%, Valid: 66.00% Test: 64.80%\n",
      "Run: 06, Epoch: 39, Loss: 0.0044, Train: 100.00%, Valid: 66.40% Test: 65.30%\n",
      "Run: 06, Epoch: 40, Loss: 0.0135, Train: 100.00%, Valid: 66.60% Test: 65.90%\n",
      "Run: 06, Epoch: 41, Loss: 0.0033, Train: 100.00%, Valid: 67.00% Test: 65.90%\n",
      "Run: 06, Epoch: 42, Loss: 0.0299, Train: 100.00%, Valid: 67.60% Test: 66.00%\n",
      "Run: 06, Epoch: 43, Loss: 0.0040, Train: 100.00%, Valid: 67.60% Test: 66.30%\n",
      "Run: 06, Epoch: 44, Loss: 0.0060, Train: 100.00%, Valid: 67.80% Test: 66.40%\n",
      "Run: 06, Epoch: 45, Loss: 0.0311, Train: 100.00%, Valid: 67.40% Test: 66.40%\n",
      "Run: 06, Epoch: 46, Loss: 0.0059, Train: 100.00%, Valid: 68.00% Test: 66.70%\n",
      "Run: 06, Epoch: 47, Loss: 0.0016, Train: 100.00%, Valid: 68.40% Test: 66.70%\n",
      "Run: 06, Epoch: 48, Loss: 0.0060, Train: 100.00%, Valid: 68.40% Test: 66.70%\n",
      "Run: 06, Epoch: 49, Loss: 0.0132, Train: 100.00%, Valid: 68.80% Test: 67.00%\n",
      "Run: 06, Epoch: 50, Loss: 0.0052, Train: 100.00%, Valid: 68.80% Test: 67.40%\n",
      "Run: 06, Epoch: 51, Loss: 0.0045, Train: 100.00%, Valid: 68.60% Test: 67.70%\n",
      "Run: 06, Epoch: 52, Loss: 0.0185, Train: 100.00%, Valid: 68.40% Test: 67.50%\n",
      "Run: 06, Epoch: 53, Loss: 0.0203, Train: 100.00%, Valid: 68.60% Test: 67.50%\n",
      "Run: 06, Epoch: 54, Loss: 0.0497, Train: 100.00%, Valid: 68.40% Test: 67.80%\n",
      "Run: 06, Epoch: 55, Loss: 0.0111, Train: 100.00%, Valid: 68.40% Test: 67.90%\n",
      "Run: 06, Epoch: 56, Loss: 0.0024, Train: 100.00%, Valid: 68.60% Test: 68.00%\n",
      "Run: 06, Epoch: 57, Loss: 0.0014, Train: 100.00%, Valid: 69.00% Test: 68.30%\n",
      "Run: 06, Epoch: 58, Loss: 0.0030, Train: 100.00%, Valid: 69.60% Test: 68.40%\n",
      "Run: 06, Epoch: 59, Loss: 0.0122, Train: 100.00%, Valid: 69.80% Test: 68.50%\n",
      "Run: 06, Epoch: 60, Loss: 0.0061, Train: 100.00%, Valid: 70.00% Test: 68.60%\n",
      "Run: 06, Epoch: 61, Loss: 0.0014, Train: 100.00%, Valid: 70.00% Test: 68.70%\n",
      "Run: 06, Epoch: 62, Loss: 0.0022, Train: 100.00%, Valid: 70.00% Test: 68.60%\n",
      "Run: 06, Epoch: 63, Loss: 0.0044, Train: 100.00%, Valid: 70.40% Test: 68.80%\n",
      "Run: 06, Epoch: 64, Loss: 0.0011, Train: 100.00%, Valid: 70.60% Test: 68.90%\n",
      "Run: 06, Epoch: 65, Loss: 0.0190, Train: 100.00%, Valid: 70.40% Test: 69.20%\n",
      "Run: 06, Epoch: 66, Loss: 0.0013, Train: 100.00%, Valid: 70.40% Test: 69.10%\n",
      "Run: 06, Epoch: 67, Loss: 0.0019, Train: 100.00%, Valid: 70.60% Test: 69.20%\n",
      "Run: 06, Epoch: 68, Loss: 0.0224, Train: 100.00%, Valid: 70.80% Test: 69.30%\n",
      "Run: 06, Epoch: 69, Loss: 0.0419, Train: 100.00%, Valid: 70.80% Test: 69.20%\n",
      "Run: 06, Epoch: 70, Loss: 0.0362, Train: 100.00%, Valid: 70.60% Test: 69.40%\n",
      "Run: 06, Epoch: 71, Loss: 0.0009, Train: 100.00%, Valid: 70.60% Test: 69.50%\n",
      "Run: 06, Epoch: 72, Loss: 0.0008, Train: 100.00%, Valid: 70.60% Test: 69.70%\n",
      "Run: 06, Epoch: 73, Loss: 0.0049, Train: 100.00%, Valid: 70.60% Test: 70.00%\n",
      "Run: 06, Epoch: 74, Loss: 0.0231, Train: 100.00%, Valid: 71.00% Test: 70.00%\n",
      "Run: 06, Epoch: 75, Loss: 0.0240, Train: 100.00%, Valid: 71.00% Test: 69.90%\n",
      "Run: 06, Epoch: 76, Loss: 0.0182, Train: 100.00%, Valid: 71.20% Test: 69.90%\n",
      "Run: 06, Epoch: 77, Loss: 0.0091, Train: 100.00%, Valid: 71.20% Test: 69.80%\n",
      "Run: 06, Epoch: 78, Loss: 0.0149, Train: 100.00%, Valid: 70.80% Test: 69.80%\n",
      "Run: 06, Epoch: 79, Loss: 0.0009, Train: 100.00%, Valid: 70.80% Test: 69.80%\n",
      "Run: 06, Epoch: 80, Loss: 0.0009, Train: 100.00%, Valid: 70.60% Test: 69.80%\n",
      "Run: 06, Epoch: 81, Loss: 0.0033, Train: 100.00%, Valid: 70.60% Test: 69.70%\n",
      "Run: 06, Epoch: 82, Loss: 0.0032, Train: 100.00%, Valid: 70.60% Test: 69.70%\n",
      "Run: 06, Epoch: 83, Loss: 0.0081, Train: 100.00%, Valid: 70.40% Test: 69.50%\n",
      "Run: 06, Epoch: 84, Loss: 0.0010, Train: 100.00%, Valid: 70.00% Test: 69.50%\n",
      "Run: 06, Epoch: 85, Loss: 0.0061, Train: 100.00%, Valid: 70.20% Test: 69.30%\n",
      "Run: 06, Epoch: 86, Loss: 0.0088, Train: 100.00%, Valid: 70.00% Test: 69.10%\n",
      "Run: 06, Epoch: 87, Loss: 0.0013, Train: 100.00%, Valid: 70.80% Test: 69.00%\n",
      "Run: 06, Epoch: 88, Loss: 0.0071, Train: 100.00%, Valid: 70.80% Test: 68.90%\n",
      "Run: 06, Epoch: 89, Loss: 0.0041, Train: 100.00%, Valid: 71.20% Test: 68.70%\n",
      "Run: 06, Epoch: 90, Loss: 0.0020, Train: 100.00%, Valid: 71.20% Test: 68.60%\n",
      "Run: 06, Epoch: 91, Loss: 0.0004, Train: 100.00%, Valid: 71.20% Test: 68.50%\n",
      "Run: 06, Epoch: 92, Loss: 0.0013, Train: 100.00%, Valid: 71.20% Test: 68.50%\n",
      "Run: 06, Epoch: 93, Loss: 0.0019, Train: 100.00%, Valid: 71.00% Test: 68.30%\n",
      "Run: 06, Epoch: 94, Loss: 0.0013, Train: 100.00%, Valid: 70.80% Test: 68.10%\n",
      "Run: 06, Epoch: 95, Loss: 0.0065, Train: 100.00%, Valid: 70.80% Test: 68.10%\n",
      "Run: 06, Epoch: 96, Loss: 0.0028, Train: 100.00%, Valid: 70.60% Test: 68.10%\n",
      "Run: 06, Epoch: 97, Loss: 0.0021, Train: 100.00%, Valid: 70.60% Test: 68.10%\n",
      "Run: 06, Epoch: 98, Loss: 0.0035, Train: 100.00%, Valid: 70.60% Test: 68.10%\n",
      "Run: 06, Epoch: 99, Loss: 0.0018, Train: 100.00%, Valid: 70.60% Test: 68.00%\n",
      "Run: 06, Epoch: 100, Loss: 0.0058, Train: 100.00%, Valid: 70.60% Test: 68.00%\n",
      "Run 06:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 71.20\n",
      "  Final Train: 100.00\n",
      "   Final Test: 69.90\n",
      "Run: 07, Epoch: 01, Loss: 1.2096, Train: 51.67%, Valid: 29.40% Test: 29.70%\n",
      "Run: 07, Epoch: 02, Loss: 0.5907, Train: 65.00%, Valid: 54.00% Test: 51.00%\n",
      "Run: 07, Epoch: 03, Loss: 0.4189, Train: 65.00%, Valid: 51.80% Test: 48.90%\n",
      "Run: 07, Epoch: 04, Loss: 0.2441, Train: 63.33%, Valid: 49.20% Test: 47.70%\n",
      "Run: 07, Epoch: 05, Loss: 0.2219, Train: 65.00%, Valid: 48.60% Test: 48.00%\n",
      "Run: 07, Epoch: 06, Loss: 0.2113, Train: 73.33%, Valid: 50.40% Test: 48.80%\n",
      "Run: 07, Epoch: 07, Loss: 0.1655, Train: 81.67%, Valid: 51.80% Test: 50.60%\n",
      "Run: 07, Epoch: 08, Loss: 0.1596, Train: 95.00%, Valid: 53.80% Test: 51.50%\n",
      "Run: 07, Epoch: 09, Loss: 0.1011, Train: 96.67%, Valid: 56.40% Test: 53.00%\n",
      "Run: 07, Epoch: 10, Loss: 0.0833, Train: 98.33%, Valid: 59.20% Test: 56.00%\n",
      "Run: 07, Epoch: 11, Loss: 0.1195, Train: 98.33%, Valid: 61.60% Test: 59.10%\n",
      "Run: 07, Epoch: 12, Loss: 0.0754, Train: 100.00%, Valid: 63.00% Test: 61.00%\n",
      "Run: 07, Epoch: 13, Loss: 0.0640, Train: 100.00%, Valid: 64.60% Test: 62.10%\n",
      "Run: 07, Epoch: 14, Loss: 0.0808, Train: 100.00%, Valid: 65.40% Test: 62.90%\n",
      "Run: 07, Epoch: 15, Loss: 0.1007, Train: 100.00%, Valid: 66.80% Test: 63.70%\n",
      "Run: 07, Epoch: 16, Loss: 0.0616, Train: 100.00%, Valid: 67.60% Test: 64.10%\n",
      "Run: 07, Epoch: 17, Loss: 0.0358, Train: 100.00%, Valid: 68.60% Test: 64.90%\n",
      "Run: 07, Epoch: 18, Loss: 0.0512, Train: 100.00%, Valid: 68.60% Test: 66.20%\n",
      "Run: 07, Epoch: 19, Loss: 0.0532, Train: 100.00%, Valid: 69.20% Test: 67.20%\n",
      "Run: 07, Epoch: 20, Loss: 0.0216, Train: 100.00%, Valid: 70.00% Test: 68.40%\n",
      "Run: 07, Epoch: 21, Loss: 0.0188, Train: 100.00%, Valid: 70.60% Test: 68.90%\n",
      "Run: 07, Epoch: 22, Loss: 0.0280, Train: 100.00%, Valid: 71.60% Test: 69.20%\n",
      "Run: 07, Epoch: 23, Loss: 0.0096, Train: 100.00%, Valid: 71.60% Test: 69.50%\n",
      "Run: 07, Epoch: 24, Loss: 0.0260, Train: 100.00%, Valid: 71.60% Test: 70.00%\n",
      "Run: 07, Epoch: 25, Loss: 0.0077, Train: 100.00%, Valid: 72.40% Test: 70.20%\n",
      "Run: 07, Epoch: 26, Loss: 0.0508, Train: 100.00%, Valid: 72.40% Test: 71.00%\n",
      "Run: 07, Epoch: 27, Loss: 0.0085, Train: 100.00%, Valid: 72.40% Test: 71.00%\n",
      "Run: 07, Epoch: 28, Loss: 0.0317, Train: 100.00%, Valid: 72.80% Test: 71.20%\n",
      "Run: 07, Epoch: 29, Loss: 0.0064, Train: 100.00%, Valid: 73.20% Test: 71.00%\n",
      "Run: 07, Epoch: 30, Loss: 0.0126, Train: 100.00%, Valid: 73.60% Test: 70.90%\n",
      "Run: 07, Epoch: 31, Loss: 0.0191, Train: 100.00%, Valid: 74.00% Test: 71.20%\n",
      "Run: 07, Epoch: 32, Loss: 0.0127, Train: 100.00%, Valid: 74.40% Test: 71.70%\n",
      "Run: 07, Epoch: 33, Loss: 0.0192, Train: 100.00%, Valid: 74.00% Test: 71.90%\n",
      "Run: 07, Epoch: 34, Loss: 0.0039, Train: 100.00%, Valid: 74.00% Test: 72.10%\n",
      "Run: 07, Epoch: 35, Loss: 0.0211, Train: 100.00%, Valid: 74.20% Test: 71.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 07, Epoch: 36, Loss: 0.0110, Train: 100.00%, Valid: 74.80% Test: 71.80%\n",
      "Run: 07, Epoch: 37, Loss: 0.0213, Train: 100.00%, Valid: 74.40% Test: 71.90%\n",
      "Run: 07, Epoch: 38, Loss: 0.0275, Train: 100.00%, Valid: 74.20% Test: 71.90%\n",
      "Run: 07, Epoch: 39, Loss: 0.0110, Train: 100.00%, Valid: 74.60% Test: 71.80%\n",
      "Run: 07, Epoch: 40, Loss: 0.0345, Train: 100.00%, Valid: 74.60% Test: 71.50%\n",
      "Run: 07, Epoch: 41, Loss: 0.0029, Train: 100.00%, Valid: 74.40% Test: 71.60%\n",
      "Run: 07, Epoch: 42, Loss: 0.0065, Train: 100.00%, Valid: 74.60% Test: 71.50%\n",
      "Run: 07, Epoch: 43, Loss: 0.0012, Train: 100.00%, Valid: 74.40% Test: 71.30%\n",
      "Run: 07, Epoch: 44, Loss: 0.0057, Train: 100.00%, Valid: 74.20% Test: 71.40%\n",
      "Run: 07, Epoch: 45, Loss: 0.0015, Train: 100.00%, Valid: 73.80% Test: 71.00%\n",
      "Run: 07, Epoch: 46, Loss: 0.0218, Train: 100.00%, Valid: 74.00% Test: 70.90%\n",
      "Run: 07, Epoch: 47, Loss: 0.0022, Train: 100.00%, Valid: 74.00% Test: 70.80%\n",
      "Run: 07, Epoch: 48, Loss: 0.0093, Train: 100.00%, Valid: 74.00% Test: 70.90%\n",
      "Run: 07, Epoch: 49, Loss: 0.0007, Train: 100.00%, Valid: 74.00% Test: 70.90%\n",
      "Run: 07, Epoch: 50, Loss: 0.0020, Train: 100.00%, Valid: 73.60% Test: 70.80%\n",
      "Run: 07, Epoch: 51, Loss: 0.0013, Train: 100.00%, Valid: 74.00% Test: 70.70%\n",
      "Run: 07, Epoch: 52, Loss: 0.0032, Train: 100.00%, Valid: 74.00% Test: 70.70%\n",
      "Run: 07, Epoch: 53, Loss: 0.0028, Train: 100.00%, Valid: 74.00% Test: 70.70%\n",
      "Run: 07, Epoch: 54, Loss: 0.0045, Train: 100.00%, Valid: 73.80% Test: 70.70%\n",
      "Run: 07, Epoch: 55, Loss: 0.0069, Train: 100.00%, Valid: 74.00% Test: 70.60%\n",
      "Run: 07, Epoch: 56, Loss: 0.0017, Train: 100.00%, Valid: 74.00% Test: 70.70%\n",
      "Run: 07, Epoch: 57, Loss: 0.0157, Train: 100.00%, Valid: 74.00% Test: 70.90%\n",
      "Run: 07, Epoch: 58, Loss: 0.0168, Train: 100.00%, Valid: 73.80% Test: 70.70%\n",
      "Run: 07, Epoch: 59, Loss: 0.0085, Train: 100.00%, Valid: 73.40% Test: 70.70%\n",
      "Run: 07, Epoch: 60, Loss: 0.0098, Train: 100.00%, Valid: 73.20% Test: 70.60%\n",
      "Run: 07, Epoch: 61, Loss: 0.0020, Train: 100.00%, Valid: 73.20% Test: 70.60%\n",
      "Run: 07, Epoch: 62, Loss: 0.0020, Train: 100.00%, Valid: 73.20% Test: 70.50%\n",
      "Run: 07, Epoch: 63, Loss: 0.0027, Train: 100.00%, Valid: 73.20% Test: 70.30%\n",
      "Run: 07, Epoch: 64, Loss: 0.0594, Train: 100.00%, Valid: 73.40% Test: 70.20%\n",
      "Run: 07, Epoch: 65, Loss: 0.0010, Train: 100.00%, Valid: 73.80% Test: 69.90%\n",
      "Run: 07, Epoch: 66, Loss: 0.0069, Train: 100.00%, Valid: 73.60% Test: 70.00%\n",
      "Run: 07, Epoch: 67, Loss: 0.0028, Train: 100.00%, Valid: 73.60% Test: 70.00%\n",
      "Run: 07, Epoch: 68, Loss: 0.0142, Train: 100.00%, Valid: 73.60% Test: 70.00%\n",
      "Run: 07, Epoch: 69, Loss: 0.0016, Train: 100.00%, Valid: 73.60% Test: 69.90%\n",
      "Run: 07, Epoch: 70, Loss: 0.0038, Train: 100.00%, Valid: 73.80% Test: 69.90%\n",
      "Run: 07, Epoch: 71, Loss: 0.0010, Train: 100.00%, Valid: 73.80% Test: 69.90%\n",
      "Run: 07, Epoch: 72, Loss: 0.0081, Train: 100.00%, Valid: 73.40% Test: 69.80%\n",
      "Run: 07, Epoch: 73, Loss: 0.0014, Train: 100.00%, Valid: 73.40% Test: 69.80%\n",
      "Run: 07, Epoch: 74, Loss: 0.0087, Train: 100.00%, Valid: 73.00% Test: 69.70%\n",
      "Run: 07, Epoch: 75, Loss: 0.0074, Train: 100.00%, Valid: 73.00% Test: 69.70%\n",
      "Run: 07, Epoch: 76, Loss: 0.0009, Train: 100.00%, Valid: 73.00% Test: 69.90%\n",
      "Run: 07, Epoch: 77, Loss: 0.0010, Train: 100.00%, Valid: 73.00% Test: 69.90%\n",
      "Run: 07, Epoch: 78, Loss: 0.0049, Train: 100.00%, Valid: 73.20% Test: 69.90%\n",
      "Run: 07, Epoch: 79, Loss: 0.0013, Train: 100.00%, Valid: 73.20% Test: 69.70%\n",
      "Run: 07, Epoch: 80, Loss: 0.0094, Train: 100.00%, Valid: 73.20% Test: 69.70%\n",
      "Run: 07, Epoch: 81, Loss: 0.0195, Train: 100.00%, Valid: 73.20% Test: 69.60%\n",
      "Run: 07, Epoch: 82, Loss: 0.0018, Train: 100.00%, Valid: 73.20% Test: 69.70%\n",
      "Run: 07, Epoch: 83, Loss: 0.0145, Train: 100.00%, Valid: 73.20% Test: 69.90%\n",
      "Run: 07, Epoch: 84, Loss: 0.0018, Train: 100.00%, Valid: 73.60% Test: 69.90%\n",
      "Run: 07, Epoch: 85, Loss: 0.0007, Train: 100.00%, Valid: 73.60% Test: 69.90%\n",
      "Run: 07, Epoch: 86, Loss: 0.0027, Train: 100.00%, Valid: 73.40% Test: 69.90%\n",
      "Run: 07, Epoch: 87, Loss: 0.0044, Train: 100.00%, Valid: 73.40% Test: 69.70%\n",
      "Run: 07, Epoch: 88, Loss: 0.0012, Train: 100.00%, Valid: 73.40% Test: 69.90%\n",
      "Run: 07, Epoch: 89, Loss: 0.0033, Train: 100.00%, Valid: 73.40% Test: 69.80%\n",
      "Run: 07, Epoch: 90, Loss: 0.0011, Train: 100.00%, Valid: 73.40% Test: 69.90%\n",
      "Run: 07, Epoch: 91, Loss: 0.0041, Train: 100.00%, Valid: 73.60% Test: 69.70%\n",
      "Run: 07, Epoch: 92, Loss: 0.0007, Train: 100.00%, Valid: 73.60% Test: 69.60%\n",
      "Run: 07, Epoch: 93, Loss: 0.0033, Train: 100.00%, Valid: 73.60% Test: 69.50%\n",
      "Run: 07, Epoch: 94, Loss: 0.0002, Train: 100.00%, Valid: 73.60% Test: 69.50%\n",
      "Run: 07, Epoch: 95, Loss: 0.0027, Train: 100.00%, Valid: 73.60% Test: 69.60%\n",
      "Run: 07, Epoch: 96, Loss: 0.0031, Train: 100.00%, Valid: 73.60% Test: 69.60%\n",
      "Run: 07, Epoch: 97, Loss: 0.0136, Train: 100.00%, Valid: 73.60% Test: 69.70%\n",
      "Run: 07, Epoch: 98, Loss: 0.0022, Train: 100.00%, Valid: 73.20% Test: 69.60%\n",
      "Run: 07, Epoch: 99, Loss: 0.0069, Train: 100.00%, Valid: 72.80% Test: 69.70%\n",
      "Run: 07, Epoch: 100, Loss: 0.0002, Train: 100.00%, Valid: 72.60% Test: 69.50%\n",
      "Run 07:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 74.80\n",
      "  Final Train: 100.00\n",
      "   Final Test: 71.80\n",
      "Run: 08, Epoch: 01, Loss: 1.3219, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 08, Epoch: 02, Loss: 0.6375, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 08, Epoch: 03, Loss: 0.4840, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 08, Epoch: 04, Loss: 0.3426, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 08, Epoch: 05, Loss: 0.2922, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 08, Epoch: 06, Loss: 0.1985, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 08, Epoch: 07, Loss: 0.1626, Train: 36.67%, Valid: 19.60% Test: 18.00%\n",
      "Run: 08, Epoch: 08, Loss: 0.1458, Train: 41.67%, Valid: 19.80% Test: 18.00%\n",
      "Run: 08, Epoch: 09, Loss: 0.1465, Train: 61.67%, Valid: 20.80% Test: 18.30%\n",
      "Run: 08, Epoch: 10, Loss: 0.1131, Train: 66.67%, Valid: 21.80% Test: 19.80%\n",
      "Run: 08, Epoch: 11, Loss: 0.0820, Train: 83.33%, Valid: 25.40% Test: 23.50%\n",
      "Run: 08, Epoch: 12, Loss: 0.0708, Train: 90.00%, Valid: 31.00% Test: 27.70%\n",
      "Run: 08, Epoch: 13, Loss: 0.0482, Train: 95.00%, Valid: 35.80% Test: 32.70%\n",
      "Run: 08, Epoch: 14, Loss: 0.1076, Train: 98.33%, Valid: 38.20% Test: 38.40%\n",
      "Run: 08, Epoch: 15, Loss: 0.0471, Train: 98.33%, Valid: 41.40% Test: 43.80%\n",
      "Run: 08, Epoch: 16, Loss: 0.0362, Train: 98.33%, Valid: 44.00% Test: 47.60%\n",
      "Run: 08, Epoch: 17, Loss: 0.0416, Train: 98.33%, Valid: 47.60% Test: 50.00%\n",
      "Run: 08, Epoch: 18, Loss: 0.0456, Train: 100.00%, Valid: 49.80% Test: 52.80%\n",
      "Run: 08, Epoch: 19, Loss: 0.0153, Train: 100.00%, Valid: 52.60% Test: 54.60%\n",
      "Run: 08, Epoch: 20, Loss: 0.0838, Train: 100.00%, Valid: 55.60% Test: 57.20%\n",
      "Run: 08, Epoch: 21, Loss: 0.0278, Train: 100.00%, Valid: 57.80% Test: 59.20%\n",
      "Run: 08, Epoch: 22, Loss: 0.0296, Train: 100.00%, Valid: 59.60% Test: 60.80%\n",
      "Run: 08, Epoch: 23, Loss: 0.0227, Train: 100.00%, Valid: 60.60% Test: 61.60%\n",
      "Run: 08, Epoch: 24, Loss: 0.0133, Train: 100.00%, Valid: 61.20% Test: 63.20%\n",
      "Run: 08, Epoch: 25, Loss: 0.0365, Train: 100.00%, Valid: 61.60% Test: 64.90%\n",
      "Run: 08, Epoch: 26, Loss: 0.0232, Train: 100.00%, Valid: 63.60% Test: 65.70%\n",
      "Run: 08, Epoch: 27, Loss: 0.0298, Train: 100.00%, Valid: 64.20% Test: 66.80%\n",
      "Run: 08, Epoch: 28, Loss: 0.0431, Train: 100.00%, Valid: 64.80% Test: 67.00%\n",
      "Run: 08, Epoch: 29, Loss: 0.0322, Train: 100.00%, Valid: 64.80% Test: 67.00%\n",
      "Run: 08, Epoch: 30, Loss: 0.0381, Train: 100.00%, Valid: 65.00% Test: 67.60%\n",
      "Run: 08, Epoch: 31, Loss: 0.0064, Train: 100.00%, Valid: 64.80% Test: 67.90%\n",
      "Run: 08, Epoch: 32, Loss: 0.0275, Train: 100.00%, Valid: 65.20% Test: 68.30%\n",
      "Run: 08, Epoch: 33, Loss: 0.0097, Train: 100.00%, Valid: 66.00% Test: 68.40%\n",
      "Run: 08, Epoch: 34, Loss: 0.0050, Train: 100.00%, Valid: 66.20% Test: 68.50%\n",
      "Run: 08, Epoch: 35, Loss: 0.0115, Train: 100.00%, Valid: 66.60% Test: 68.80%\n",
      "Run: 08, Epoch: 36, Loss: 0.0236, Train: 100.00%, Valid: 66.60% Test: 68.90%\n",
      "Run: 08, Epoch: 37, Loss: 0.0090, Train: 100.00%, Valid: 67.40% Test: 68.90%\n",
      "Run: 08, Epoch: 38, Loss: 0.0159, Train: 100.00%, Valid: 68.00% Test: 68.90%\n",
      "Run: 08, Epoch: 39, Loss: 0.0022, Train: 100.00%, Valid: 68.60% Test: 69.10%\n",
      "Run: 08, Epoch: 40, Loss: 0.0094, Train: 100.00%, Valid: 68.80% Test: 69.30%\n",
      "Run: 08, Epoch: 41, Loss: 0.0223, Train: 100.00%, Valid: 69.00% Test: 69.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 08, Epoch: 42, Loss: 0.0024, Train: 100.00%, Valid: 69.00% Test: 69.60%\n",
      "Run: 08, Epoch: 43, Loss: 0.0388, Train: 100.00%, Valid: 69.00% Test: 69.80%\n",
      "Run: 08, Epoch: 44, Loss: 0.0067, Train: 100.00%, Valid: 69.20% Test: 69.90%\n",
      "Run: 08, Epoch: 45, Loss: 0.0108, Train: 100.00%, Valid: 69.60% Test: 70.00%\n",
      "Run: 08, Epoch: 46, Loss: 0.0219, Train: 100.00%, Valid: 69.60% Test: 70.10%\n",
      "Run: 08, Epoch: 47, Loss: 0.0219, Train: 100.00%, Valid: 69.60% Test: 70.00%\n",
      "Run: 08, Epoch: 48, Loss: 0.0018, Train: 100.00%, Valid: 69.60% Test: 70.00%\n",
      "Run: 08, Epoch: 49, Loss: 0.0064, Train: 100.00%, Valid: 69.80% Test: 69.90%\n",
      "Run: 08, Epoch: 50, Loss: 0.0093, Train: 100.00%, Valid: 70.20% Test: 70.40%\n",
      "Run: 08, Epoch: 51, Loss: 0.0135, Train: 100.00%, Valid: 70.00% Test: 70.60%\n",
      "Run: 08, Epoch: 52, Loss: 0.0060, Train: 100.00%, Valid: 69.80% Test: 70.70%\n",
      "Run: 08, Epoch: 53, Loss: 0.0030, Train: 100.00%, Valid: 69.80% Test: 71.00%\n",
      "Run: 08, Epoch: 54, Loss: 0.0022, Train: 100.00%, Valid: 70.20% Test: 71.20%\n",
      "Run: 08, Epoch: 55, Loss: 0.0173, Train: 100.00%, Valid: 70.40% Test: 71.20%\n",
      "Run: 08, Epoch: 56, Loss: 0.0015, Train: 100.00%, Valid: 70.60% Test: 71.20%\n",
      "Run: 08, Epoch: 57, Loss: 0.0130, Train: 100.00%, Valid: 70.60% Test: 70.90%\n",
      "Run: 08, Epoch: 58, Loss: 0.0166, Train: 100.00%, Valid: 70.60% Test: 70.80%\n",
      "Run: 08, Epoch: 59, Loss: 0.0038, Train: 100.00%, Valid: 70.00% Test: 70.70%\n",
      "Run: 08, Epoch: 60, Loss: 0.0026, Train: 100.00%, Valid: 70.20% Test: 70.70%\n",
      "Run: 08, Epoch: 61, Loss: 0.0035, Train: 100.00%, Valid: 70.00% Test: 70.80%\n",
      "Run: 08, Epoch: 62, Loss: 0.0010, Train: 100.00%, Valid: 70.00% Test: 70.80%\n",
      "Run: 08, Epoch: 63, Loss: 0.0008, Train: 100.00%, Valid: 70.00% Test: 70.80%\n",
      "Run: 08, Epoch: 64, Loss: 0.0194, Train: 100.00%, Valid: 70.20% Test: 71.00%\n",
      "Run: 08, Epoch: 65, Loss: 0.0137, Train: 100.00%, Valid: 70.00% Test: 70.90%\n",
      "Run: 08, Epoch: 66, Loss: 0.0147, Train: 100.00%, Valid: 70.00% Test: 71.00%\n",
      "Run: 08, Epoch: 67, Loss: 0.0021, Train: 100.00%, Valid: 70.00% Test: 71.00%\n",
      "Run: 08, Epoch: 68, Loss: 0.0074, Train: 100.00%, Valid: 70.00% Test: 70.90%\n",
      "Run: 08, Epoch: 69, Loss: 0.0021, Train: 100.00%, Valid: 70.00% Test: 70.90%\n",
      "Run: 08, Epoch: 70, Loss: 0.0090, Train: 100.00%, Valid: 70.00% Test: 71.00%\n",
      "Run: 08, Epoch: 71, Loss: 0.0078, Train: 100.00%, Valid: 70.00% Test: 70.90%\n",
      "Run: 08, Epoch: 72, Loss: 0.0090, Train: 100.00%, Valid: 70.00% Test: 71.00%\n",
      "Run: 08, Epoch: 73, Loss: 0.0011, Train: 100.00%, Valid: 70.00% Test: 71.20%\n",
      "Run: 08, Epoch: 74, Loss: 0.0015, Train: 100.00%, Valid: 70.00% Test: 71.30%\n",
      "Run: 08, Epoch: 75, Loss: 0.0030, Train: 100.00%, Valid: 69.80% Test: 71.20%\n",
      "Run: 08, Epoch: 76, Loss: 0.0013, Train: 100.00%, Valid: 69.80% Test: 71.10%\n",
      "Run: 08, Epoch: 77, Loss: 0.0068, Train: 100.00%, Valid: 70.00% Test: 71.20%\n",
      "Run: 08, Epoch: 78, Loss: 0.0019, Train: 100.00%, Valid: 70.00% Test: 71.20%\n",
      "Run: 08, Epoch: 79, Loss: 0.0045, Train: 100.00%, Valid: 70.00% Test: 71.40%\n",
      "Run: 08, Epoch: 80, Loss: 0.0017, Train: 100.00%, Valid: 69.80% Test: 71.40%\n",
      "Run: 08, Epoch: 81, Loss: 0.0008, Train: 100.00%, Valid: 69.80% Test: 71.50%\n",
      "Run: 08, Epoch: 82, Loss: 0.0046, Train: 100.00%, Valid: 69.80% Test: 71.70%\n",
      "Run: 08, Epoch: 83, Loss: 0.0011, Train: 100.00%, Valid: 70.00% Test: 71.70%\n",
      "Run: 08, Epoch: 84, Loss: 0.0031, Train: 100.00%, Valid: 70.00% Test: 71.60%\n",
      "Run: 08, Epoch: 85, Loss: 0.0109, Train: 100.00%, Valid: 70.00% Test: 71.70%\n",
      "Run: 08, Epoch: 86, Loss: 0.0048, Train: 100.00%, Valid: 70.00% Test: 71.70%\n",
      "Run: 08, Epoch: 87, Loss: 0.0006, Train: 100.00%, Valid: 69.80% Test: 71.70%\n",
      "Run: 08, Epoch: 88, Loss: 0.0004, Train: 100.00%, Valid: 69.80% Test: 71.70%\n",
      "Run: 08, Epoch: 89, Loss: 0.0046, Train: 100.00%, Valid: 69.80% Test: 71.70%\n",
      "Run: 08, Epoch: 90, Loss: 0.0005, Train: 100.00%, Valid: 69.80% Test: 71.80%\n",
      "Run: 08, Epoch: 91, Loss: 0.0047, Train: 100.00%, Valid: 69.80% Test: 71.80%\n",
      "Run: 08, Epoch: 92, Loss: 0.0035, Train: 100.00%, Valid: 70.00% Test: 71.60%\n",
      "Run: 08, Epoch: 93, Loss: 0.0025, Train: 100.00%, Valid: 70.00% Test: 71.60%\n",
      "Run: 08, Epoch: 94, Loss: 0.0003, Train: 100.00%, Valid: 70.00% Test: 71.60%\n",
      "Run: 08, Epoch: 95, Loss: 0.0008, Train: 100.00%, Valid: 70.00% Test: 71.50%\n",
      "Run: 08, Epoch: 96, Loss: 0.0008, Train: 100.00%, Valid: 70.00% Test: 71.20%\n",
      "Run: 08, Epoch: 97, Loss: 0.0008, Train: 100.00%, Valid: 70.00% Test: 71.20%\n",
      "Run: 08, Epoch: 98, Loss: 0.0026, Train: 100.00%, Valid: 70.20% Test: 71.20%\n",
      "Run: 08, Epoch: 99, Loss: 0.0009, Train: 100.00%, Valid: 70.20% Test: 71.20%\n",
      "Run: 08, Epoch: 100, Loss: 0.0140, Train: 100.00%, Valid: 70.20% Test: 71.20%\n",
      "Run 08:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 70.60\n",
      "  Final Train: 100.00\n",
      "   Final Test: 71.20\n",
      "Run: 09, Epoch: 01, Loss: 1.2773, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 09, Epoch: 02, Loss: 0.7461, Train: 33.33%, Valid: 41.60% Test: 40.70%\n",
      "Run: 09, Epoch: 03, Loss: 0.4853, Train: 43.33%, Valid: 41.80% Test: 41.10%\n",
      "Run: 09, Epoch: 04, Loss: 0.3732, Train: 56.67%, Valid: 43.80% Test: 42.90%\n",
      "Run: 09, Epoch: 05, Loss: 0.2816, Train: 70.00%, Valid: 47.60% Test: 47.30%\n",
      "Run: 09, Epoch: 06, Loss: 0.2252, Train: 88.33%, Valid: 54.00% Test: 52.10%\n",
      "Run: 09, Epoch: 07, Loss: 0.1596, Train: 98.33%, Valid: 58.60% Test: 58.60%\n",
      "Run: 09, Epoch: 08, Loss: 0.1360, Train: 98.33%, Valid: 63.00% Test: 62.70%\n",
      "Run: 09, Epoch: 09, Loss: 0.1255, Train: 98.33%, Valid: 66.40% Test: 65.80%\n",
      "Run: 09, Epoch: 10, Loss: 0.0825, Train: 100.00%, Valid: 70.80% Test: 67.20%\n",
      "Run: 09, Epoch: 11, Loss: 0.1181, Train: 100.00%, Valid: 71.60% Test: 67.50%\n",
      "Run: 09, Epoch: 12, Loss: 0.0718, Train: 100.00%, Valid: 71.40% Test: 69.20%\n",
      "Run: 09, Epoch: 13, Loss: 0.0575, Train: 100.00%, Valid: 71.60% Test: 69.80%\n",
      "Run: 09, Epoch: 14, Loss: 0.0353, Train: 100.00%, Valid: 71.40% Test: 70.30%\n",
      "Run: 09, Epoch: 15, Loss: 0.0294, Train: 100.00%, Valid: 71.40% Test: 70.30%\n",
      "Run: 09, Epoch: 16, Loss: 0.0389, Train: 100.00%, Valid: 72.40% Test: 70.10%\n",
      "Run: 09, Epoch: 17, Loss: 0.0261, Train: 100.00%, Valid: 73.00% Test: 70.00%\n",
      "Run: 09, Epoch: 18, Loss: 0.0204, Train: 100.00%, Valid: 73.20% Test: 70.30%\n",
      "Run: 09, Epoch: 19, Loss: 0.0178, Train: 100.00%, Valid: 73.00% Test: 70.50%\n",
      "Run: 09, Epoch: 20, Loss: 0.0413, Train: 100.00%, Valid: 73.80% Test: 70.60%\n",
      "Run: 09, Epoch: 21, Loss: 0.0265, Train: 100.00%, Valid: 74.60% Test: 70.60%\n",
      "Run: 09, Epoch: 22, Loss: 0.0260, Train: 100.00%, Valid: 74.60% Test: 70.60%\n",
      "Run: 09, Epoch: 23, Loss: 0.0133, Train: 100.00%, Valid: 74.40% Test: 70.80%\n",
      "Run: 09, Epoch: 24, Loss: 0.0163, Train: 100.00%, Valid: 74.00% Test: 70.70%\n",
      "Run: 09, Epoch: 25, Loss: 0.0138, Train: 100.00%, Valid: 73.60% Test: 70.90%\n",
      "Run: 09, Epoch: 26, Loss: 0.0138, Train: 100.00%, Valid: 73.20% Test: 71.10%\n",
      "Run: 09, Epoch: 27, Loss: 0.0508, Train: 100.00%, Valid: 73.00% Test: 71.10%\n",
      "Run: 09, Epoch: 28, Loss: 0.0085, Train: 100.00%, Valid: 72.80% Test: 71.20%\n",
      "Run: 09, Epoch: 29, Loss: 0.0416, Train: 100.00%, Valid: 72.80% Test: 71.10%\n",
      "Run: 09, Epoch: 30, Loss: 0.0069, Train: 100.00%, Valid: 73.20% Test: 71.20%\n",
      "Run: 09, Epoch: 31, Loss: 0.0168, Train: 100.00%, Valid: 73.20% Test: 71.10%\n",
      "Run: 09, Epoch: 32, Loss: 0.0199, Train: 100.00%, Valid: 73.00% Test: 70.90%\n",
      "Run: 09, Epoch: 33, Loss: 0.0051, Train: 100.00%, Valid: 73.20% Test: 70.70%\n",
      "Run: 09, Epoch: 34, Loss: 0.0055, Train: 100.00%, Valid: 73.00% Test: 70.80%\n",
      "Run: 09, Epoch: 35, Loss: 0.0348, Train: 100.00%, Valid: 73.00% Test: 70.60%\n",
      "Run: 09, Epoch: 36, Loss: 0.0054, Train: 100.00%, Valid: 73.00% Test: 70.30%\n",
      "Run: 09, Epoch: 37, Loss: 0.0176, Train: 100.00%, Valid: 73.20% Test: 70.30%\n",
      "Run: 09, Epoch: 38, Loss: 0.0083, Train: 100.00%, Valid: 72.80% Test: 70.20%\n",
      "Run: 09, Epoch: 39, Loss: 0.0091, Train: 100.00%, Valid: 73.40% Test: 70.20%\n",
      "Run: 09, Epoch: 40, Loss: 0.0087, Train: 100.00%, Valid: 73.40% Test: 70.10%\n",
      "Run: 09, Epoch: 41, Loss: 0.0128, Train: 100.00%, Valid: 73.40% Test: 70.30%\n",
      "Run: 09, Epoch: 42, Loss: 0.0364, Train: 100.00%, Valid: 73.40% Test: 70.30%\n",
      "Run: 09, Epoch: 43, Loss: 0.0038, Train: 100.00%, Valid: 73.00% Test: 70.40%\n",
      "Run: 09, Epoch: 44, Loss: 0.0170, Train: 100.00%, Valid: 73.20% Test: 70.60%\n",
      "Run: 09, Epoch: 45, Loss: 0.0138, Train: 100.00%, Valid: 73.00% Test: 70.90%\n",
      "Run: 09, Epoch: 46, Loss: 0.0019, Train: 100.00%, Valid: 73.20% Test: 71.00%\n",
      "Run: 09, Epoch: 47, Loss: 0.0148, Train: 100.00%, Valid: 73.20% Test: 71.10%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 09, Epoch: 48, Loss: 0.0018, Train: 100.00%, Valid: 73.00% Test: 71.30%\n",
      "Run: 09, Epoch: 49, Loss: 0.0300, Train: 100.00%, Valid: 73.20% Test: 71.30%\n",
      "Run: 09, Epoch: 50, Loss: 0.0031, Train: 100.00%, Valid: 73.40% Test: 71.40%\n",
      "Run: 09, Epoch: 51, Loss: 0.0094, Train: 100.00%, Valid: 73.40% Test: 71.40%\n",
      "Run: 09, Epoch: 52, Loss: 0.0039, Train: 100.00%, Valid: 73.40% Test: 71.30%\n",
      "Run: 09, Epoch: 53, Loss: 0.0066, Train: 100.00%, Valid: 73.40% Test: 71.30%\n",
      "Run: 09, Epoch: 54, Loss: 0.0022, Train: 100.00%, Valid: 73.60% Test: 71.50%\n",
      "Run: 09, Epoch: 55, Loss: 0.0049, Train: 100.00%, Valid: 73.60% Test: 71.40%\n",
      "Run: 09, Epoch: 56, Loss: 0.0086, Train: 100.00%, Valid: 73.60% Test: 71.50%\n",
      "Run: 09, Epoch: 57, Loss: 0.0054, Train: 100.00%, Valid: 73.60% Test: 71.40%\n",
      "Run: 09, Epoch: 58, Loss: 0.0086, Train: 100.00%, Valid: 73.60% Test: 71.30%\n",
      "Run: 09, Epoch: 59, Loss: 0.0100, Train: 100.00%, Valid: 73.40% Test: 71.30%\n",
      "Run: 09, Epoch: 60, Loss: 0.0209, Train: 100.00%, Valid: 73.40% Test: 71.30%\n",
      "Run: 09, Epoch: 61, Loss: 0.0142, Train: 100.00%, Valid: 73.40% Test: 71.10%\n",
      "Run: 09, Epoch: 62, Loss: 0.0007, Train: 100.00%, Valid: 73.40% Test: 71.10%\n",
      "Run: 09, Epoch: 63, Loss: 0.0120, Train: 100.00%, Valid: 73.60% Test: 71.30%\n",
      "Run: 09, Epoch: 64, Loss: 0.0032, Train: 100.00%, Valid: 73.60% Test: 71.30%\n",
      "Run: 09, Epoch: 65, Loss: 0.0036, Train: 100.00%, Valid: 73.60% Test: 71.30%\n",
      "Run: 09, Epoch: 66, Loss: 0.0021, Train: 100.00%, Valid: 73.60% Test: 71.40%\n",
      "Run: 09, Epoch: 67, Loss: 0.0036, Train: 100.00%, Valid: 73.80% Test: 71.40%\n",
      "Run: 09, Epoch: 68, Loss: 0.0088, Train: 100.00%, Valid: 73.80% Test: 71.30%\n",
      "Run: 09, Epoch: 69, Loss: 0.0086, Train: 100.00%, Valid: 73.80% Test: 71.30%\n",
      "Run: 09, Epoch: 70, Loss: 0.0025, Train: 100.00%, Valid: 74.00% Test: 71.30%\n",
      "Run: 09, Epoch: 71, Loss: 0.0013, Train: 100.00%, Valid: 74.00% Test: 71.30%\n",
      "Run: 09, Epoch: 72, Loss: 0.0039, Train: 100.00%, Valid: 74.00% Test: 71.40%\n",
      "Run: 09, Epoch: 73, Loss: 0.0006, Train: 100.00%, Valid: 73.60% Test: 71.40%\n",
      "Run: 09, Epoch: 74, Loss: 0.0113, Train: 100.00%, Valid: 73.60% Test: 71.00%\n",
      "Run: 09, Epoch: 75, Loss: 0.0020, Train: 100.00%, Valid: 73.20% Test: 71.00%\n",
      "Run: 09, Epoch: 76, Loss: 0.0132, Train: 100.00%, Valid: 73.20% Test: 71.00%\n",
      "Run: 09, Epoch: 77, Loss: 0.0013, Train: 100.00%, Valid: 73.20% Test: 70.90%\n",
      "Run: 09, Epoch: 78, Loss: 0.0015, Train: 100.00%, Valid: 73.00% Test: 70.80%\n",
      "Run: 09, Epoch: 79, Loss: 0.0016, Train: 100.00%, Valid: 73.20% Test: 70.80%\n",
      "Run: 09, Epoch: 80, Loss: 0.0222, Train: 100.00%, Valid: 73.20% Test: 70.90%\n",
      "Run: 09, Epoch: 81, Loss: 0.0098, Train: 100.00%, Valid: 73.20% Test: 70.90%\n",
      "Run: 09, Epoch: 82, Loss: 0.0071, Train: 100.00%, Valid: 73.20% Test: 70.80%\n",
      "Run: 09, Epoch: 83, Loss: 0.0002, Train: 100.00%, Valid: 73.40% Test: 70.80%\n",
      "Run: 09, Epoch: 84, Loss: 0.0021, Train: 100.00%, Valid: 73.40% Test: 70.90%\n",
      "Run: 09, Epoch: 85, Loss: 0.0029, Train: 100.00%, Valid: 73.40% Test: 70.80%\n",
      "Run: 09, Epoch: 86, Loss: 0.0040, Train: 100.00%, Valid: 73.40% Test: 70.90%\n",
      "Run: 09, Epoch: 87, Loss: 0.0006, Train: 100.00%, Valid: 73.40% Test: 71.00%\n",
      "Run: 09, Epoch: 88, Loss: 0.0014, Train: 100.00%, Valid: 73.40% Test: 71.10%\n",
      "Run: 09, Epoch: 89, Loss: 0.0006, Train: 100.00%, Valid: 73.80% Test: 70.90%\n",
      "Run: 09, Epoch: 90, Loss: 0.0002, Train: 100.00%, Valid: 73.80% Test: 71.00%\n",
      "Run: 09, Epoch: 91, Loss: 0.0032, Train: 100.00%, Valid: 73.80% Test: 71.00%\n",
      "Run: 09, Epoch: 92, Loss: 0.0015, Train: 100.00%, Valid: 73.80% Test: 71.00%\n",
      "Run: 09, Epoch: 93, Loss: 0.0024, Train: 100.00%, Valid: 73.80% Test: 71.00%\n",
      "Run: 09, Epoch: 94, Loss: 0.0087, Train: 100.00%, Valid: 73.80% Test: 70.90%\n",
      "Run: 09, Epoch: 95, Loss: 0.0120, Train: 100.00%, Valid: 74.00% Test: 70.80%\n",
      "Run: 09, Epoch: 96, Loss: 0.0048, Train: 100.00%, Valid: 74.00% Test: 70.80%\n",
      "Run: 09, Epoch: 97, Loss: 0.0005, Train: 100.00%, Valid: 74.00% Test: 70.70%\n",
      "Run: 09, Epoch: 98, Loss: 0.0010, Train: 100.00%, Valid: 74.00% Test: 70.60%\n",
      "Run: 09, Epoch: 99, Loss: 0.0025, Train: 100.00%, Valid: 74.00% Test: 70.60%\n",
      "Run: 09, Epoch: 100, Loss: 0.0034, Train: 100.00%, Valid: 73.80% Test: 70.60%\n",
      "Run 09:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 74.60\n",
      "  Final Train: 100.00\n",
      "   Final Test: 70.60\n",
      "Run: 10, Epoch: 01, Loss: 1.2964, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 10, Epoch: 02, Loss: 0.6431, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 10, Epoch: 03, Loss: 0.5260, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 10, Epoch: 04, Loss: 0.4123, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 10, Epoch: 05, Loss: 0.3048, Train: 33.33%, Valid: 19.60% Test: 18.00%\n",
      "Run: 10, Epoch: 06, Loss: 0.2456, Train: 35.00%, Valid: 19.60% Test: 18.00%\n",
      "Run: 10, Epoch: 07, Loss: 0.2201, Train: 36.67%, Valid: 19.60% Test: 18.00%\n",
      "Run: 10, Epoch: 08, Loss: 0.1560, Train: 41.67%, Valid: 19.60% Test: 18.00%\n",
      "Run: 10, Epoch: 09, Loss: 0.1551, Train: 46.67%, Valid: 20.20% Test: 18.30%\n",
      "Run: 10, Epoch: 10, Loss: 0.1126, Train: 61.67%, Valid: 20.60% Test: 18.70%\n",
      "Run: 10, Epoch: 11, Loss: 0.1022, Train: 81.67%, Valid: 21.20% Test: 19.30%\n",
      "Run: 10, Epoch: 12, Loss: 0.1041, Train: 85.00%, Valid: 23.00% Test: 20.80%\n",
      "Run: 10, Epoch: 13, Loss: 0.0756, Train: 93.33%, Valid: 25.60% Test: 24.20%\n",
      "Run: 10, Epoch: 14, Loss: 0.0842, Train: 98.33%, Valid: 29.20% Test: 27.20%\n",
      "Run: 10, Epoch: 15, Loss: 0.0812, Train: 98.33%, Valid: 31.40% Test: 29.30%\n",
      "Run: 10, Epoch: 16, Loss: 0.0299, Train: 98.33%, Valid: 34.40% Test: 31.20%\n",
      "Run: 10, Epoch: 17, Loss: 0.1066, Train: 100.00%, Valid: 37.20% Test: 34.00%\n",
      "Run: 10, Epoch: 18, Loss: 0.0395, Train: 100.00%, Valid: 39.40% Test: 37.20%\n",
      "Run: 10, Epoch: 19, Loss: 0.0269, Train: 100.00%, Valid: 42.00% Test: 39.80%\n",
      "Run: 10, Epoch: 20, Loss: 0.0456, Train: 100.00%, Valid: 44.20% Test: 42.20%\n",
      "Run: 10, Epoch: 21, Loss: 0.0509, Train: 100.00%, Valid: 46.20% Test: 43.30%\n",
      "Run: 10, Epoch: 22, Loss: 0.0539, Train: 100.00%, Valid: 47.80% Test: 46.10%\n",
      "Run: 10, Epoch: 23, Loss: 0.0287, Train: 100.00%, Valid: 50.00% Test: 48.40%\n",
      "Run: 10, Epoch: 24, Loss: 0.0236, Train: 100.00%, Valid: 52.00% Test: 50.60%\n",
      "Run: 10, Epoch: 25, Loss: 0.0185, Train: 100.00%, Valid: 54.40% Test: 52.90%\n",
      "Run: 10, Epoch: 26, Loss: 0.0474, Train: 100.00%, Valid: 55.40% Test: 55.10%\n",
      "Run: 10, Epoch: 27, Loss: 0.0187, Train: 100.00%, Valid: 56.60% Test: 56.20%\n",
      "Run: 10, Epoch: 28, Loss: 0.0192, Train: 100.00%, Valid: 58.00% Test: 57.50%\n",
      "Run: 10, Epoch: 29, Loss: 0.0355, Train: 100.00%, Valid: 59.40% Test: 58.40%\n",
      "Run: 10, Epoch: 30, Loss: 0.0275, Train: 100.00%, Valid: 60.60% Test: 59.60%\n",
      "Run: 10, Epoch: 31, Loss: 0.0255, Train: 100.00%, Valid: 61.00% Test: 60.10%\n",
      "Run: 10, Epoch: 32, Loss: 0.0558, Train: 100.00%, Valid: 61.60% Test: 60.60%\n",
      "Run: 10, Epoch: 33, Loss: 0.0142, Train: 100.00%, Valid: 61.80% Test: 60.80%\n",
      "Run: 10, Epoch: 34, Loss: 0.0431, Train: 100.00%, Valid: 62.20% Test: 61.40%\n",
      "Run: 10, Epoch: 35, Loss: 0.0196, Train: 100.00%, Valid: 62.40% Test: 62.00%\n",
      "Run: 10, Epoch: 36, Loss: 0.0288, Train: 100.00%, Valid: 63.20% Test: 62.20%\n",
      "Run: 10, Epoch: 37, Loss: 0.0202, Train: 100.00%, Valid: 63.80% Test: 63.20%\n",
      "Run: 10, Epoch: 38, Loss: 0.0070, Train: 100.00%, Valid: 64.40% Test: 63.40%\n",
      "Run: 10, Epoch: 39, Loss: 0.0257, Train: 100.00%, Valid: 64.20% Test: 64.00%\n",
      "Run: 10, Epoch: 40, Loss: 0.0365, Train: 100.00%, Valid: 64.80% Test: 64.10%\n",
      "Run: 10, Epoch: 41, Loss: 0.0199, Train: 100.00%, Valid: 64.80% Test: 64.30%\n",
      "Run: 10, Epoch: 42, Loss: 0.0049, Train: 100.00%, Valid: 65.20% Test: 64.60%\n",
      "Run: 10, Epoch: 43, Loss: 0.0049, Train: 100.00%, Valid: 65.20% Test: 64.80%\n",
      "Run: 10, Epoch: 44, Loss: 0.0064, Train: 100.00%, Valid: 65.40% Test: 64.90%\n",
      "Run: 10, Epoch: 45, Loss: 0.0115, Train: 100.00%, Valid: 66.00% Test: 65.20%\n",
      "Run: 10, Epoch: 46, Loss: 0.0133, Train: 100.00%, Valid: 66.00% Test: 66.10%\n",
      "Run: 10, Epoch: 47, Loss: 0.0704, Train: 100.00%, Valid: 66.60% Test: 66.20%\n",
      "Run: 10, Epoch: 48, Loss: 0.0728, Train: 100.00%, Valid: 66.80% Test: 66.50%\n",
      "Run: 10, Epoch: 49, Loss: 0.0113, Train: 100.00%, Valid: 67.40% Test: 66.90%\n",
      "Run: 10, Epoch: 50, Loss: 0.0079, Train: 100.00%, Valid: 67.60% Test: 67.20%\n",
      "Run: 10, Epoch: 51, Loss: 0.0124, Train: 100.00%, Valid: 67.60% Test: 67.30%\n",
      "Run: 10, Epoch: 52, Loss: 0.0067, Train: 100.00%, Valid: 67.60% Test: 67.40%\n",
      "Run: 10, Epoch: 53, Loss: 0.0131, Train: 100.00%, Valid: 68.20% Test: 67.30%\n",
      "Run: 10, Epoch: 54, Loss: 0.0080, Train: 100.00%, Valid: 68.40% Test: 67.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 10, Epoch: 55, Loss: 0.0057, Train: 100.00%, Valid: 68.60% Test: 67.50%\n",
      "Run: 10, Epoch: 56, Loss: 0.0145, Train: 100.00%, Valid: 69.00% Test: 67.40%\n",
      "Run: 10, Epoch: 57, Loss: 0.0018, Train: 100.00%, Valid: 69.00% Test: 67.20%\n",
      "Run: 10, Epoch: 58, Loss: 0.0185, Train: 100.00%, Valid: 69.40% Test: 67.40%\n",
      "Run: 10, Epoch: 59, Loss: 0.0466, Train: 100.00%, Valid: 69.20% Test: 67.20%\n",
      "Run: 10, Epoch: 60, Loss: 0.0043, Train: 100.00%, Valid: 69.00% Test: 67.10%\n",
      "Run: 10, Epoch: 61, Loss: 0.0020, Train: 100.00%, Valid: 69.00% Test: 67.20%\n",
      "Run: 10, Epoch: 62, Loss: 0.0105, Train: 100.00%, Valid: 69.00% Test: 67.10%\n",
      "Run: 10, Epoch: 63, Loss: 0.0113, Train: 100.00%, Valid: 69.00% Test: 67.30%\n",
      "Run: 10, Epoch: 64, Loss: 0.0022, Train: 100.00%, Valid: 68.80% Test: 67.40%\n",
      "Run: 10, Epoch: 65, Loss: 0.0319, Train: 100.00%, Valid: 68.80% Test: 67.30%\n",
      "Run: 10, Epoch: 66, Loss: 0.0473, Train: 100.00%, Valid: 69.20% Test: 67.20%\n",
      "Run: 10, Epoch: 67, Loss: 0.0031, Train: 100.00%, Valid: 69.00% Test: 67.10%\n",
      "Run: 10, Epoch: 68, Loss: 0.0257, Train: 100.00%, Valid: 68.60% Test: 67.00%\n",
      "Run: 10, Epoch: 69, Loss: 0.0056, Train: 100.00%, Valid: 68.40% Test: 66.70%\n",
      "Run: 10, Epoch: 70, Loss: 0.0377, Train: 100.00%, Valid: 68.20% Test: 66.50%\n",
      "Run: 10, Epoch: 71, Loss: 0.0250, Train: 100.00%, Valid: 68.40% Test: 66.60%\n",
      "Run: 10, Epoch: 72, Loss: 0.0175, Train: 100.00%, Valid: 68.40% Test: 66.60%\n",
      "Run: 10, Epoch: 73, Loss: 0.0034, Train: 100.00%, Valid: 68.60% Test: 66.80%\n",
      "Run: 10, Epoch: 74, Loss: 0.0021, Train: 100.00%, Valid: 68.60% Test: 67.10%\n",
      "Run: 10, Epoch: 75, Loss: 0.0043, Train: 100.00%, Valid: 68.60% Test: 67.20%\n",
      "Run: 10, Epoch: 76, Loss: 0.0010, Train: 100.00%, Valid: 68.40% Test: 67.20%\n",
      "Run: 10, Epoch: 77, Loss: 0.0058, Train: 100.00%, Valid: 68.40% Test: 67.30%\n",
      "Run: 10, Epoch: 78, Loss: 0.0022, Train: 100.00%, Valid: 68.60% Test: 67.40%\n",
      "Run: 10, Epoch: 79, Loss: 0.0136, Train: 100.00%, Valid: 68.60% Test: 67.70%\n",
      "Run: 10, Epoch: 80, Loss: 0.0059, Train: 100.00%, Valid: 68.80% Test: 67.60%\n",
      "Run: 10, Epoch: 81, Loss: 0.0025, Train: 100.00%, Valid: 69.20% Test: 67.60%\n",
      "Run: 10, Epoch: 82, Loss: 0.0023, Train: 100.00%, Valid: 69.20% Test: 67.60%\n",
      "Run: 10, Epoch: 83, Loss: 0.0194, Train: 100.00%, Valid: 69.40% Test: 67.50%\n",
      "Run: 10, Epoch: 84, Loss: 0.0254, Train: 100.00%, Valid: 69.20% Test: 67.50%\n",
      "Run: 10, Epoch: 85, Loss: 0.0010, Train: 100.00%, Valid: 69.00% Test: 67.40%\n",
      "Run: 10, Epoch: 86, Loss: 0.0007, Train: 100.00%, Valid: 69.20% Test: 67.40%\n",
      "Run: 10, Epoch: 87, Loss: 0.0023, Train: 100.00%, Valid: 69.20% Test: 67.60%\n",
      "Run: 10, Epoch: 88, Loss: 0.0082, Train: 100.00%, Valid: 69.20% Test: 67.40%\n",
      "Run: 10, Epoch: 89, Loss: 0.0007, Train: 100.00%, Valid: 69.20% Test: 67.30%\n",
      "Run: 10, Epoch: 90, Loss: 0.0132, Train: 100.00%, Valid: 69.40% Test: 67.00%\n",
      "Run: 10, Epoch: 91, Loss: 0.0008, Train: 100.00%, Valid: 69.20% Test: 67.10%\n",
      "Run: 10, Epoch: 92, Loss: 0.0019, Train: 100.00%, Valid: 69.40% Test: 67.20%\n",
      "Run: 10, Epoch: 93, Loss: 0.0017, Train: 100.00%, Valid: 69.40% Test: 67.10%\n",
      "Run: 10, Epoch: 94, Loss: 0.0071, Train: 100.00%, Valid: 69.40% Test: 67.20%\n",
      "Run: 10, Epoch: 95, Loss: 0.0016, Train: 100.00%, Valid: 69.40% Test: 67.20%\n",
      "Run: 10, Epoch: 96, Loss: 0.0029, Train: 100.00%, Valid: 69.40% Test: 67.20%\n",
      "Run: 10, Epoch: 97, Loss: 0.0047, Train: 100.00%, Valid: 69.20% Test: 67.20%\n",
      "Run: 10, Epoch: 98, Loss: 0.0043, Train: 100.00%, Valid: 69.20% Test: 67.30%\n",
      "Run: 10, Epoch: 99, Loss: 0.0041, Train: 100.00%, Valid: 69.00% Test: 67.30%\n",
      "Run: 10, Epoch: 100, Loss: 0.0109, Train: 100.00%, Valid: 69.00% Test: 67.10%\n",
      "Run 10:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 69.40\n",
      "  Final Train: 100.00\n",
      "   Final Test: 67.40\n",
      "All runs:\n",
      "Highest Train: 100.00 ± 0.00\n",
      "Highest Valid: 73.12 ± 2.10\n",
      "  Final Train: 100.00 ± 0.00\n",
      "   Final Test: 70.93 ± 1.49\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, \n",
    "         'batch_size': 32, 'hidden_channels': 16, 'dropout': 0.5, 'epochs': 100, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-4, 'lr': 0.01}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    dataset = Planetoid(root='/tmp/PubMed', name='PubMed',transform=T.ToSparseTensor())\n",
    "    data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    train_idx = np.where(data.train_mask)[0]\n",
    "    valid_idx = np.where(data.val_mask)[0]\n",
    "    test_idx = np.where(data.test_mask)[0]\n",
    "    \n",
    "    model = SAGE(data.num_features, args.hidden_channels,dataset.num_classes, args.num_layers,\n",
    "                    args.dropout,args.heads)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52f151",
   "metadata": {},
   "source": [
    "# WISE EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a09514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], adj_t=[19717, 19717, nnz=88648])\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96f82a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.104636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3         4    5    6         7    8    9    ...  490  \\\n",
       "0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.004999  0.0  0.0  ...  0.0   \n",
       "1  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.016434  0.0  0.0  ...  0.0   \n",
       "2  0.104636  0.0  0.0  0.0  0.035178  0.0  0.0  0.019555  0.0  0.0  ...  0.0   \n",
       "3  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.008582  0.0  0.0  ...  0.0   \n",
       "4  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.007356  0.0  0.0  ...  0.0   \n",
       "\n",
       "   491  492  493  494  495  496  497  498  499  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Domain_Fec=pd.DataFrame(data.x.numpy())\n",
    "#label=pd.DataFrame(data.y.numpy(),columns =['class'])\n",
    "#Data=pd.concat([Domain_Fec,label], axis=1)\n",
    "Domain_Fec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7241d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Domain_Fec[Domain_Fec != 0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cbce905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9  ...  491  492  493  494  \\\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2   1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "5   0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  1.0   \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "7   0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "9   0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "10  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "12  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "13  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "14  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "15  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "16  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "17  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "18  1.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "19  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    495  496  497  498  499  class  \n",
       "0   0.0  0.0  0.0  0.0  0.0      1  \n",
       "1   0.0  0.0  0.0  0.0  0.0      1  \n",
       "2   0.0  0.0  0.0  0.0  0.0      0  \n",
       "3   0.0  0.0  0.0  0.0  0.0      2  \n",
       "4   0.0  0.0  0.0  0.0  0.0      1  \n",
       "5   0.0  0.0  0.0  0.0  0.0      2  \n",
       "6   0.0  0.0  0.0  0.0  0.0      2  \n",
       "7   0.0  0.0  0.0  0.0  0.0      1  \n",
       "8   0.0  0.0  0.0  0.0  0.0      2  \n",
       "9   0.0  0.0  0.0  0.0  0.0      1  \n",
       "10  0.0  0.0  0.0  0.0  0.0      2  \n",
       "11  0.0  1.0  0.0  0.0  0.0      2  \n",
       "12  0.0  0.0  0.0  0.0  1.0      2  \n",
       "13  0.0  1.0  0.0  0.0  0.0      2  \n",
       "14  0.0  0.0  0.0  0.0  0.0      2  \n",
       "15  0.0  0.0  0.0  0.0  0.0      0  \n",
       "16  0.0  0.0  0.0  1.0  0.0      1  \n",
       "17  0.0  0.0  0.0  0.0  0.0      2  \n",
       "18  0.0  0.0  0.0  0.0  0.0      1  \n",
       "19  0.0  0.0  0.0  0.0  0.0      2  \n",
       "\n",
       "[20 rows x 501 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=pd.DataFrame(data.y.numpy(),columns =['class'])\n",
    "Data=pd.concat([Domain_Fec,label], axis=1)\n",
    "Data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c805a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_nodes=len(data.y)\n",
    "fe_len=len(data.x[0])\n",
    "catagories=Data['class'].to_numpy()\n",
    "data_by_class = {cls: Data.loc[Data['class'] == cls].drop(['class'], axis=1) for cls in range(max(catagories) + 1)}\n",
    "basis = [[max(df[i]) for i in range(len(df.columns))] for df in data_by_class.values()]\n",
    "sel_basis = [[int(list(df[i].to_numpy()).count(1) >= int(len(df[i].index)*0.1)) \n",
    "              for i in range(len(df.columns))]\n",
    "             for df in data_by_class.values()]\n",
    "feature_names = [ii for ii in range(fe_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6b8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e2cb0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.104636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3         4    5    6         7    8    9  ...  491  \\\n",
       "0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.004999  0.0  0.0  ...  0.0   \n",
       "1  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.016434  0.0  0.0  ...  0.0   \n",
       "2  0.104636  0.0  0.0  0.0  0.035178  0.0  0.0  0.019555  0.0  0.0  ...  0.0   \n",
       "3  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.008582  0.0  0.0  ...  0.0   \n",
       "4  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.007356  0.0  0.0  ...  0.0   \n",
       "\n",
       "   492  493  494  495  496  497  498  499  class  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      1  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      1  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      2  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      1  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "Domain=pd.DataFrame(data.x.numpy())\n",
    "#label=pd.DataFrame(data.y.numpy(),columns =['class'])\n",
    "Domain=pd.concat([Domain,label], axis=1)\n",
    "Domain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b72b8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_nodes=len(data.y)\n",
    "fe_len=len(data.x[0])\n",
    "catagories=data.y.numpy()\n",
    "ddata_by_class = {cls: Domain.loc[Domain['class'] == cls].drop(['class'], axis=1) for cls in range(max(catagories) + 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "962fad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Average(lst):\n",
    "    # average function\n",
    "    avg = np.average(lst)\n",
    "    return(avg)\n",
    "sel_basis = [[Average(list(df[i].to_numpy())) for i in range(len(df.columns))]for df in ddata_by_class.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80d9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "775f70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 19716 (100%)"
     ]
    }
   ],
   "source": [
    "feature_names = [ii for ii in range(fe_len)]\n",
    "Euc_Fec=[]\n",
    "for i in range(Number_nodes):\n",
    "    print(\"\\rProcessing file {} ({}%)\".format(i, 100*i//(Number_nodes-1)), end='', flush=True)\n",
    "    vec=[]\n",
    "    f=Domain.loc[i, feature_names].values.flatten().tolist()\n",
    "    for j in range(max(catagories)+1):\n",
    "        vec.append(np.linalg.norm(np.array(f) - np.array(sel_basis[j])))\n",
    "    f.clear()\n",
    "    Euc_Fec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e1b82b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2859691472918565, 0.2451948567242483, 0.26428544455036057]\n"
     ]
    }
   ],
   "source": [
    "print(Euc_Fec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d0a697f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "S_Fec=scaler.fit_transform(SFec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c63a255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2535, 0.2234, 0.2358],\n",
      "        [0.2860, 0.2452, 0.2643],\n",
      "        [0.2814, 0.3077, 0.3154],\n",
      "        ...,\n",
      "        [0.3384, 0.3121, 0.3134],\n",
      "        [0.4211, 0.4479, 0.4445],\n",
      "        [0.3380, 0.3139, 0.3143]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Euc_fe=torch.tensor(Euc_Fec)\n",
    "Inc_fe=torch.tensor(Fec)\n",
    "sel_fe=torch.tensor(S_Fec)\n",
    "#CC_domain=torch.cat((sel_fe,Euc_fe), 1).float()\n",
    "#topo_fe=torch.cat((topo_betti0,topo_betti1),1)\n",
    "CC_domain=torch.tensor(Euc_Fec).float()\n",
    "print(CC_domain)\n",
    "CC_domain.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f7d51",
   "metadata": {},
   "source": [
    "# W-GSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "55c6fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 3], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], adj_t=[19717, 19717, nnz=88648], topo=[19717, 42])\n"
     ]
    }
   ],
   "source": [
    "data.x=CC_domain\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b4763ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x15e475510>\n",
      "Run 01:\n",
      "Highest Train: 95.00\n",
      "Highest Valid: 81.80\n",
      "  Final Train: 90.00\n",
      "   Final Test: 77.30\n",
      "Run 02:\n",
      "Highest Train: 91.67\n",
      "Highest Valid: 78.20\n",
      "  Final Train: 86.67\n",
      "   Final Test: 74.30\n",
      "Run 03:\n",
      "Highest Train: 90.00\n",
      "Highest Valid: 76.80\n",
      "  Final Train: 90.00\n",
      "   Final Test: 73.50\n",
      "Run 04:\n",
      "Highest Train: 90.00\n",
      "Highest Valid: 76.00\n",
      "  Final Train: 83.33\n",
      "   Final Test: 73.60\n",
      "Run 05:\n",
      "Highest Train: 91.67\n",
      "Highest Valid: 78.00\n",
      "  Final Train: 91.67\n",
      "   Final Test: 72.70\n",
      "Run 06:\n",
      "Highest Train: 80.00\n",
      "Highest Valid: 68.20\n",
      "  Final Train: 80.00\n",
      "   Final Test: 66.40\n",
      "Run 07:\n",
      "Highest Train: 91.67\n",
      "Highest Valid: 80.40\n",
      "  Final Train: 86.67\n",
      "   Final Test: 76.70\n",
      "Run 08:\n",
      "Highest Train: 93.33\n",
      "Highest Valid: 75.20\n",
      "  Final Train: 83.33\n",
      "   Final Test: 73.10\n",
      "Run 09:\n",
      "Highest Train: 93.33\n",
      "Highest Valid: 77.60\n",
      "  Final Train: 81.67\n",
      "   Final Test: 74.40\n",
      "Run 10:\n",
      "Highest Train: 88.33\n",
      "Highest Valid: 74.40\n",
      "  Final Train: 86.67\n",
      "   Final Test: 70.20\n",
      "All runs:\n",
      "Highest Train: 90.50 ± 4.16\n",
      "Highest Valid: 76.66 ± 3.72\n",
      "  Final Train: 86.00 ± 3.87\n",
      "   Final Test: 73.22 ± 3.12\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_channels': 16, 'dropout': 0.5, 'epochs': 200, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-4, 'lr': 0.01}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = Planetoid(root='/tmp/cora', name='Cora',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    train_idx = np.where(data.train_mask)[0]\n",
    "    valid_idx = np.where(data.val_mask)[0]\n",
    "    test_idx = np.where(data.test_mask)[0]\n",
    "    \n",
    "    model = SAGE(data.num_features, args.hidden_channels,\n",
    "                    dataset.num_classes, args.num_layers,\n",
    "                    args.dropout, args.heads)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "            result = test(model, data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                #print(f'Run: {run + 1:02d}, 'f'Epoch: {epoch:02d}, 'f'Loss: {loss:.4f}, 'f'Train: {100 * train_acc:.2f}%, '\n",
    "                 #     f'Valid: {100 * valid_acc:.2f}% '\n",
    "                  #    f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085c7fd",
   "metadata": {},
   "source": [
    "# TOPOLOGICAL ENCODING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "33e47b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "607be4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1378  1544  6092 ... 12278  4284 16030]\n",
      " [    0     0     0 ... 19714 19715 19716]]\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_index.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52514bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Edge_idx=data.edge_index.numpy()\n",
    "Node=range(Number_nodes)\n",
    "Edgelist=[]\n",
    "for i in range(len(Edge_idx[1])):\n",
    "    Edgelist.append((Edge_idx[0][i],Edge_idx[1][i]))\n",
    "#print(Edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a \"plain\" graph is undirected\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# give each a node a 'name', which is a letter in this case.\n",
    "#G.add_node('a')\n",
    "\n",
    "# the add_nodes_from method allows adding nodes from a sequence, in this case a list\n",
    "#nodes_to_add = ['b', 'c', 'd']\n",
    "G.add_nodes_from(Node)\n",
    "\n",
    "# add edge from 'a' to 'b'\n",
    "# since this graph is undirected, the order doesn't matter here\n",
    "#G.add_edge('a', 'b')\n",
    "\n",
    "# just like add_nodes_from, we can add edges from a sequence\n",
    "# edges should be specified as 2-tuples\n",
    "#edges_to_add = [('a', 'c'), ('b', 'c'), ('c', 'd')]\n",
    "G.add_edges_from(Edgelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781abc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Topological_Feature_subLevel(adj,filtration_fun, Filtration):\n",
    "        betti_0=[]\n",
    "        betti_1=[]\n",
    "        for p in range(len(Filtration)):\n",
    "            n_active = np.where(np.array(filtration_fun) <= Filtration[p])[0].tolist()\n",
    "            Active_node=np.unique(n_active)\n",
    "            if (len(Active_node)==0):\n",
    "                betti_0.append(0)\n",
    "                betti_1.append(0)\n",
    "            else:\n",
    "                b=adj[Active_node,:][:,Active_node]\n",
    "                my_flag=pyflagser.flagser_unweighted(b, min_dimension=0, max_dimension=2, directed=False, coeff=2, approximation=None)\n",
    "                x = my_flag[\"betti\"]\n",
    "                betti_0.append(x[0])\n",
    "                betti_1.append(x[1])\n",
    "            n_active.clear()\n",
    "        return betti_0,betti_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Degree_list(Graph):\n",
    "    degree_list = [Graph.degree(node) for node in Graph.nodes]\n",
    "    return np.array(degree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_list=Degree_list(G)\n",
    "unique_list=np.unique(degree_list)\n",
    "for d in unique_list:\n",
    "    count=0\n",
    "    for i in range(len(degree_list)):\n",
    "        if degree_list[i]==d:\n",
    "            count=count+1\n",
    "    print(int(d),\" | \",count,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7080881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyflagser\n",
    "Node_fil=[0,2,4,6,8,10,12,14,16,18,20,22,24,30,34]\n",
    "topo_betti_0=[]\n",
    "topo_betti_1=[]\n",
    "Node_Edge=[]\n",
    "for i in range(Number_nodes):\n",
    "    print(\"\\rProcessing file {} ({}%)\".format(i, 100*i//(Number_nodes-1)), end='', flush=True)\n",
    "    subgraph=ego_graph(G, i, radius=2, center=False, undirected=True, distance=None)\n",
    "    filt=Degree_list(subgraph)\n",
    "    A_sub = nx.to_numpy_array(subgraph)# adjacency matrix of subgraph\n",
    "    fe=Topological_Feature_subLevel(A_sub,filt,Node_fil)\n",
    "    topo_betti_0.append(fe[0])\n",
    "    topo_betti_1.append(fe[1])\n",
    "    Node_Edge.append([subgraph.number_of_nodes(),subgraph.number_of_edges()])\n",
    "    #topo_with_NE.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5892bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( Node_Edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0280d71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>11.1</th>\n",
       "      <th>12.1</th>\n",
       "      <th>13.1</th>\n",
       "      <th>14.1</th>\n",
       "      <th>15.1</th>\n",
       "      <th>16.1</th>\n",
       "      <th>17.1</th>\n",
       "      <th>18.1</th>\n",
       "      <th>19.1</th>\n",
       "      <th>20.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   0   1   2   3   4   5   6   7   8  ...  11.1  12.1  13.1  \\\n",
       "0           0  72  87  94  94  96  96  96  91  91  ...     0     0     0   \n",
       "1           1  20  25  28  25  25  21  21  21  21  ...     1     1     1   \n",
       "2           2  42  43  44  44  44  44  44  44  44  ...     0     0     0   \n",
       "3           3  10  14  15  12  12  12  12  12  12  ...     0     0     0   \n",
       "4           4   6  29  30  31  30  30  30  30  30  ...     0     0     0   \n",
       "\n",
       "   14.1  15.1  16.1  17.1  18.1  19.1  20.1  \n",
       "0     0     0     1     4     4     7    11  \n",
       "1     1     1     5     5     5     5     5  \n",
       "2     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     1     0     0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Feature_Pubmed.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356cab3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11.1</th>\n",
       "      <th>12.1</th>\n",
       "      <th>13.1</th>\n",
       "      <th>14.1</th>\n",
       "      <th>15.1</th>\n",
       "      <th>16.1</th>\n",
       "      <th>17.1</th>\n",
       "      <th>18.1</th>\n",
       "      <th>19.1</th>\n",
       "      <th>20.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9  ...  11.1  12.1  13.1  14.1  15.1  \\\n",
       "0  72  87  94  94  96  96  96  91  91  91  ...     0     0     0     0     0   \n",
       "1  20  25  28  25  25  21  21  21  21  21  ...     1     1     1     1     1   \n",
       "2  42  43  44  44  44  44  44  44  44  44  ...     0     0     0     0     0   \n",
       "3  10  14  15  12  12  12  12  12  12  12  ...     0     0     0     0     0   \n",
       "4   6  29  30  31  30  30  30  30  30  30  ...     0     0     0     0     0   \n",
       "\n",
       "   16.1  17.1  18.1  19.1  20.1  \n",
       "0     1     4     4     7    11  \n",
       "1     5     5     5     5     5  \n",
       "2     0     0     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     1     0     0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data1=data.drop(['Unnamed: 0'], axis=1)\n",
    "Data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ede895",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topo_fe=torch.tensor(Data1.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d065de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(topo_fe[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#for i in range(300):\n",
    "X0=[]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler to data and transform data\n",
    "topo_fe=scaler.fit_transform(topo_fe)\n",
    "print(topo_fe[0])\n",
    "topo_fe=np.array(topo_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e70862",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Topo_fe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "49a08a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], adj_t=[19717, 19717, nnz=88648])\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_betti0=torch.tensor(topo_betti_0).float()\n",
    "topo_betti1=torch.tensor(topo_betti_1).float()\n",
    "NodeEdge=torch.tensor(Node_Edge).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bbeea52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 3], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], adj_t=[19717, 19717, nnz=88648], topo=[19717, 42])\n"
     ]
    }
   ],
   "source": [
    "data.x=CC_domain\n",
    "#data.topo=torch.tensor(topo_fe).float()\n",
    "data.topo=Topo_fe\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90280a23",
   "metadata": {},
   "source": [
    "# TOPO-W-GSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd4668e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout,heads):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.heads=heads\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        #return x.log_softmax(dim=-1)\n",
    "        return x\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters_mlp(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            #x = F.relu(x)\n",
    "            x=F.sigmoid(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        #return torch.log_softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "class MLP2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP2, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters_mlp2(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            #x = F.relu(x)\n",
    "            x=F.sigmoid(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n",
    "def train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2):\n",
    "    model.train()\n",
    "    mlp_model.train()\n",
    "    mlp_2.train()\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_mlp.zero_grad()\n",
    "    optimizer_mlp2.zero_grad()\n",
    "    gcn_embedding = model(data.x, data.adj_t)[train_idx]\n",
    "    #print(gcn_embedding)\n",
    "    mlp_embedding = mlp_model(data.topo[train_idx])\n",
    "    #print(mlp_embedding)\n",
    "    combined_embedding = torch.cat((gcn_embedding, mlp_embedding), dim=1)\n",
    "    #print(combined_embedding)\n",
    "    mlp_emb = mlp_2(combined_embedding)\n",
    "    #print(mlp_emb)\n",
    "    loss = F.nll_loss(mlp_emb, data.y.squeeze()[train_idx])\n",
    "    #loss = F.nll_loss(combined_embedding, data.y.squeeze()[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer_mlp2.step()\n",
    "    optimizer.step()\n",
    "    optimizer_mlp.step()\n",
    "    \n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def ACC(Prediction, Label):\n",
    "    correct = Prediction.view(-1).eq(Label).sum().item()\n",
    "    total=len(Label)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx):\n",
    "    model.eval()\n",
    "    mlp_model.eval()\n",
    "    mlp_2.eval()\n",
    "\n",
    "    gcn_out = model(data.x, data.adj_t)\n",
    "    #print(gcn_out[0])\n",
    "    mlp_out=mlp_model(data.topo)\n",
    "    #print(mlp_out)\n",
    "    #out=torch.cat((gcn_out,mlp_out),dim=1)\n",
    "    Com=torch.cat((gcn_out,mlp_out),dim=1)\n",
    "    out=mlp_2(Com)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    #print(y_pred[0])\n",
    "    y_pred=y_pred.view(-1)\n",
    "    train_acc=ACC(data.y[train_idx],y_pred[train_idx])\n",
    "    valid_acc=ACC(data.y[valid_idx],y_pred[valid_idx])\n",
    "    test_acc =ACC(data.y[test_idx],y_pred[test_idx])\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ef21f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x2ab20a2c0>\n",
      "Run 01:\n",
      "Highest Train: 86.67\n",
      "Highest Valid: 79.00\n",
      "  Final Train: 86.67\n",
      "   Final Test: 77.00\n",
      "Run 02:\n",
      "Highest Train: 90.00\n",
      "Highest Valid: 77.40\n",
      "  Final Train: 88.33\n",
      "   Final Test: 74.60\n",
      "Run 03:\n",
      "Highest Train: 96.67\n",
      "Highest Valid: 77.00\n",
      "  Final Train: 96.67\n",
      "   Final Test: 70.00\n",
      "Run 04:\n",
      "Highest Train: 95.00\n",
      "Highest Valid: 74.40\n",
      "  Final Train: 90.00\n",
      "   Final Test: 71.20\n",
      "Run 05:\n",
      "Highest Train: 90.00\n",
      "Highest Valid: 81.80\n",
      "  Final Train: 86.67\n",
      "   Final Test: 77.60\n",
      "Run 06:\n",
      "Highest Train: 91.67\n",
      "Highest Valid: 73.20\n",
      "  Final Train: 81.67\n",
      "   Final Test: 72.30\n",
      "Run 07:\n",
      "Highest Train: 91.67\n",
      "Highest Valid: 78.00\n",
      "  Final Train: 83.33\n",
      "   Final Test: 76.70\n",
      "Run 08:\n",
      "Highest Train: 91.67\n",
      "Highest Valid: 75.80\n",
      "  Final Train: 91.67\n",
      "   Final Test: 74.10\n",
      "Run 09:\n",
      "Highest Train: 93.33\n",
      "Highest Valid: 75.80\n",
      "  Final Train: 86.67\n",
      "   Final Test: 74.70\n",
      "Run 10:\n",
      "Highest Train: 86.67\n",
      "Highest Valid: 76.20\n",
      "  Final Train: 86.67\n",
      "   Final Test: 71.70\n",
      "All runs:\n",
      "Highest Train: 91.33 ± 3.22\n",
      "Highest Valid: 76.86 ± 2.42\n",
      "  Final Train: 87.83 ± 4.23\n",
      "   Final Test: 73.99 ± 2.63\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_channels': 16, 'dropout': 0.5, 'epochs': 200, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-4, 'lr': 0.01,'hidden_channels_mlp': 20,'dropout_mlp': 0.5,'num_layers_mlp': 3}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = Planetoid(root='/tmp/cora', name='Cora',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    X = data.topo\n",
    "    y_true = data.y\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    train_idx = np.where(data.train_mask)[0]\n",
    "    valid_idx = np.where(data.val_mask)[0]\n",
    "    test_idx = np.where(data.test_mask)[0]\n",
    "    \n",
    "    model = SAGE(data.num_features, args.hidden_channels,20, args.num_layers,args.dropout,args.heads)\n",
    "    mlp_model = MLP(X.size(-1), args.hidden_channels_mlp, 10,args.num_layers_mlp, args.dropout_mlp)\n",
    "    #print(mlp_model.parameters())\n",
    "    mlp_2 = MLP2(30, 100, dataset.num_classes,3, 0.3)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        mlp_model.reset_parameters_mlp()\n",
    "        mlp_2.reset_parameters_mlp2()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        optimizer_mlp=torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "        optimizer_mlp2=torch.optim.Adam(mlp_2.parameters(), lr=0.01)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2)\n",
    "            result = test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                #print(f'Run: {run + 1:02d}, 'f'Epoch: {epoch:02d}, 'f'Loss: {loss:.4f}, 'f'Train: {100 * train_acc:.2f}%, '\n",
    "                      #f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      #f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5721b69",
   "metadata": {},
   "source": [
    "# Topo-GSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceda79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], adj_t=[19717, 19717, nnz=88648], topo=[19717, 42])\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed',transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "data.topo=Topo_fe\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9911aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.objectview object at 0x148958b20>\n",
      "Run 01:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 79.60\n",
      "  Final Train: 100.00\n",
      "   Final Test: 76.40\n",
      "Run 02:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 78.80\n",
      "  Final Train: 100.00\n",
      "   Final Test: 76.20\n",
      "Run 03:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 78.00\n",
      "  Final Train: 100.00\n",
      "   Final Test: 75.70\n",
      "Run 04:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 78.00\n",
      "  Final Train: 100.00\n",
      "   Final Test: 74.50\n",
      "Run 05:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 79.80\n",
      "  Final Train: 100.00\n",
      "   Final Test: 74.30\n",
      "Run 06:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 77.80\n",
      "  Final Train: 100.00\n",
      "   Final Test: 73.80\n",
      "Run 07:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 78.80\n",
      "  Final Train: 100.00\n",
      "   Final Test: 74.50\n",
      "Run 08:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 79.00\n",
      "  Final Train: 100.00\n",
      "   Final Test: 76.70\n",
      "Run 09:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 78.20\n",
      "  Final Train: 100.00\n",
      "   Final Test: 75.80\n",
      "Run 10:\n",
      "Highest Train: 100.00\n",
      "Highest Valid: 76.80\n",
      "  Final Train: 100.00\n",
      "   Final Test: 75.80\n",
      "All runs:\n",
      "Highest Train: 100.00 ± 0.00\n",
      "Highest Valid: 78.48 ± 0.90\n",
      "  Final Train: 100.00 ± 0.00\n",
      "   Final Test: 75.37 ± 1.01\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args={'model_type': 'GCN', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_channels': 16, 'dropout': 0.5, 'epochs': 100, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0,'runs':10, 'log_steps':1,\n",
    "         'weight_decay': 5e-4, 'lr': 0.01,'hidden_channels_mlp': 20,'dropout_mlp': 0.5,'num_layers_mlp': 3}\n",
    "\n",
    "    args = objectview(args)\n",
    "    print(args)\n",
    "    # call the dataset here with x,y,train_mask,test_mask,Val_mask, and Adj\n",
    "    # To add extra feature we can simply update data.x=new fev tensor or we can add new feature\n",
    "    #dataset = Planetoid(root='/tmp/cora', name='Cora',transform=T.ToSparseTensor())\n",
    "    #data = dataset[0]\n",
    "    X = data.topo\n",
    "    y_true = data.y\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    \n",
    "    train_idx = np.where(data.train_mask)[0]\n",
    "    valid_idx = np.where(data.val_mask)[0]\n",
    "    test_idx = np.where(data.test_mask)[0]\n",
    "    \n",
    "    model = SAGE(data.num_features, args.hidden_channels,20, args.num_layers,args.dropout,args.heads)\n",
    "    mlp_model = MLP(X.size(-1), args.hidden_channels_mlp, 10,args.num_layers_mlp, args.dropout_mlp)\n",
    "    #print(mlp_model.parameters())\n",
    "    mlp_2 = MLP2(30, 100, dataset.num_classes,3, 0.3)\n",
    "\n",
    "    logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        mlp_model.reset_parameters_mlp()\n",
    "        mlp_2.reset_parameters_mlp2()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        optimizer_mlp=torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "        optimizer_mlp2=torch.optim.Adam(mlp_2.parameters(), lr=0.01)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "            loss = train(model,mlp_model,mlp_2,data, train_idx, optimizer,optimizer_mlp,optimizer_mlp2)\n",
    "            result = test(model,mlp_model,mlp_2,data, train_idx,valid_idx,test_idx)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                #print(f'Run: {run + 1:02d}, 'f'Epoch: {epoch:02d}, 'f'Loss: {loss:.4f}, 'f'Train: {100 * train_acc:.2f}%, '\n",
    "                      #f'Valid: {100 * valid_acc:.2f}% '\n",
    "                      #f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed4f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
